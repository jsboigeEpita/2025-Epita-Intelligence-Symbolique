CMAKE_ARGS="-DGGML_CUDA=on" FORCE_CMAKE=1 pip install --upgrade --no-cache-dir llama-cpp-python

wget https://huggingface.co/bartowski/Llama-3SOME-8B-v2-GGUF/resolve/main/Llama-3SOME-8B-v2-Q6_K.gguf -O model.gguf

wget https://huggingface.co/bartowski/Mistral-NeMo-Minitron-8B-Instruct-GGUF/resolve/main/Mistral-NeMo-Minitron-8B-Instruct-Q8_0.gguf -O model.gguf

wget https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q8_0.gguf -O model.gguf

wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf -O model.gguf

wget https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q8_0.gguf -O model.gguf

wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q6_K.gguf -O model.gguf

wget https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K_L.gguf -O model.gguf

wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf -O model.gguf

wget https://huggingface.co/Qwen/Qwen3-14B-GGUF/resolve/main/Qwen3-14B-Q5_K_M.gguf -O model.gguf