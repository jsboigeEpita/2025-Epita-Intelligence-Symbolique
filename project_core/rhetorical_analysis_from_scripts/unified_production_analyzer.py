#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Production Analyzer - Point d'entr√©e CLI principal pour l'analyse en production
========================================================================================

Script consolid√© int√©grant les meilleurs √©l√©ments de :
- scripts/main/analyze_text.py (CLI complet avec 20+ param√®tres)
- scripts/execution/advanced_rhetorical_analysis.py (production mature)
- scripts/main/analyze_text_authentic.py (authenticit√© 100%)

Innovations int√©gr√©es :
- Configuration LLM centralis√©e (√©limine 15+ duplications)
- M√©canisme de retry automatique pour TweetyProject
- Syst√®me de validation d'authenticit√© √† 100%
- Architecture d'orchestration multi-agents sophistiqu√©e
- TraceAnalyzer v2.0 avec conversation agentielle

Version: 1.0.0
Cr√©√©: 10/06/2025
Auteur: Roo
"""
# Workflow d'Ex√©cution :
# 1. Parsing des arguments CLI et/ou d'un fichier de configuration.
# 2. Validation des d√©pendances critiques (Python, Tweety, LLM).
# 3. Initialisation des services (LLM, TraceAnalyzer).
# 4. Traitement de l'entr√©e (texte simple, fichier, ou dossier en mode batch).
# 5. Orchestration de l'analyse via le UnifiedProductionAnalyzer.
# 6. G√©n√©ration et sauvegarde d'un rapport de session d√©taill√©.
import os # D√©plac√© ici
import sys # D√©plac√© ici

# Configuration UTF-8 pour la sortie standard et les erreurs
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')
if sys.stderr.encoding != 'utf-8':
    sys.stderr.reconfigure(encoding='utf-8')
import asyncio
import logging
import argparse
import json
import time
import warnings # Ajout de warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from pydantic import BaseModel
from dataclasses import field
from enum import Enum
import traceback

# Ajout du r√©pertoire racine du projet au chemin

# --- GESTION DE LA D√âPR√âCIATION ---
# Ce script est maintenant consid√©r√© comme obsol√®te et son importation ne doit pas
# provoquer d'effets de bord comme l'activation d'environnement ou un sys.exit().
# L'activation est g√©r√©e par des scripts de plus haut niveau comme activate_project_env.ps1.
warnings.warn(
    "Le script 'unified_production_analyzer.py' est obsol√®te. "
    "Son importation est une op√©ration sans effet (no-op) pour maintenir la compatibilit√© des tests. "
    "L'activation de l'environnement est maintenant g√©r√©e en amont.",
    DeprecationWarning,
    stacklevel=2
)
# Le bloc d'activation d'environnement pr√©c√©dent a √©t√© supprim√© pour √©viter les sys.exit().
# Configuration avanc√©e du logging (d√©plac√©e ici AVANT l'activation de l'env)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)8s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("UnifiedProductionAnalyzer_Global") # Nom diff√©rent pour √©viter conflit potentiel

# Auto-activation de l'environnement CONDA/VENV via le one-liner
# C'est maintenant la m√©thode standard recommand√©e pour s'assurer que l'environnement
# est pr√™t, y compris le chargement de .env et la configuration de JAVA_HOME.
# import scripts.core.auto_env # NEUTRALIS√â - L'environnement est maintenant activ√© en amont par environment_manager.py
class LogicType(Enum):
    """Types de logique support√©s avec fallback automatique"""
    FOL = "fol"
    PL = "propositional"
    MODAL = "modal"


class MockLevel(Enum):
    """Niveaux de simulation pour le contr√¥le d'authenticit√©"""
    NONE = "none"        # 100% authentique - d√©faut production
    PARTIAL = "partial"   # Hybride d√©veloppement
    FULL = "full"        # Test uniquement


class OrchestrationType(Enum):
    """Types d'orchestration multi-agents"""
    UNIFIED = "unified"
    CONVERSATION = "conversation"
    MICRO = "micro"
    REAL_LLM = "real_llm"


class AnalysisMode(Enum):
    """Modes d'analyse rh√©torique"""
    FALLACIES = "fallacies"
    COHERENCE = "coherence"
    SEMANTIC = "semantic"
    UNIFIED = "unified"
    ADVANCED = "advanced"


class UnifiedProductionConfig(BaseModel):
    """Configuration centralis√©e pour l'analyse en production"""
    
    # === Configuration LLM Centralis√©e ===
    llm_service: str = "openai"
    llm_model: str = "gpt-4"
    llm_temperature: float = 0.3
    llm_max_tokens: int = 2000
    llm_timeout: int = 60
    llm_retry_count: int = 3
    llm_retry_delay: float = 1.0
    
    # === Configuration Logique ===
    logic_type: LogicType = LogicType.FOL
    enable_fallback: bool = True
    fallback_order: List[LogicType] = field(default_factory=lambda: [LogicType.FOL, LogicType.PL])
    
    # === Configuration Authenticit√© ===
    mock_level: MockLevel = MockLevel.NONE
    require_real_gpt: bool = True
    require_real_tweety: bool = True
    require_full_taxonomy: bool = True
    validate_tools: bool = True
    
    # === Configuration Orchestration ===
    orchestration_type: OrchestrationType = OrchestrationType.UNIFIED
    enable_conversation_trace: bool = True
    enable_micro_orchestration: bool = False
    max_agents: int = 5
    
    # === Configuration Analyse ===
    analysis_modes: List[AnalysisMode] = field(default_factory=lambda: [AnalysisMode.UNIFIED])
    batch_size: int = 10
    enable_parallel: bool = True
    max_workers: int = 4
    
    # === Configuration Retry TweetyProject ===
    tweety_retry_count: int = 5
    tweety_retry_delay: float = 2.0
    tweety_timeout: int = 30
    tweety_memory_limit: str = "2g"
    
    # === Configuration Rapports ===
    output_format: str = "json"
    enable_detailed_logs: bool = True
    save_intermediate: bool = False
    report_level: str = "production"
    save_trace: bool = True
    save_orchestration_trace: bool = True
    
    # === Configuration Validation ===
    validate_inputs: bool = True
    validate_outputs: bool = True
    check_dependencies: bool = True
    
    def validate(self) -> Tuple[bool, List[str]]:
        """Valide la configuration et retourne les erreurs d√©tect√©es"""
        errors = []
        
        # Validation authenticit√©
        if self.mock_level == MockLevel.NONE:
            if not self.require_real_gpt:
                errors.append("Mode production requiert require_real_gpt=True")
            if not self.require_real_tweety:
                errors.append("Mode production requiert require_real_tweety=True")
        
        # Validation retry
        if self.tweety_retry_count < 1:
            errors.append("tweety_retry_count doit √™tre >= 1")
        if self.llm_retry_count < 1:
            errors.append("llm_retry_count doit √™tre >= 1")
            
        # Validation parall√©lisme
        if self.enable_parallel and self.max_workers < 1:
            errors.append("max_workers doit √™tre >= 1 si enable_parallel=True")
            
        return len(errors) == 0, errors


class RetryMechanism:
    """M√©canisme de retry intelligent pour TweetyProject et LLM"""
    
    def __init__(self, config: UnifiedProductionConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.RetryMechanism")
        
    async def retry_with_fallback(self, operation: callable, *args, **kwargs) -> Any:
        """
        Ex√©cute une op√©ration avec retry et fallback automatique
        
        Args:
            operation: Fonction √† ex√©cuter
            *args, **kwargs: Arguments de la fonction
            
        Returns:
            R√©sultat de l'op√©ration ou exception
        """
        last_exception = None
        
        # Retry principal
        for attempt in range(self.config.tweety_retry_count):
            try:
                self.logger.debug(f"Tentative {attempt + 1}/{self.config.tweety_retry_count}")
                result = await operation(*args, **kwargs)
                if attempt > 0:
                    self.logger.info(f"‚úÖ Succ√®s apr√®s {attempt + 1} tentatives")
                return result
                
            except Exception as e:
                last_exception = e
                self.logger.warning(f"‚ö†Ô∏è Tentative {attempt + 1} √©chou√©e: {e}")
                
                if attempt < self.config.tweety_retry_count - 1:
                    delay = self.config.tweety_retry_delay * (2 ** attempt)  # Backoff exponentiel
                    self.logger.debug(f"Attente {delay}s avant retry...")
                    await asyncio.sleep(delay)
        
        # Fallback si activ√©
        if self.config.enable_fallback and hasattr(operation, '__fallback__'):
            try:
                self.logger.warning("üîÑ Activation du fallback...")
                return await operation.__fallback__(*args, **kwargs)
            except Exception as fallback_error:
                self.logger.error(f"‚ùå √âchec du fallback: {fallback_error}")
                
        # √âchec final
        self.logger.error(f"‚ùå √âchec d√©finitif apr√®s {self.config.tweety_retry_count} tentatives")
        raise last_exception


class TraceAnalyzerV2:
    """TraceAnalyzer v2.0 avec conversation agentielle avanc√©e"""
    
    def __init__(self, config: UnifiedProductionConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.TraceAnalyzerV2")
        self.conversation_history = []
        self.agent_states = {}
        
    async def initialize(self) -> bool:
        """Initialise l'analyseur de traces"""
        try:
            self.logger.info("üîß Initialisation TraceAnalyzer v2.0...")
            
            # Initialisation des √©tats d'agents
            self.agent_states = {
                "sherlock": {"status": "ready", "context": {}},
                "watson": {"status": "ready", "context": {}}, 
                "oracle": {"status": "ready", "context": {}},
                "synthesis": {"status": "ready", "context": {}}
            }
            
            self.logger.info("‚úÖ TraceAnalyzer v2.0 initialis√©")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation TraceAnalyzer: {e}")
            return False
    
    async def capture_conversation(self, agent_name: str, message: str, response: str) -> str:
        """Capture une interaction conversationnelle"""
        timestamp = datetime.now().isoformat()
        
        conversation_entry = {
            "timestamp": timestamp,
            "agent": agent_name,
            "message": message,
            "response": response,
            "context": self.agent_states.get(agent_name, {})
        }
        
        self.conversation_history.append(conversation_entry)
        
        # Mise √† jour de l'√©tat de l'agent
        if agent_name in self.agent_states:
            self.agent_states[agent_name]["last_interaction"] = timestamp
            self.agent_states[agent_name]["interaction_count"] = \
                self.agent_states[agent_name].get("interaction_count", 0) + 1
        
        self.logger.debug(f"üìù Conversation captur√©e: {agent_name} -> {len(response)} chars")
        return conversation_entry["timestamp"]
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """G√©n√®re un r√©sum√© de la conversation"""
        return {
            "total_interactions": len(self.conversation_history),
            "agents_used": list(self.agent_states.keys()),
            "conversation_duration": self._calculate_duration(),
            "agent_stats": self.agent_states
        }
    
    def _calculate_duration(self) -> float:
        """Calcule la dur√©e totale de la conversation"""
        if len(self.conversation_history) < 2:
            return 0.0
        
        start_time = datetime.fromisoformat(self.conversation_history[0]["timestamp"])
        end_time = datetime.fromisoformat(self.conversation_history[-1]["timestamp"])
        
        return (end_time - start_time).total_seconds()


class UnifiedLLMService:
    """Service LLM centralis√© avec gestion d'authenticit√©"""
    
    def __init__(self, config: UnifiedProductionConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.UnifiedLLMService")
        self.retry_mechanism = RetryMechanism(config)
        self._client = None
        
    async def initialize(self) -> bool:
        """Initialise le service LLM selon la configuration"""
        try:
            self.logger.info(f"üîß Initialisation service LLM ({self.config.llm_service})...")
            
            # Validation authenticit√©
            if self.config.mock_level == MockLevel.NONE and not self.config.require_real_gpt:
                raise ValueError("Mode production requiert un LLM authentique")
            
            # Initialisation selon le mode mock (prioritaire sur le service)
            if self.config.mock_level == MockLevel.FULL:
                await self._initialize_mock()
            elif self.config.llm_service == "openai":
                await self._initialize_openai()
            elif self.config.llm_service == "mock":
                await self._initialize_mock()
            else:
                raise ValueError(f"Service LLM non support√©: {self.config.llm_service}")
            
            self.logger.info("‚úÖ Service LLM initialis√©")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation LLM: {e}")
            return False
    
    async def _initialize_openai(self):
        """Initialise le client OpenAI authentique"""
        try:
            import openai
            self._client = openai.AsyncOpenAI()
            
            # Test de connexion seulement si mode authentique
            if self.config.mock_level == MockLevel.NONE:
                await self._client.models.list()
                self.logger.info("‚úÖ Client OpenAI authentique connect√©")
            else:
                self.logger.info("‚úÖ Client OpenAI initialis√© (mode test)")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur connexion OpenAI: {e}")
            raise
    
    async def _initialize_mock(self):
        """Initialise un service mock pour les tests"""
        self.logger.warning("‚ö†Ô∏è Utilisation d'un service LLM mock")
        self._client = "mock_client"
    
    async def analyze_text(self, text: str, analysis_type: str = "rhetorical") -> Dict[str, Any]:
        """Analyse un texte avec retry automatique"""
        
        async def _perform_analysis():
            # Logique de d√©cision am√©lior√©e :
            # 1. Si le service est explicitement 'mock', on utilise TOUJOURS l'analyse mock.
            # 2. Sinon, on se base sur le mock_level pour d√©cider entre r√©el et mock.
            if self.config.llm_service == "mock" or self.config.mock_level != MockLevel.NONE:
                return await self._mock_analysis(text, analysis_type)
            else:
                return await self._real_analysis(text, analysis_type)
        
        return await self.retry_mechanism.retry_with_fallback(_perform_analysis)
    
    async def _real_analysis(self, text: str, analysis_type: str) -> Dict[str, Any]:
        """Analyse authentique via OpenAI"""
        prompt = self._build_prompt(text, analysis_type)
        
        response = await self._client.chat.completions.create(
            model=self.config.llm_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.config.llm_temperature,
            max_tokens=self.config.llm_max_tokens,
            timeout=self.config.llm_timeout
        )
        
        return {
            "analysis": response.choices[0].message.content,
            "model_used": self.config.llm_model,
            "tokens_used": response.usage.total_tokens,
            "authentic": True
        }
    
    async def _mock_analysis(self, text: str, analysis_type: str) -> Dict[str, Any]:
        """Analyse simul√©e pour tests"""
        await asyncio.sleep(0.1)  # Simulation latence
        return {
            "analysis": f"[MOCK] Analyse {analysis_type} de {len(text)} caract√®res",
            "model_used": "mock",
            "tokens_used": len(text) // 4,
            "authentic": False
        }
    
    def _build_prompt(self, text: str, analysis_type: str) -> str:
        """Construit le prompt d'analyse selon le type"""
        prompts = {
            "rhetorical": f"Analysez les techniques rh√©toriques dans ce texte:\n\n{text}",
            "fallacies": f"Identifiez les sophismes dans ce texte:\n\n{text}",
            "coherence": f"√âvaluez la coh√©rence logique de ce texte:\n\n{text}",
            "semantic": f"Analysez la s√©mantique de ce texte:\n\n{text}"
        }
        
        return prompts.get(analysis_type, prompts["rhetorical"])


class DependencyValidator:
    """Validateur de d√©pendances pr√©-ex√©cution"""
    
    def __init__(self, config: UnifiedProductionConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.DependencyValidator")
        
    async def validate_all(self) -> Tuple[bool, List[str]]:
        """Valide toutes les d√©pendances critiques"""
        self.logger.info("üîç Validation des d√©pendances...")
        
        errors = []
        
        # Validation packages Python
        python_deps = await self._validate_python_dependencies()
        errors.extend(python_deps)
        
        # Validation TweetyProject si requis
        if self.config.require_real_tweety:
            tweety_deps = await self._validate_tweety_dependencies()
            errors.extend(tweety_deps)
        
        # Validation LLM
        llm_deps = await self._validate_llm_dependencies()
        errors.extend(llm_deps)
        
        # Validation syst√®me
        system_deps = await self._validate_system_dependencies()
        errors.extend(system_deps)
        
        success = len(errors) == 0
        
        if success:
            self.logger.info("‚úÖ Toutes les d√©pendances valid√©es")
        else:
            self.logger.error(f"‚ùå {len(errors)} erreurs de d√©pendances d√©tect√©es")
            
        return success, errors
    
    async def _validate_python_dependencies(self) -> List[str]:
        """Valide les d√©pendances Python critiques"""
        errors = []
        critical_packages = [
            "asyncio", "pathlib", "json", "logging",
            "argparse", "datetime", "typing"
        ]
        
        for package in critical_packages:
            try:
                __import__(package)
            except ImportError:
                errors.append(f"Package Python manquant: {package}")
        
        return errors
    
    async def _validate_tweety_dependencies(self) -> List[str]:
        """Valide TweetyProject et JPype"""
        from pathlib import Path
        errors = []

        try:
            # === DEBUT BLOC DE DEBUGGING JPYPE ===
            self.logger.info("="*20 + " DEBUT DEBUG JPYPE " + "="*20)
            
            # 1. V√©rifier JAVA_HOME
            java_home = os.getenv('JAVA_HOME')
            if java_home:
                self.logger.info(f"[DEBUG_JPYPE] JAVA_HOME trouv√©: {java_home}")
                if not os.path.exists(java_home):
                    self.logger.warning(f"[DEBUG_JPYPE] ATTENTION: Le chemin JAVA_HOME n'existe pas: {java_home}")
            else:
                self.logger.warning("[DEBUG_JPYPE] ATTENTION: La variable d'environnement JAVA_HOME n'est pas d√©finie.")

            # 2. Afficher les variables d'environnement li√©es √† Java et Conda
            self.logger.debug("[DEBUG_JPYPE] Variables d'environnement pertinentes:")
            for var in ['JAVA_HOME', 'JDK_HOME', 'JRE_HOME', 'CONDA_PREFIX', 'PATH']:
                self.logger.debug(f"[DEBUG_JPYPE]   - {var}: {os.getenv(var)}")

            self.logger.info("="*20 + " FIN DEBUG JPYPE " + "="*20)
            # === FIN BLOC DE DEBUGGING JPYPE ===
            # === Le PATCH a √©t√© supprim√©. On fait confiance √† l'environnement d'ex√©cution. ===
            # La configuration correcte du PATH, y compris les chemins de la JVM,
            # est maintenant enti√®rement g√©r√©e par `environment_manager.py`.
            # Le script n'a plus besoin d'essayer de r√©parer son propre `sys.path`.

            import jpype
            # S'assurer que la JVM est d√©marr√©e avant de v√©rifier son √©tat
            if not jpype.isJVMStarted():
                try:
                    jpype.startJVM(jpype.getDefaultJVMPath(), convertStrings=False)
                    self.logger.info("JVM d√©marr√©e avec succ√®s par DependencyValidator.")
                except Exception as e:
                    errors.append(f"Impossible de d√©marrer la JVM pour TweetyProject: {e}")
            if not jpype.isJVMStarted() and not any("Impossible de d√©marrer la JVM" in err for err in errors): # Rev√©rifier apr√®s tentative de d√©marrage
                errors.append("JVM TweetyProject non d√©marr√©e (apr√®s tentative)")
        except ImportError as e:
            self.logger.error(f"√âchec de l'import JPype1. sys.path: {sys.path}")
            self.logger.error(f"Traceback de l'erreur d'importation JPype:\n{traceback.format_exc()}")
            errors.append(f"JPype1 import failed: {e}")
            
        # V√©rification des JARs TweetyProject
        project_root = Path(__file__).resolve().parent.parent.parent.parent
        libs_dir = project_root / "libs" / "tweety"
        if not libs_dir.exists():
            errors.append("R√©pertoire libs/tweety manquant")
        else:
            jar_files = list(libs_dir.glob("*.jar"))
            if len(jar_files) == 0:
                errors.append("Aucun JAR TweetyProject trouv√©")
        
        return errors
    
    async def _validate_llm_dependencies(self) -> List[str]:
        """Valide les d√©pendances LLM"""
        errors = []
        
        if self.config.llm_service == "openai" and self.config.mock_level == MockLevel.NONE:
            try:
                import openai
                # V√©rification cl√© API
                if not os.getenv("OPENAI_API_KEY"):
                    errors.append("Variable OPENAI_API_KEY manquante")
            except ImportError:
                errors.append("Package openai manquant")
        
        return errors
    
    async def _validate_system_dependencies(self) -> List[str]:
        """Valide les d√©pendances syst√®me"""
        errors = []
        
        # V√©rification espace disque
        import shutil
        free_space = shutil.disk_usage(".").free
        min_space = 1024 * 1024 * 100  # 100MB minimum
        
        if free_space < min_space:
            errors.append(f"Espace disque insuffisant: {free_space // (1024*1024)}MB disponible")
        
        # V√©rification m√©moire (estimation)
        try:
            import psutil
            available_memory = psutil.virtual_memory().available
            min_memory = 1024 * 1024 * 512  # 512MB minimum
            
            if available_memory < min_memory:
                errors.append(f"M√©moire insuffisante: {available_memory // (1024*1024)}MB disponible")
        except ImportError:
            # psutil optionnel
            pass
        
        return errors


class UnifiedProductionAnalyzer:
    """Analyseur de production unifi√© - Point d'entr√©e principal"""
    
    def __init__(self, config: UnifiedProductionConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.UnifiedProductionAnalyzer")
        
        # Composants principaux
        self.llm_service = UnifiedLLMService(config)
        self.trace_analyzer = TraceAnalyzerV2(config)
        self.dependency_validator = DependencyValidator(config)
        
        # √âtat
        self.initialized = False
        self.analysis_results = []
        self.session_id = None
        
    async def initialize(self) -> bool:
        """Initialise tous les composants du syst√®me"""
        try:
            self.logger.info("üöÄ Initialisation Unified Production Analyzer...")
            
            # G√©n√©ration ID de session
            self.session_id = f"upa_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.logger.info(f"üìã Session ID: {self.session_id}")
            
            # Validation configuration
            config_valid, config_errors = self.config.validate()
            if not config_valid:
                self.logger.error("‚ùå Configuration invalide:")
                for error in config_errors:
                    self.logger.error(f"  - {error}")
                return False
            
            # Validation d√©pendances si activ√©e
            if self.config.check_dependencies:
                deps_valid, deps_errors = await self.dependency_validator.validate_all()
                if not deps_valid:
                    self.logger.error("‚ùå D√©pendances invalides:")
                    for error in deps_errors:
                        self.logger.error(f"  - {error}")
                    return False
            
            # Initialisation des composants
            if not await self.llm_service.initialize():
                return False
                
            if not await self.trace_analyzer.initialize():
                return False
            
            self.initialized = True
            self.logger.info("‚úÖ Unified Production Analyzer initialis√©")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation: {e}")
            self.logger.debug(traceback.format_exc())
            return False
    
    async def analyze_text(self, text: str, analysis_modes: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Analyse un texte selon les modes sp√©cifi√©s
        
        Args:
            text: Texte √† analyser
            analysis_modes: Modes d'analyse (par d√©faut: configuration)
            
        Returns:
            R√©sultats d'analyse structur√©s
        """
        if not self.initialized:
            raise RuntimeError("Analyseur non initialis√©")
        
        start_time = time.time()
        analysis_id = f"analysis_{len(self.analysis_results) + 1}"
        
        self.logger.info(f"üîç D√©marrage analyse: {analysis_id}")
        self.logger.debug(f"Texte: {len(text)} caract√®res")
        
        # D√©termination des modes d'analyse
        modes = analysis_modes or [mode.value for mode in self.config.analysis_modes]
        self.logger.debug(f"Modes: {modes}")
        
        try:
            results = {}
            
            # Analyse selon chaque mode
            for mode in modes:
                self.logger.debug(f"Mode: {mode}")
                
                # Capture conversation si activ√©e
                if self.config.enable_conversation_trace:
                    await self.trace_analyzer.capture_conversation(
                        "analyzer", f"Starting {mode} analysis", f"Processing {len(text)} chars"
                    )
                
                # Analyse LLM
                mode_result = await self.llm_service.analyze_text(text, mode)
                results[mode] = mode_result
                
                self.logger.debug(f"‚úÖ Mode {mode} termin√©")
            
            # Synth√®se des r√©sultats
            analysis_result = {
                "id": analysis_id,
                "session_id": self.session_id,
                "timestamp": datetime.now().isoformat(),
                "text_length": len(text),
                "modes_analyzed": modes,
                "results": results,
                "execution_time": time.time() - start_time,
                "config_snapshot": {
                    "logic_type": self.config.logic_type.value,
                    "mock_level": self.config.mock_level.value,
                    "orchestration_type": self.config.orchestration_type.value
                }
            }
            
            # Conversation summary si activ√©
            if self.config.enable_conversation_trace:
                analysis_result["conversation_summary"] = self.trace_analyzer.get_conversation_summary()
            
            # Stockage r√©sultat
            self.analysis_results.append(analysis_result)
            
            self.logger.info(f"‚úÖ Analyse {analysis_id} termin√©e en {analysis_result['execution_time']:.2f}s")
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur analyse {analysis_id}: {e}")
            self.logger.debug(traceback.format_exc())
            
            # R√©sultat d'erreur
            error_result = {
                "id": analysis_id,
                "session_id": self.session_id,
                "timestamp": datetime.now().isoformat(),
                "error": str(e),
                "execution_time": time.time() - start_time
            }
            
            self.analysis_results.append(error_result)
            raise
    
    def _map_orchestration_mode(self):
        """Mappe le type d'orchestration vers les modes du pipeline unifi√©"""
        mapping = {
            OrchestrationType.UNIFIED: "pipeline",
            OrchestrationType.CONVERSATION: "conversation",
            OrchestrationType.MICRO: "operational_direct",
            OrchestrationType.REAL_LLM: "hierarchical_full"
        }
        
        # Cr√©er un objet avec attribut value
        class ModeResult:
            def __init__(self, value):
                self.value = value
        
        return ModeResult(mapping.get(self.config.orchestration_type, "pipeline"))
    
    def _map_analysis_type(self, analysis_type: str):
        """Mappe les types d'analyse vers les modes unifi√©s"""
        mapping = {
            "rhetorical": "rhetorical",
            "fallacies": "fallacy_focused",
            "coherence": "argument_structure",
            "semantic": "comprehensive",
            "unified": "rhetorical",
            "advanced": "comprehensive"
        }
        
        # Cr√©er un objet avec attribut value
        class AnalysisResult:
            def __init__(self, value):
                self.value = value
        
        return AnalysisResult(mapping.get(analysis_type, "rhetorical"))
    
    def _build_config(self, analysis_type: str):
        """Construit la configuration unifi√©e pour le pipeline"""
        # Cr√©er un objet configuration compatible
        class UnifiedConfig:
            def __init__(self, config, analyzer):
                self.analysis_modes = [mode.value for mode in config.analysis_modes]
                self.orchestration_mode = analyzer._map_orchestration_mode().value
                self.enable_hierarchical = config.orchestration_type in [
                    OrchestrationType.UNIFIED, OrchestrationType.REAL_LLM
                ]
                self.logic_type = config.logic_type.value
                self.mock_level = config.mock_level.value
                
                # Attributs additionnels attendus par les tests
                self.enable_specialized_orchestrators = True
                self.enable_communication_middleware = True
                self.save_orchestration_trace = config.save_orchestration_trace
                
        return UnifiedConfig(self.config, self)
    
    def generate_report(self, output_file: Path) -> Dict[str, Any]:
        """G√©n√®re un rapport final des analyses"""
        
        # Calculs statistiques
        successful_analyses = len([r for r in self.analysis_results if 'error' not in r])
        failed_analyses = len([r for r in self.analysis_results if 'error' in r])
        total_time = sum(r.get('execution_time', 0) for r in self.analysis_results)
        avg_time = total_time / len(self.analysis_results) if self.analysis_results else 0
        
        report = {
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat(),
            "results_summary": {
                "total_analyses": len(self.analysis_results),
                "successful_analyses": successful_analyses,
                "failed_analyses": failed_analyses,
                "total_execution_time": total_time,
                "average_execution_time": avg_time
            },
            "configuration": {
                "logic_type": self.config.logic_type.value,
                "mock_level": self.config.mock_level.value,
                "orchestration_type": self.config.orchestration_type.value,
                "analysis_modes": [mode.value for mode in self.config.analysis_modes]
            },
            "detailed_results": self.analysis_results
        }
        
        # Sauvegarde du rapport
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"üìä Rapport sauvegard√©: {output_file}")
        return report

    async def analyze_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """Analyse un lot de textes en parall√®le ou s√©quentiel"""
        self.logger.info(f"üì¶ Analyse batch: {len(texts)} textes")
        
        if self.config.enable_parallel:
            return await self._analyze_batch_parallel(texts)
        else:
            return await self._analyze_batch_sequential(texts)
    
    async def _analyze_batch_parallel(self, texts: List[str]) -> List[Dict[str, Any]]:
        """Analyse en parall√®le avec contr√¥le de concurrence"""
        semaphore = asyncio.Semaphore(self.config.max_workers)
        
        async def analyze_with_semaphore(text):
            async with semaphore:
                return await self.analyze_text(text)
        
        tasks = [analyze_with_semaphore(text) for text in texts]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Conversion des exceptions en r√©sultats d'erreur
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "id": f"batch_error_{i}",
                    "error": str(result),
                    "text_index": i
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _analyze_batch_sequential(self, texts: List[str]) -> List[Dict[str, Any]]:
        """Analyse s√©quentielle avec barre de progression"""
        results = []
        
        for i, text in enumerate(texts):
            self.logger.info(f"üìÑ Texte {i+1}/{len(texts)}")
            try:
                result = await self.analyze_text(text)
                results.append(result)
            except Exception as e:
                self.logger.error(f"‚ùå Erreur texte {i+1}: {e}")
                results.append({
                    "id": f"sequential_error_{i}",
                    "error": str(e),
                    "text_index": i
                })
        
        return results
    
    def generate_report(self, output_path: Optional[Path] = None) -> Dict[str, Any]:
        """G√©n√®re un rapport complet de session"""
        
        report = {
            "session_info": {
                "id": self.session_id,
                "timestamp": datetime.now().isoformat(),
                "total_analyses": len(self.analysis_results),
                "config": {
                    "logic_type": self.config.logic_type.value,
                    "mock_level": self.config.mock_level.value,
                    "orchestration_type": self.config.orchestration_type.value,
                    "analysis_modes": [mode.value for mode in self.config.analysis_modes]
                }
            },
            "results_summary": {
                "successful_analyses": len([r for r in self.analysis_results if "error" not in r]),
                "failed_analyses": len([r for r in self.analysis_results if "error" in r]),
                "total_execution_time": sum(r.get("execution_time", 0) for r in self.analysis_results),
                "average_execution_time": None
            },
            "detailed_results": self.analysis_results
        }
        
        # Calcul moyenne si analyses r√©ussies
        successful_count = report["results_summary"]["successful_analyses"]
        if successful_count > 0:
            report["results_summary"]["average_execution_time"] = \
                report["results_summary"]["total_execution_time"] / successful_count
        
        # Sauvegarde si chemin sp√©cifi√©
        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.logger.info(f"üìÑ Rapport sauvegard√©: {output_path}")
        
        return report


def create_cli_parser() -> argparse.ArgumentParser:
    """Cr√©e le parser CLI avec tous les param√®tres essentiels"""
    
    parser = argparse.ArgumentParser(
        description="Unified Production Analyzer - Analyse rh√©torique en production",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # === Arguments principaux ===
    parser.add_argument(
        "input",
        nargs="?",
        help="Texte √† analyser ou chemin vers fichier/dossier"
    )
    
    parser.add_argument(
        "--batch",
        action="store_true",
        help="Mode batch pour analyse multiple"
    )
    
    # === Configuration LLM ===
    llm_group = parser.add_argument_group("Configuration LLM")
    llm_group.add_argument(
        "--llm-service",
        choices=["openai", "mock"],
        default="openai",
        help="Service LLM √† utiliser"
    )
    llm_group.add_argument(
        "--llm-model",
        default="gpt-4",
        help="Mod√®le LLM sp√©cifique"
    )
    llm_group.add_argument(
        "--llm-temperature",
        type=float,
        default=0.3,
        help="Temp√©rature du mod√®le LLM"
    )
    llm_group.add_argument(
        "--llm-max-tokens",
        type=int,
        default=2000,
        help="Nombre maximum de tokens"
    )
    
    # === Configuration Logique ===
    logic_group = parser.add_argument_group("Configuration Logique")
    logic_group.add_argument(
        "--logic-type",
        choices=["fol", "propositional", "modal"],
        default="fol",
        help="Type de logique √† utiliser"
    )
    logic_group.add_argument(
        "--enable-fallback",
        action="store_true",
        default=True,
        help="Activer fallback automatique FOL->PL"
    )
    logic_group.add_argument(
        "--no-fallback",
        action="store_false",
        dest="enable_fallback",
        help="D√©sactiver fallback automatique"
    )
    
    # === Configuration Authenticit√© ===
    auth_group = parser.add_argument_group("Configuration Authenticit√©")
    auth_group.add_argument(
        "--mock-level",
        choices=["none", "partial", "full"],
        default="none",
        help="Niveau de simulation (none=100%% authentique)"
    )
    auth_group.add_argument(
        "--require-real-gpt",
        action="store_true",
        default=True,
        help="Exiger un LLM authentique"
    )
    auth_group.add_argument(
        "--allow-mock-gpt",
        action="store_false",
        dest="require_real_gpt",
        help="Autoriser LLM simul√©"
    )
    auth_group.add_argument(
        "--require-real-tweety",
        action="store_true",
        default=True,
        help="Exiger TweetyProject authentique"
    )
    auth_group.add_argument(
        "--allow-mock-tweety",
        action="store_false", 
        dest="require_real_tweety",
        help="Autoriser TweetyProject simul√©"
    )
    
    # === Configuration Orchestration ===
    orch_group = parser.add_argument_group("Configuration Orchestration")
    orch_group.add_argument(
        "--orchestration-type",
        choices=["unified", "conversation", "micro", "real_llm"],
        default="unified",
        help="Type d'orchestration multi-agents"
    )
    orch_group.add_argument(
        "--enable-conversation-trace",
        action="store_true",
        default=True,
        help="Activer capture conversation agentielle"
    )
    orch_group.add_argument(
        "--no-conversation-trace",
        action="store_false",
        dest="enable_conversation_trace",
        help="D√©sactiver capture conversation"
    )
    orch_group.add_argument(
        "--max-agents",
        type=int,
        default=5,
        help="Nombre maximum d'agents simultan√©s"
    )
    
    # === Configuration Analyse ===
    analysis_group = parser.add_argument_group("Configuration Analyse")
    analysis_group.add_argument(
        "--analysis-modes",
        nargs="+",
        choices=["fallacies", "coherence", "semantic", "unified", "advanced"],
        default=["unified"],
        help="Modes d'analyse √† appliquer"
    )
    analysis_group.add_argument(
        "--enable-parallel",
        action="store_true",
        default=True,
        help="Activer traitement parall√®le"
    )
    analysis_group.add_argument(
        "--no-parallel",
        action="store_false",
        dest="enable_parallel",
        help="Forcer traitement s√©quentiel"
    )
    analysis_group.add_argument(
        "--max-workers",
        type=int,
        default=4,
        help="Nombre maximum de workers parall√®les"
    )
    
    # === Configuration Retry ===
    retry_group = parser.add_argument_group("Configuration Retry")
    retry_group.add_argument(
        "--tweety-retry-count",
        type=int,
        default=5,
        help="Nombre de tentatives TweetyProject"
    )
    retry_group.add_argument(
        "--tweety-retry-delay",
        type=float,
        default=2.0,
        help="D√©lai entre tentatives TweetyProject"
    )
    retry_group.add_argument(
        "--llm-retry-count",
        type=int,
        default=3,
        help="Nombre de tentatives LLM"
    )
    
    # === Configuration Sortie ===
    output_group = parser.add_argument_group("Configuration Sortie")
    output_group.add_argument(
        "--output-format",
        choices=["json", "yaml", "txt"],
        default="json",
        help="Format de sortie des r√©sultats"
    )
    output_group.add_argument(
        "--output-file",
        type=Path,
        help="Fichier de sortie (d√©faut: auto-g√©n√©r√©)"
    )
    output_group.add_argument(
        "--save-intermediate",
        action="store_true",
        help="Sauvegarder r√©sultats interm√©diaires"
    )
    output_group.add_argument(
        "--report-level",
        choices=["minimal", "standard", "production", "debug"],
        default="production",
        help="Niveau de d√©tail des rapports"
    )
    
    # === Configuration Validation ===
    validation_group = parser.add_argument_group("Configuration Validation")
    validation_group.add_argument(
        "--validate-inputs",
        action="store_true",
        default=True,
        help="Valider les entr√©es"
    )
    validation_group.add_argument(
        "--no-validate-inputs",
        action="store_false",
        dest="validate_inputs",
        help="D√©sactiver validation entr√©es"
    )
    validation_group.add_argument(
        "--check-dependencies",
        action="store_true",
        default=True,
        help="V√©rifier les d√©pendances au d√©marrage"
    )
    validation_group.add_argument(
        "--no-check-dependencies",
        action="store_false",
        dest="check_dependencies",
        help="Ignorer v√©rification d√©pendances"
    )
    
    # === Options g√©n√©rales ===
    general_group = parser.add_argument_group("Options G√©n√©rales")
    general_group.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Mode verbeux"
    )
    general_group.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="Mode silencieux"
    )
    general_group.add_argument(
        "--config-file",
        type=Path,
        help="Fichier de configuration JSON"
    )
    general_group.add_argument(
        "--version",
        action="version",
        version="Unified Production Analyzer 1.0.0"
    )
    
    return parser


def load_config_from_file(config_path: Path) -> Dict[str, Any]:
    """Charge la configuration depuis un fichier JSON"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"‚ùå Erreur chargement configuration {config_path}: {e}")
        return {}


def create_config_from_args(args) -> UnifiedProductionConfig:
    """Cr√©e la configuration depuis les arguments CLI"""
    
    # Chargement config fichier si sp√©cifi√©
    file_config = {}
    if args.config_file and args.config_file.exists():
        file_config = load_config_from_file(args.config_file)
        logger.info(f"üìÑ Configuration charg√©e depuis {args.config_file}")
    
    # Mapping des enums
    logic_type_map = {
        "fol": LogicType.FOL,
        "propositional": LogicType.PL,
        "modal": LogicType.MODAL
    }
    
    mock_level_map = {
        "none": MockLevel.NONE,
        "partial": MockLevel.PARTIAL,
        "full": MockLevel.FULL
    }
    
    orchestration_type_map = {
        "unified": OrchestrationType.UNIFIED,
        "conversation": OrchestrationType.CONVERSATION,
        "micro": OrchestrationType.MICRO,
        "real_llm": OrchestrationType.REAL_LLM
    }
    
    analysis_modes_map = {
        "fallacies": AnalysisMode.FALLACIES,
        "coherence": AnalysisMode.COHERENCE,
        "semantic": AnalysisMode.SEMANTIC,
        "unified": AnalysisMode.UNIFIED,
        "advanced": AnalysisMode.ADVANCED
    }
    
    # Construction configuration (CLI prioritaire sur fichier)
    config_dict = {
        # LLM
        "llm_service": getattr(args, "llm_service", file_config.get("llm_service", "openai")),
        "llm_model": getattr(args, "llm_model", file_config.get("llm_model", "gpt-4")),
        "llm_temperature": getattr(args, "llm_temperature", file_config.get("llm_temperature", 0.3)),
        "llm_max_tokens": getattr(args, "llm_max_tokens", file_config.get("llm_max_tokens", 2000)),
        "llm_retry_count": getattr(args, "llm_retry_count", file_config.get("llm_retry_count", 3)),
        
        # Logique
        "logic_type": logic_type_map.get(
            getattr(args, "logic_type", file_config.get("logic_type", "fol")),
            LogicType.FOL
        ),
        "enable_fallback": getattr(args, "enable_fallback", file_config.get("enable_fallback", True)),
        
        # Authenticit√©
        "mock_level": mock_level_map.get(
            getattr(args, "mock_level", file_config.get("mock_level", "none")),
            MockLevel.NONE
        ),
        "require_real_gpt": getattr(args, "require_real_gpt", file_config.get("require_real_gpt", True)),
        "require_real_tweety": getattr(args, "require_real_tweety", file_config.get("require_real_tweety", True)),
        
        # Orchestration
        "orchestration_type": orchestration_type_map.get(
            getattr(args, "orchestration_type", file_config.get("orchestration_type", "unified")),
            OrchestrationType.UNIFIED
        ),
        "enable_conversation_trace": getattr(args, "enable_conversation_trace", 
                                           file_config.get("enable_conversation_trace", True)),
        "max_agents": getattr(args, "max_agents", file_config.get("max_agents", 5)),
        
        # Analyse
        "analysis_modes": [
            analysis_modes_map.get(mode, AnalysisMode.UNIFIED) 
            for mode in getattr(args, "analysis_modes", file_config.get("analysis_modes", ["unified"]))
        ],
        "enable_parallel": getattr(args, "enable_parallel", file_config.get("enable_parallel", True)),
        "max_workers": getattr(args, "max_workers", file_config.get("max_workers", 4)),
        
        # Retry
        "tweety_retry_count": getattr(args, "tweety_retry_count", file_config.get("tweety_retry_count", 5)),
        "tweety_retry_delay": getattr(args, "tweety_retry_delay", file_config.get("tweety_retry_delay", 2.0)),
        
        # Sortie
        "output_format": getattr(args, "output_format", file_config.get("output_format", "json")),
        "save_intermediate": getattr(args, "save_intermediate", file_config.get("save_intermediate", False)),
        "report_level": getattr(args, "report_level", file_config.get("report_level", "production")),
        
        # Validation
        "validate_inputs": getattr(args, "validate_inputs", file_config.get("validate_inputs", True)),
        "check_dependencies": getattr(args, "check_dependencies", file_config.get("check_dependencies", True))
    }
    
    return UnifiedProductionConfig(**config_dict)


async def main():
    """Fonction principale du script"""
    
    # Configuration logging selon verbosit√©
    parser = create_cli_parser()
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    elif args.quiet:
        logging.getLogger().setLevel(logging.WARNING)
    
    logger.info("üöÄ Unified Production Analyzer v1.0.0")
    logger.info("=" * 60)
    
    try:
        # Cr√©ation configuration
        config = create_config_from_args(args)
        logger.info(f"‚öôÔ∏è Configuration: {config.logic_type.value}/{config.mock_level.value}")
        
        # Validation authenticit√© mode production
        if config.mock_level == MockLevel.NONE:
            logger.info("üõ°Ô∏è Mode production authentique activ√© (0% mocks)")
        else:
            logger.warning(f"‚ö†Ô∏è Mode {config.mock_level.value} - Non recommand√© en production")
        
        # Initialisation analyseur
        analyzer = UnifiedProductionAnalyzer(config)
        
        if not await analyzer.initialize():
            logger.error("‚ùå √âchec initialisation - Arr√™t")
            sys.exit(1)
        
        # D√©termination de l'entr√©e
        if not args.input:
            logger.error("‚ùå Aucune entr√©e sp√©cifi√©e")
            parser.print_help()
            sys.exit(1)
        
        input_path = Path(args.input)
        
        # Traitement selon le type d'entr√©e
        if input_path.exists():
            if input_path.is_file():
                # Fichier unique
                logger.info(f"üìÑ Analyse fichier: {input_path}")
                with open(input_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                result = await analyzer.analyze_text(text, args.analysis_modes)
                logger.info(f"‚úÖ Analyse termin√©e: {result['id']}")
                
            elif input_path.is_dir() and args.batch:
                # Dossier en mode batch
                logger.info(f"üìÅ Analyse dossier: {input_path}")
                
                text_files = list(input_path.glob("*.txt")) + list(input_path.glob("*.md"))
                texts = []
                
                for file_path in text_files:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        texts.append(f.read())
                
                results = await analyzer.analyze_batch(texts)
                logger.info(f"‚úÖ Batch termin√©: {len(results)} analyses")
                
            else:
                logger.error("‚ùå Dossier sp√©cifi√© mais mode batch non activ√© (--batch)")
                sys.exit(1)
        else:
            # Texte direct
            logger.info("üìù Analyse texte direct")
            result = await analyzer.analyze_text(args.input, args.analysis_modes)
            logger.info(f"‚úÖ Analyse termin√©e: {result['id']}")
        
        # G√©n√©ration rapport final
        output_file = args.output_file
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = Path(f"reports/unified_analysis_{timestamp}.json")
        
        report = analyzer.generate_report(output_file)
        
        # R√©sum√© final
        logger.info("=" * 60)
        logger.info("üìä R√âSUM√â FINAL")
        logger.info(f"Session: {analyzer.session_id}")
        logger.info(f"Analyses r√©ussies: {report['results_summary']['successful_analyses']}")
        logger.info(f"Analyses √©chou√©es: {report['results_summary']['failed_analyses']}")
        logger.info(f"Temps total: {report['results_summary']['total_execution_time']:.2f}s")
        
        if report['results_summary']['average_execution_time']:
            logger.info(f"Temps moyen: {report['results_summary']['average_execution_time']:.2f}s")
        
        logger.info(f"Rapport: {output_file}")
        logger.info("üéâ Analyse termin√©e avec succ√®s!")
        
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è Interruption utilisateur")
        sys.exit(130)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        logger.debug(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    # Gestion de l'event loop selon la plateforme
    try:
        asyncio.run(main())
    except Exception as e:
        logger.error(f"‚ùå Erreur event loop: {e}")
        sys.exit(1)