#!/usr/bin/env python3
"""
ANALYSE DE LA TRACE ACTUELLE SHERLOCK-WATSON-MORIARTY

Script d'analyse pour √©valuer la qualit√© des conversations entre les 3 agents
et identifier les axes d'am√©lioration vers une "trace id√©ale".

Objectifs:
1. Ex√©cuter une session compl√®te avec workflow 3-agents
2. Capturer int√©gralement la trace conversationnelle
3. Analyser la qualit√© sur plusieurs dimensions
4. D√©finir les crit√®res de "trace id√©ale"
5. Proposer un plan d'optimisation incr√©mentale
"""

import asyncio
import json
import logging
import sys
import traceback
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict

# Imports de l'infrastructure Oracle
from argumentation_analysis.orchestration.cluedo_extended_orchestrator import (
    CluedoExtendedOrchestrator, run_cluedo_oracle_game
)
from argumentation_analysis.core.cluedo_oracle_state import CluedoOracleState
from semantic_kernel import Kernel

# Configuration du logging pour une trace d√©taill√©e
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('trace_analysis.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

# Forcer l'encodage UTF-8 pour √©viter les probl√®mes avec les emojis
import os
os.environ['PYTHONIOENCODING'] = 'utf-8'
logger = logging.getLogger(__name__)


@dataclass
class ConversationalMetrics:
    """M√©triques d'√©valuation de la qualit√© conversationnelle."""
    naturalite_score: float = 0.0
    pertinence_score: float = 0.0
    progression_logique_score: float = 0.0
    personnalite_distincte_score: float = 0.0
    fluidite_transitions_score: float = 0.0
    dosage_revelations_score: float = 0.0
    satisfaction_resolution_score: float = 0.0
    
    def moyenne_globale(self) -> float:
        """Calcule la moyenne globale des scores."""
        scores = [
            self.naturalite_score, self.pertinence_score, self.progression_logique_score,
            self.personnalite_distincte_score, self.fluidite_transitions_score,
            self.dosage_revelations_score, self.satisfaction_resolution_score
        ]
        return sum(scores) / len(scores)


@dataclass
class ConversationAnalysis:
    """Analyse compl√®te d'une conversation entre agents."""
    total_messages: int = 0
    messages_par_agent: Dict[str, int] = None
    longueur_moyenne_messages: Dict[str, float] = None
    mots_cles_detectes: Dict[str, List[str]] = None
    transitions_abruptes: List[Dict[str, Any]] = None
    repetitions_detectees: List[Dict[str, Any]] = None
    progression_deductive: List[Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.messages_par_agent is None:
            self.messages_par_agent = {}
        if self.longueur_moyenne_messages is None:
            self.longueur_moyenne_messages = {}
        if self.mots_cles_detectes is None:
            self.mots_cles_detectes = {}
        if self.transitions_abruptes is None:
            self.transitions_abruptes = []
        if self.repetitions_detectees is None:
            self.repetitions_detectees = []
        if self.progression_deductive is None:
            self.progression_deductive = []


class TraceQualityAnalyzer:
    """
    Analyseur de qualit√© des traces conversationnelles pour le workflow 3-agents.
    """
    
    def __init__(self):
        self.conversation_history: List[Dict[str, Any]] = []
        self.agent_personalities = {
            "Sherlock": ["m√©thodique", "d√©ductif", "leader", "incisif", "observateur"],
            "Watson": ["logique", "analytique", "assistant", "rigoureux", "technique"],
            "Moriarty": ["myst√©rieux", "r√©v√©lateur", "strat√©gique", "manipulateur", "oracle"]
        }
        self.conversation_patterns = {
            "suggestions": [],
            "validations": [], 
            "revelations": [],
            "transitions": []
        }
    
    def capture_conversation(self, workflow_result: Dict[str, Any]) -> None:
        """Capture et structure l'historique conversationnel."""
        logger.info("üîç Capture de l'historique conversationnel...")
        
        self.conversation_history = workflow_result.get("conversation_history", [])
        self.oracle_stats = workflow_result.get("oracle_statistics", {})
        self.solution_analysis = workflow_result.get("solution_analysis", {})
        
        logger.info(f"‚úÖ {len(self.conversation_history)} messages captur√©s")
        
        # Analyse des patterns conversationnels
        self._extract_conversation_patterns()
    
    def _extract_conversation_patterns(self) -> None:
        """Extrait les patterns conversationnels cl√©s."""
        logger.info("üîç Extraction des patterns conversationnels...")
        
        for i, message in enumerate(self.conversation_history):
            sender = message.get("sender", "Unknown")
            content = message.get("message", "").lower()
            
            # D√©tection des suggestions
            if "sugg√©r" in content or "suspect" in content and "arme" in content:
                self.conversation_patterns["suggestions"].append({
                    "index": i,
                    "sender": sender,
                    "content_preview": message.get("message", "")[:100]
                })
            
            # D√©tection des validations logiques
            if "valide" in content or "logique" in content or "formula" in content:
                self.conversation_patterns["validations"].append({
                    "index": i,
                    "sender": sender,
                    "content_preview": message.get("message", "")[:100]
                })
            
            # D√©tection des r√©v√©lations Oracle
            if "r√©v√®le" in content or "carte" in content or "r√©futation" in content:
                self.conversation_patterns["revelations"].append({
                    "index": i,
                    "sender": sender,
                    "content_preview": message.get("message", "")[:100]
                })
            
            # D√©tection des transitions abruptes
            if i > 0:
                prev_sender = self.conversation_history[i-1].get("sender", "")
                if sender != prev_sender:
                    self.conversation_patterns["transitions"].append({
                        "from": prev_sender,
                        "to": sender,
                        "index": i,
                        "prev_content": self.conversation_history[i-1].get("message", "")[:50],
                        "curr_content": message.get("message", "")[:50]
                    })
    
    def analyze_conversational_quality(self) -> ConversationalMetrics:
        """Analyse la qualit√© conversationnelle selon les crit√®res d√©finis."""
        logger.info("üìä Analyse de la qualit√© conversationnelle...")
        
        metrics = ConversationalMetrics()
        
        # 1. Naturalit√© du dialogue
        metrics.naturalite_score = self._evaluate_naturalness()
        
        # 2. Pertinence des interventions
        metrics.pertinence_score = self._evaluate_relevance()
        
        # 3. Logique de progression
        metrics.progression_logique_score = self._evaluate_logical_progression()
        
        # 4. Personnalit√©s distinctes
        metrics.personnalite_distincte_score = self._evaluate_personality_distinction()
        
        # 5. Fluidit√© des transitions
        metrics.fluidite_transitions_score = self._evaluate_transition_fluidity()
        
        # 6. Dosage des r√©v√©lations Moriarty
        metrics.dosage_revelations_score = self._evaluate_revelation_dosage()
        
        # 7. Satisfaction de la r√©solution
        metrics.satisfaction_resolution_score = self._evaluate_resolution_satisfaction()
        
        logger.info(f"üìä Score global: {metrics.moyenne_globale():.2f}/10")
        return metrics
    
    def _evaluate_naturalness(self) -> float:
        """√âvalue la naturalit√© du dialogue (0-10)."""
        score = 5.0  # Score de base
        
        # P√©nalit√©s pour r√©p√©titions
        repetitions = self._detect_repetitions()
        score -= min(len(repetitions) * 0.5, 2.0)
        
        # Bonus pour vari√©t√© du vocabulaire
        unique_words = self._count_unique_words()
        if unique_words > 100:
            score += 1.0
        
        # Bonus pour longueur appropri√©e des messages
        avg_lengths = self._calculate_average_message_lengths()
        if all(20 <= length <= 200 for length in avg_lengths.values()):
            score += 1.0
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_relevance(self) -> float:
        """√âvalue la pertinence des interventions (0-10)."""
        score = 5.0
        
        # V√©rifier que Sherlock m√®ne l'enqu√™te
        sherlock_suggestions = len([p for p in self.conversation_patterns["suggestions"] 
                                  if p["sender"] == "Sherlock"])
        total_suggestions = len(self.conversation_patterns["suggestions"])
        
        if total_suggestions > 0:
            sherlock_leadership = sherlock_suggestions / total_suggestions
            score += sherlock_leadership * 3.0  # Max +3 si Sherlock fait toutes les suggestions
        
        # V√©rifier que Watson fait de la logique
        watson_logic = len([p for p in self.conversation_patterns["validations"] 
                          if p["sender"] == "Watson"])
        if watson_logic > 0:
            score += 1.0
        
        # V√©rifier que Moriarty r√©v√®le des cartes
        moriarty_revelations = len([p for p in self.conversation_patterns["revelations"] 
                                  if p["sender"] == "Moriarty"])
        if moriarty_revelations > 0:
            score += 1.0
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_logical_progression(self) -> float:
        """√âvalue la logique de progression de l'enqu√™te (0-10)."""
        score = 5.0
        
        # V√©rifier que l'enqu√™te progresse vers la solution
        if self.solution_analysis.get("success", False):
            score += 3.0
        elif self.solution_analysis.get("proposed_solution"):
            score += 1.5  # Tentative m√™me si incorrecte
        
        # V√©rifier l'utilisation progressive des r√©v√©lations
        revelations_count = len(self.conversation_patterns["revelations"])
        if 2 <= revelations_count <= 5:  # Nombre optimal de r√©v√©lations
            score += 1.0
        
        # V√©rifier la structure cyclique Sherlock->Watson->Moriarty
        cycle_adherence = self._check_cycle_adherence()
        score += cycle_adherence * 1.0
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_personality_distinction(self) -> float:
        """√âvalue la distinction des personnalit√©s (0-10)."""
        score = 5.0
        
        # Analyser les mots-cl√©s par agent
        agent_keywords = self._extract_agent_keywords()
        
        for agent, keywords in agent_keywords.items():
            expected_keywords = self.agent_personalities.get(agent, [])
            matching_keywords = sum(1 for kw in expected_keywords 
                                  if any(kw in keyword.lower() for keyword in keywords))
            if matching_keywords > 0:
                score += 0.5  # Bonus par agent avec personnalit√© distincte
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_transition_fluidity(self) -> float:
        """√âvalue la fluidit√© des transitions entre agents (0-10)."""
        score = 8.0  # Score de base √©lev√© (transitions g√©n√©ralement fluides)
        
        # Analyser les transitions abruptes
        abrupt_transitions = 0
        for transition in self.conversation_patterns["transitions"]:
            # Transition abrupte si pas de r√©f√©rence au message pr√©c√©dent
            curr_content = transition["curr_content"].lower()
            prev_content = transition["prev_content"].lower()
            
            # Simple heuristique: chercher des mots de liaison
            liaison_words = ["donc", "ainsi", "cependant", "de plus", "en effet", "par ailleurs"]
            has_liaison = any(word in curr_content for word in liaison_words)
            
            if not has_liaison and len(curr_content) > 20:
                abrupt_transitions += 1
        
        # P√©nalit√© pour transitions abruptes
        score -= abrupt_transitions * 0.5
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_revelation_dosage(self) -> float:
        """√âvalue le dosage des r√©v√©lations Moriarty (0-10)."""
        score = 5.0
        
        revelations = self.conversation_patterns["revelations"]
        total_messages = len(self.conversation_history)
        
        if total_messages > 0:
            revelation_density = len(revelations) / total_messages
            
            # Densit√© optimale: 10-30% des messages
            if 0.1 <= revelation_density <= 0.3:
                score += 3.0
            elif 0.05 <= revelation_density <= 0.5:
                score += 1.5
            else:
                score -= 1.0
        
        # V√©rifier que les r√©v√©lations sont bien r√©parties
        if len(revelations) >= 2:
            # Calcul de la variance des positions
            positions = [r["index"] for r in revelations]
            if self._is_well_distributed(positions, total_messages):
                score += 2.0
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_resolution_satisfaction(self) -> float:
        """√âvalue la satisfaction de la r√©solution (0-10)."""
        score = 3.0  # Score de base
        
        # Bonus si solution trouv√©e
        if self.solution_analysis.get("success", False):
            score += 5.0
        elif self.solution_analysis.get("proposed_solution"):
            score += 2.0  # Tentative
        
        # Bonus si r√©solution logique et coh√©rente
        oracle_interactions = self.oracle_stats.get("workflow_metrics", {}).get("oracle_interactions", 0)
        if oracle_interactions >= 3:  # Interactions suffisantes
            score += 1.0
        
        # Bonus pour efficacit√© (r√©solution en moins de 10 tours)
        total_turns = self.oracle_stats.get("agent_interactions", {}).get("total_turns", 0)
        if total_turns <= 10:
            score += 1.0
        
        return max(0.0, min(10.0, score))
    
    # M√©thodes utilitaires d'analyse
    
    def _detect_repetitions(self) -> List[Dict[str, Any]]:
        """D√©tecte les r√©p√©titions dans les messages."""
        repetitions = []
        seen_patterns = {}
        
        for i, message in enumerate(self.conversation_history):
            content = message.get("message", "").lower()
            # Simplifier pour d√©tecter les patterns r√©p√©titifs
            words = content.split()[:10]  # Prendre les 10 premiers mots
            pattern = " ".join(words)
            
            if pattern in seen_patterns:
                repetitions.append({
                    "pattern": pattern,
                    "first_occurrence": seen_patterns[pattern],
                    "repeat_at": i,
                    "sender": message.get("sender", "")
                })
            else:
                seen_patterns[pattern] = i
        
        return repetitions
    
    def _count_unique_words(self) -> int:
        """Compte les mots uniques dans toute la conversation."""
        all_words = set()
        for message in self.conversation_history:
            content = message.get("message", "").lower()
            words = content.split()
            all_words.update(words)
        return len(all_words)
    
    def _calculate_average_message_lengths(self) -> Dict[str, float]:
        """Calcule la longueur moyenne des messages par agent."""
        agent_lengths = {}
        agent_counts = {}
        
        for message in self.conversation_history:
            sender = message.get("sender", "Unknown")
            length = len(message.get("message", ""))
            
            if sender not in agent_lengths:
                agent_lengths[sender] = 0
                agent_counts[sender] = 0
            
            agent_lengths[sender] += length
            agent_counts[sender] += 1
        
        return {agent: agent_lengths[agent] / agent_counts[agent] 
                for agent in agent_lengths if agent_counts[agent] > 0}
    
    def _check_cycle_adherence(self) -> float:
        """V√©rifie l'adh√©rence au cycle Sherlock->Watson->Moriarty (0-1)."""
        if len(self.conversation_history) < 3:
            return 0.0
        
        expected_cycle = ["Sherlock", "Watson", "Moriarty"]
        cycle_matches = 0
        total_cycles = 0
        
        for i in range(0, len(self.conversation_history) - 2, 3):
            total_cycles += 1
            if (i + 2 < len(self.conversation_history) and
                self.conversation_history[i].get("sender") == expected_cycle[0] and
                self.conversation_history[i + 1].get("sender") == expected_cycle[1] and
                self.conversation_history[i + 2].get("sender") == expected_cycle[2]):
                cycle_matches += 1
        
        return cycle_matches / total_cycles if total_cycles > 0 else 0.0
    
    def _extract_agent_keywords(self) -> Dict[str, List[str]]:
        """Extrait les mots-cl√©s caract√©ristiques par agent."""
        agent_keywords = {}
        
        for message in self.conversation_history:
            sender = message.get("sender", "Unknown")
            content = message.get("message", "").lower()
            
            if sender not in agent_keywords:
                agent_keywords[sender] = []
            
            # Extraire mots significatifs (plus de 4 caract√®res, pas de stop words)
            stop_words = {"avec", "dans", "pour", "vous", "nous", "quel", "cette", "faire"}
            words = [word for word in content.split() 
                    if len(word) > 4 and word not in stop_words]
            agent_keywords[sender].extend(words)
        
        return agent_keywords
    
    def _is_well_distributed(self, positions: List[int], total: int) -> bool:
        """V√©rifie si les positions sont bien distribu√©es."""
        if len(positions) < 2:
            return True
        
        # Calculer les intervalles
        intervals = []
        for i in range(1, len(positions)):
            intervals.append(positions[i] - positions[i-1])
        
        # V√©rifier que la variance des intervalles n'est pas trop √©lev√©e
        mean_interval = sum(intervals) / len(intervals)
        variance = sum((x - mean_interval) ** 2 for x in intervals) / len(intervals)
        
        # Seuil arbitraire pour une bonne distribution
        return variance < (mean_interval ** 2)
    
    def analyze_detailed_conversation(self) -> ConversationAnalysis:
        """Analyse d√©taill√©e de la structure conversationnelle."""
        analysis = ConversationAnalysis()
        
        analysis.total_messages = len(self.conversation_history)
        
        # Messages par agent
        for message in self.conversation_history:
            sender = message.get("sender", "Unknown")
            analysis.messages_par_agent[sender] = analysis.messages_par_agent.get(sender, 0) + 1
        
        # Longueur moyenne des messages
        analysis.longueur_moyenne_messages = self._calculate_average_message_lengths()
        
        # Mots-cl√©s d√©tect√©s
        analysis.mots_cles_detectes = self._extract_agent_keywords()
        
        # Transitions abruptes
        analysis.transitions_abruptes = [
            {
                "de": t["from"],
                "vers": t["to"],
                "position": t["index"],
                "fluidite": "abrupte" if "donc" not in t["curr_content"] else "fluide"
            }
            for t in self.conversation_patterns["transitions"][:5]  # Limiter √† 5
        ]
        
        # R√©p√©titions d√©tect√©es
        analysis.repetitions_detectees = self._detect_repetitions()[:5]  # Limiter √† 5
        
        # Progression d√©ductive
        analysis.progression_deductive = [
            {
                "etape": i + 1,
                "type": "suggestion" if "sugg√©r" in p["content_preview"].lower() else "autre",
                "agent": p["sender"],
                "contenu": p["content_preview"]
            }
            for i, p in enumerate(self.conversation_patterns["suggestions"][:5])
        ]
        
        return analysis
    
    def generate_improvement_recommendations(self, metrics: ConversationalMetrics) -> List[Dict[str, Any]]:
        """G√©n√®re des recommandations d'am√©lioration bas√©es sur l'analyse."""
        recommendations = []
        
        # Naturalit√©
        if metrics.naturalite_score < 6.0:
            recommendations.append({
                "domaine": "Naturalit√© du dialogue",
                "score_actuel": metrics.naturalite_score,
                "probleme": "Dialogue peu naturel, r√©p√©titions ou longueurs inappropri√©es",
                "solutions": [
                    "Enrichir le vocabulaire des prompts agents",
                    "Ajouter de la vari√©t√© dans les expressions",
                    "Calibrer la longueur des r√©ponses"
                ],
                "priorite": "√©lev√©e" if metrics.naturalite_score < 4.0 else "moyenne"
            })
        
        # Pertinence
        if metrics.pertinence_score < 6.0:
            recommendations.append({
                "domaine": "Pertinence des interventions",
                "score_actuel": metrics.pertinence_score,
                "probleme": "Agents ne respectent pas leurs r√¥les d√©finis",
                "solutions": [
                    "Renforcer les prompts de r√¥le sp√©cifiques",
                    "Ajouter des contraintes comportementales",
                    "Am√©liorer la s√©lection cyclique"
                ],
                "priorite": "√©lev√©e"
            })
        
        # Progression logique
        if metrics.progression_logique_score < 6.0:
            recommendations.append({
                "domaine": "Progression logique",
                "score_actuel": metrics.progression_logique_score,
                "probleme": "Enqu√™te ne progresse pas logiquement vers la solution",
                "solutions": [
                    "Am√©liorer la strat√©gie de r√©v√©lation Moriarty",
                    "Ajouter des m√©canismes de progression forc√©e",
                    "Optimiser les crit√®res de terminaison"
                ],
                "priorite": "√©lev√©e"
            })
        
        # Personnalit√©s distinctes
        if metrics.personnalite_distincte_score < 6.0:
            recommendations.append({
                "domaine": "Personnalit√©s distinctes",
                "score_actuel": metrics.personnalite_distincte_score,
                "probleme": "Agents manquent de personnalit√© distinctive",
                "solutions": [
                    "Enrichir les prompts avec des traits de personnalit√©",
                    "Ajouter des expressions caract√©ristiques",
                    "D√©velopper des styles de communication diff√©renci√©s"
                ],
                "priorite": "moyenne"
            })
        
        # Fluidit√© des transitions
        if metrics.fluidite_transitions_score < 6.0:
            recommendations.append({
                "domaine": "Fluidit√© des transitions",
                "score_actuel": metrics.fluidite_transitions_score,
                "probleme": "Transitions abruptes entre agents",
                "solutions": [
                    "Ajouter des mots de liaison dans les prompts",
                    "Impl√©menter une m√©moire contextuelle",
                    "Am√©liorer la passation de tour"
                ],
                "priorite": "moyenne"
            })
        
        # Dosage des r√©v√©lations
        if metrics.dosage_revelations_score < 6.0:
            recommendations.append({
                "domaine": "Dosage des r√©v√©lations Moriarty",
                "score_actuel": metrics.dosage_revelations_score,
                "probleme": "R√©v√©lations mal tim√©es ou mal dos√©es",
                "solutions": [
                    "Calibrer la strat√©gie de r√©v√©lation",
                    "Impl√©menter des r√©v√©lations progressives",
                    "Am√©liorer les crit√®res de pertinence"
                ],
                "priorite": "√©lev√©e"
            })
        
        # Satisfaction de r√©solution
        if metrics.satisfaction_resolution_score < 6.0:
            recommendations.append({
                "domaine": "Satisfaction de la r√©solution",
                "score_actuel": metrics.satisfaction_resolution_score,
                "probleme": "R√©solution insatisfaisante ou incompl√®te",
                "solutions": [
                    "Am√©liorer la logique de terminaison",
                    "Ajouter des v√©rifications de coh√©rence",
                    "Optimiser l'efficacit√© du workflow"
                ],
                "priorite": "√©lev√©e"
            })
        
        return recommendations


async def execute_workflow_analysis():
    """Ex√©cute une session compl√®te et analyse la qualit√© conversationnelle."""
    logger.info("[D√âBUT] ANALYSE DE LA TRACE CLUEDO ORACLE")
    logger.info("=" * 60)
    
    try:
        # 1. Configuration du kernel (simulation)
        logger.info("[CONFIG] Configuration du kernel Semantic...")
        kernel = Kernel()
        # NOTE: En production, configurez ici votre service LLM
        
        # 2. Ex√©cution du workflow 3-agents
        logger.info("üé≠ Lancement du workflow 3-agents...")
        workflow_result = await run_cluedo_oracle_game(
            kernel=kernel,
            initial_question="L'enqu√™te commence. Sherlock, menez l'investigation !",
            max_turns=12,
            max_cycles=4,
            oracle_strategy="balanced"
        )
        
        logger.info("‚úÖ Workflow termin√© avec succ√®s")
        
        # 3. Analyse de la qualit√© conversationnelle
        logger.info("üîç Analyse de la qualit√© conversationnelle...")
        analyzer = TraceQualityAnalyzer()
        analyzer.capture_conversation(workflow_result)
        
        # M√©triques de qualit√©
        metrics = analyzer.analyze_conversational_quality()
        
        # Analyse d√©taill√©e
        detailed_analysis = analyzer.analyze_detailed_conversation()
        
        # Recommandations d'am√©lioration
        recommendations = analyzer.generate_improvement_recommendations(metrics)
        
        # 4. G√©n√©ration du rapport complet
        report = generate_comprehensive_report(
            workflow_result, metrics, detailed_analysis, recommendations
        )
        
        # 5. Sauvegarde des r√©sultats
        save_analysis_results(report)
        
        # 6. Affichage du r√©sum√©
        display_analysis_summary(metrics, recommendations)
        
        logger.info("‚úÖ ANALYSE TERMIN√âE AVEC SUCC√àS")
        return report
        
    except Exception as e:
        logger.error(f"‚ùå Erreur durant l'analyse: {e}")
        traceback.print_exc()
        raise


def generate_comprehensive_report(
    workflow_result: Dict[str, Any],
    metrics: ConversationalMetrics,
    detailed_analysis: ConversationAnalysis,
    recommendations: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """G√©n√®re un rapport complet d'analyse."""
    
    return {
        "metadata": {
            "analyse_timestamp": datetime.now().isoformat(),
            "version_oracle": "1.0.0",
            "workflow_strategy": workflow_result.get("workflow_info", {}).get("strategy", "unknown")
        },
        "trace_complete": {
            "conversation_history": workflow_result.get("conversation_history", []),
            "solution_analysis": workflow_result.get("solution_analysis", {}),
            "oracle_statistics": workflow_result.get("oracle_statistics", {}),
            "final_state": workflow_result.get("final_state", {})
        },
        "analyse_qualitative": {
            "metriques_conversationnelles": asdict(metrics),
            "analyse_detaillee": asdict(detailed_analysis),
            "score_global": metrics.moyenne_globale(),
            "diagnostic": generate_quality_diagnostic(metrics)
        },
        "recommandations_amelioration": recommendations,
        "criteres_trace_ideale": define_ideal_trace_criteria(),
        "plan_optimisation": generate_optimization_plan(recommendations),
        "benchmark_2vs3_agents": compare_with_2agent_system(workflow_result)
    }


def generate_quality_diagnostic(metrics: ConversationalMetrics) -> Dict[str, str]:
    """G√©n√®re un diagnostic de qualit√© bas√© sur les m√©triques."""
    score = metrics.moyenne_globale()
    
    if score >= 8.0:
        level = "EXCELLENT"
        description = "Conversation de tr√®s haute qualit√©, proche de la trace id√©ale"
    elif score >= 6.5:
        level = "BON"
        description = "Conversation de bonne qualit√© avec quelques am√©liorations possibles"
    elif score >= 5.0:
        level = "MOYEN"
        description = "Conversation acceptable mais n√©cessitant des optimisations"
    elif score >= 3.0:
        level = "FAIBLE"
        description = "Conversation de qualit√© insuffisante, optimisations importantes n√©cessaires"
    else:
        level = "TR√àS FAIBLE"
        description = "Conversation de tr√®s mauvaise qualit√©, refonte n√©cessaire"
    
    return {
        "niveau": level,
        "score": f"{score:.2f}/10",
        "description": description,
        "statut": "ACCEPTABLE" if score >= 5.0 else "√Ä AM√âLIORER"
    }


def define_ideal_trace_criteria() -> Dict[str, Any]:
    """D√©finit les crit√®res d'une trace id√©ale."""
    return {
        "dialogue_naturel": {
            "description": "√âchanges fluides et humains entre agents",
            "criteres": [
                "Vari√©t√© du vocabulaire (>150 mots uniques)",
                "Longueur appropri√©e des messages (50-150 mots)",
                "Absence de r√©p√©titions (< 5% des messages)",
                "Expressions naturelles et vari√©es"
            ],
            "score_cible": 8.0
        },
        "personnalites_distinctes": {
            "description": "Chaque agent a une personnalit√© unique et reconnaissable",
            "criteres": [
                "Sherlock: Leadership, m√©thode, d√©duction",
                "Watson: Logique, assistance, rigueur",
                "Moriarty: Myst√®re, r√©v√©lation, strat√©gie"
            ],
            "score_cible": 7.5
        },
        "progression_logique": {
            "description": "Enqu√™te progresse m√©thodiquement vers la solution",
            "criteres": [
                "Suggestions de Sherlock pertinentes",
                "Validations logiques de Watson",
                "R√©v√©lations strat√©giques de Moriarty",
                "Convergence vers la solution correcte"
            ],
            "score_cible": 8.5
        },
        "revelations_strategiques": {
            "description": "Informations r√©v√©l√©es au bon moment",
            "criteres": [
                "Dosage appropri√© (20-30% des tours)",
                "R√©partition √©quilibr√©e dans le temps",
                "Pertinence par rapport aux suggestions",
                "Progression vers la r√©solution"
            ],
            "score_cible": 8.0
        },
        "resolution_satisfaisante": {
            "description": "Conclusion logique et satisfaisante",
            "criteres": [
                "Solution correcte propos√©e",
                "Justification logique claire",
                "Efficacit√© temporelle (<10 tours)",
                "Coh√©rence narrative"
            ],
            "score_cible": 9.0
        }
    }


def generate_optimization_plan(recommendations: List[Dict[str, Any]]) -> Dict[str, Any]:
    """G√©n√®re un plan d'optimisation incr√©mentale."""
    
    # Tri des recommandations par priorit√©
    high_priority = [r for r in recommendations if r.get("priorite") == "√©lev√©e"]
    medium_priority = [r for r in recommendations if r.get("priorite") == "moyenne"]
    
    return {
        "phase_A_prompts_personnalites": {
            "description": "Optimisation des prompts et personnalit√©s d'agents",
            "actions": [
                "Enrichir le prompt Sherlock avec expressions caract√©ristiques",
                "Am√©liorer la diff√©renciation Watson/Sherlock",
                "D√©velopper la personnalit√© myst√©rieuse de Moriarty",
                "Ajouter des mots de liaison pour fluidit√©"
            ],
            "priorite": "√âLEV√âE",
            "effort_estime": "2-3 jours",
            "impact_attendu": "+1.5 points sur personnalit√©s distinctes"
        },
        "phase_B_logique_revelations": {
            "description": "Am√©lioration de la logique de r√©v√©lations Moriarty",
            "actions": [
                "Calibrer la strat√©gie de r√©v√©lation 'balanced'",
                "Impl√©menter r√©v√©lations progressives",
                "Am√©liorer timing des r√©v√©lations",
                "Optimiser crit√®res de pertinence"
            ],
            "priorite": "√âLEV√âE",
            "effort_estime": "3-4 jours",
            "impact_attendu": "+2.0 points sur dosage r√©v√©lations"
        },
        "phase_C_transitions_fluidite": {
            "description": "Affinement des transitions et fluidit√©",
            "actions": [
                "Impl√©menter m√©moire contextuelle entre tours",
                "Ajouter r√©f√©rences au message pr√©c√©dent",
                "Am√©liorer la s√©lection adaptative",
                "Optimiser la passation de tour"
            ],
            "priorite": "MOYENNE",
            "effort_estime": "2-3 jours",
            "impact_attendu": "+1.0 point sur fluidit√© transitions"
        },
        "phase_D_validation_metriques": {
            "description": "Validation et m√©triques qualitatives",
            "actions": [
                "Impl√©menter m√©triques automatiques",
                "Tests A/B avec diff√©rentes strat√©gies",
                "Validation utilisateur",
                "Benchmark performance"
            ],
            "priorite": "MOYENNE",
            "effort_estime": "2-3 jours",
            "impact_attendu": "Mesure objective des am√©liorations"
        }
    }


def compare_with_2agent_system(workflow_result: Dict[str, Any]) -> Dict[str, Any]:
    """Compare avec le syst√®me 2-agents th√©orique."""
    oracle_stats = workflow_result.get("oracle_statistics", {})
    
    return {
        "efficacite_temporelle": {
            "tours_3_agents": oracle_stats.get("agent_interactions", {}).get("total_turns", 0),
            "estimation_2_agents": oracle_stats.get("agent_interactions", {}).get("total_turns", 0) * 1.5,
            "gain_estime": "25-30% reduction in turns"
        },
        "richesse_informationnelle": {
            "cartes_revelees": oracle_stats.get("workflow_metrics", {}).get("cards_revealed", 0),
            "interactions_oracle": oracle_stats.get("workflow_metrics", {}).get("oracle_interactions", 0),
            "avantage_3_agents": "R√©v√©lations strat√©giques vs al√©atoires"
        },
        "qualite_resolution": {
            "succes_actuel": workflow_result.get("solution_analysis", {}).get("success", False),
            "methode": "D√©duction assist√©e par Oracle",
            "vs_2_agents": "√âlimination pure par suggestions"
        }
    }


def save_analysis_results(report: Dict[str, Any]) -> None:
    """Sauvegarde les r√©sultats d'analyse."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Sauvegarde du rapport complet
    with open(f"analyse_trace_complete_{timestamp}.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # Sauvegarde du r√©sum√© ex√©cutif
    summary = {
        "timestamp": report["metadata"]["analyse_timestamp"],
        "score_global": report["analyse_qualitative"]["score_global"],
        "diagnostic": report["analyse_qualitative"]["diagnostic"],
        "recommandations_prioritaires": [
            r for r in report["recommandations_amelioration"] 
            if r.get("priorite") == "√©lev√©e"
        ][:3],
        "prochaines_etapes": list(report["plan_optimisation"].keys())[:2]
    }
    
    with open(f"resume_executif_{timestamp}.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    logger.info(f"üìÑ R√©sultats sauvegard√©s: analyse_trace_complete_{timestamp}.json")


def display_analysis_summary(metrics: ConversationalMetrics, recommendations: List[Dict[str, Any]]) -> None:
    """Affiche le r√©sum√© de l'analyse."""
    print("\n" + "="*80)
    print("üéØ R√âSUM√â DE L'ANALYSE DE LA TRACE SHERLOCK-WATSON-MORIARTY")
    print("="*80)
    
    print(f"\nüìä SCORE GLOBAL: {metrics.moyenne_globale():.2f}/10")
    print(f"üé≠ Naturalit√©: {metrics.naturalite_score:.1f}/10")
    print(f"üéØ Pertinence: {metrics.pertinence_score:.1f}/10")
    print(f"üß† Progression logique: {metrics.progression_logique_score:.1f}/10")
    print(f"üë§ Personnalit√©s distinctes: {metrics.personnalite_distincte_score:.1f}/10")
    print(f"üîÑ Fluidit√© transitions: {metrics.fluidite_transitions_score:.1f}/10")
    print(f"üíé Dosage r√©v√©lations: {metrics.dosage_revelations_score:.1f}/10")
    print(f"‚úÖ Satisfaction r√©solution: {metrics.satisfaction_resolution_score:.1f}/10")
    
    print(f"\nüö® POINTS D'AM√âLIORATION PRIORITAIRES:")
    high_priority = [r for r in recommendations if r.get("priorite") == "√©lev√©e"]
    for i, rec in enumerate(high_priority[:3], 1):
        print(f"{i}. {rec['domaine']} (Score: {rec['score_actuel']:.1f}/10)")
        print(f"   ‚Üí {rec['probleme']}")
    
    print(f"\nüìà PROCHAINES √âTAPES:")
    print("1. Phase A: Optimisation prompts et personnalit√©s (2-3 jours)")
    print("2. Phase B: Am√©lioration logique r√©v√©lations Moriarty (3-4 jours)")
    print("3. Phase C: Affinement transitions et fluidit√© (2-3 jours)")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    asyncio.run(execute_workflow_analysis())