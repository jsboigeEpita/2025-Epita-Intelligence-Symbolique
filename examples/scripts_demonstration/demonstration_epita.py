# -*- coding: utf-8 -*-
"""
Script principal de d√©monstration EPITA - Architecture Modulaire
Intelligence Symbolique - Menu Cat√©goris√© + Validation Donn√©es Custom

VERSION 2.1 - Ajout validation avec donn√©es d√©di√©es en param√®tre
D√©tection automatique des mocks vs traitement r√©el

Utilisation :
  python demonstration_epita.py                    # Menu interactif
  python demonstration_epita.py --interactive      # Mode interactif avec modules
  python demonstration_epita.py --quick-start      # Quick start √©tudiants
  python demonstration_epita.py --metrics          # M√©triques seulement
  python demonstration_epita.py --validate-custom  # Validation avec donn√©es d√©di√©es
  python demonstration_epita.py --custom-data "texte"  # Test avec donn√©es custom
"""

import sys
import os
import argparse
import importlib.util
import subprocess
import time
import hashlib
import json
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass
from datetime import datetime

# Configuration du chemin pour les modules
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
os.chdir(project_root)

# V√©rifier et installer PyYAML si n√©cessaire
def ensure_yaml_dependency():
    try:
        import yaml
    except ImportError:
        print("Installation de PyYAML...")
        subprocess.run([sys.executable, "-m", "pip", "install", "PyYAML"], check=True)
        import yaml

ensure_yaml_dependency()

# Classes pour la validation avec donn√©es d√©di√©es
@dataclass
class CustomTestDataset:
    """Dataset de test personnalis√© pour validation √âpita."""
    name: str
    content: str
    content_hash: str
    expected_indicators: List[str]
    test_purpose: str
    marker: str

@dataclass
class ValidationResult:
    """R√©sultat de validation d'un test."""
    dataset_name: str
    mode_tested: str
    timestamp: str
    success: bool
    output_captured: str
    real_processing_detected: bool
    mock_detected: bool
    custom_data_processed: bool
    execution_time: float
    error: str = ""

class EpitaValidator:
    """Validateur pour d√©tecter mocks vs traitement r√©el."""
    
    def __init__(self):
        self.real_indicators = [
            "analyse en cours", "traitement", "parsing", "d√©tection",
            "calcul", "m√©trique", "score", "r√©sultat", "argument", "sophisme"
        ]
        self.mock_indicators = [
            "simulation", "mock", "exemple g√©n√©rique", "donn√©es factices",
            "placeholder", "test pattern", "demo content"
        ]
    
    def create_custom_datasets(self) -> List[CustomTestDataset]:
        """Cr√©e des datasets avec marqueurs uniques."""
        timestamp = int(time.time())
        datasets = []
        
        # Dataset 1: Logique √âpita avec marqueur unique
        content1 = f"[EPITA_VALID_{timestamp}] Tous les algorithmes √âpita sont optimis√©s. Cet algorithme est optimis√©. Donc cet algorithme est un algorithme √âpita."
        datasets.append(CustomTestDataset(
            name="logique_epita_custom",
            content=content1,
            content_hash=hashlib.md5(content1.encode()).hexdigest(),
            expected_indicators=["syllogisme", "logique", "pr√©misse"],
            test_purpose="Test logique avec identifiant unique",
            marker=f"EPITA_VALID_{timestamp}"
        ))
        
        # Dataset 2: Sophisme technique avec marqueur
        content2 = f"[EPITA_TECH_{timestamp + 1}] Cette technologie est adopt√©e par 90% des entreprises. Notre projet doit donc l'utiliser pour r√©ussir."
        datasets.append(CustomTestDataset(
            name="sophisme_tech_custom",
            content=content2,
            content_hash=hashlib.md5(content2.encode()).hexdigest(),
            expected_indicators=["argumentum ad populum", "sophisme", "fallacy"],
            test_purpose="D√©tection sophisme technique",
            marker=f"EPITA_TECH_{timestamp + 1}"
        ))
        
        # Dataset 3: Unicode et caract√®res sp√©ciaux
        content3 = f"[EPITA_UNICODE_{timestamp + 2}] Algorithme: O(n¬≤) ‚Üí O(n log n) üöÄ Performance: +100% ‚úì Caf√© ‚òï"
        datasets.append(CustomTestDataset(
            name="unicode_test_custom",
            content=content3,
            content_hash=hashlib.md5(content3.encode()).hexdigest(),
            expected_indicators=["algorithme", "complexit√©", "unicode"],
            test_purpose="Test robustesse Unicode",
            marker=f"EPITA_UNICODE_{timestamp + 2}"
        ))
        
        return datasets
    
    def validate_with_dataset(self, dataset: CustomTestDataset, module_func, mode: str) -> ValidationResult:
        """Valide un module avec un dataset custom."""
        start_time = time.time()
        
        try:
            # Cr√©er un fichier temporaire avec les donn√©es custom
            temp_file = Path(f"temp_epita_test_{dataset.name}_{int(time.time())}.txt")
            temp_file.write_text(dataset.content, encoding='utf-8')
            
            # Capturer stdout/stderr pour analyser la sortie
            import io
            import contextlib
            
            captured_output = io.StringIO()
            
            with contextlib.redirect_stdout(captured_output), contextlib.redirect_stderr(captured_output):
                try:
                    if hasattr(module_func, '__call__'):
                        # Tenter de passer les donn√©es custom au module
                        result = module_func() if not module_func.__code__.co_argcount else module_func(dataset.content)
                    else:
                        result = True
                except Exception as e:
                    result = False
            
            output = captured_output.getvalue()
            execution_time = time.time() - start_time
            
            # Analyser la sortie pour d√©tecter traitement r√©el vs mock
            real_processing = any(indicator.lower() in output.lower() for indicator in self.real_indicators)
            mock_detected = any(indicator.lower() in output.lower() for indicator in self.mock_indicators)
            
            # V√©rifier si le marqueur custom appara√Æt (preuve que les donn√©es ont √©t√© lues)
            custom_data_processed = (dataset.marker in output or
                                   dataset.content_hash in output or
                                   any(expected.lower() in output.lower() for expected in dataset.expected_indicators))
            
            # Nettoyer le fichier temporaire
            if temp_file.exists():
                temp_file.unlink()
            
            return ValidationResult(
                dataset_name=dataset.name,
                mode_tested=mode,
                timestamp=datetime.now().isoformat(),
                success=result is not False,
                output_captured=output[:500],  # Limiter la sortie
                real_processing_detected=real_processing,
                mock_detected=mock_detected,
                custom_data_processed=custom_data_processed,
                execution_time=execution_time
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return ValidationResult(
                dataset_name=dataset.name,
                mode_tested=mode,
                timestamp=datetime.now().isoformat(),
                success=False,
                output_captured="",
                real_processing_detected=False,
                mock_detected=False,
                custom_data_processed=False,
                execution_time=execution_time,
                error=str(e)
            )

# Import des utilitaires depuis le module
modules_path = Path(__file__).parent / "modules"
sys.path.insert(0, str(modules_path))

try:
    from demo_utils import (
        DemoLogger, Colors, Symbols, charger_config_categories,
        afficher_progression, pause_interactive, confirmer_action,
        valider_environnement
    )
except ImportError as e:
    print(f"Erreur d'import des utilitaires : {e}")
    print("Chargement du mode legacy...")
    # Fallback vers le mode legacy si les modules ne sont pas disponibles
    from demonstration_epita_legacy import main as legacy_main
    legacy_main()
    sys.exit(0)

def afficher_banniere_principale():
    """Affiche la banni√®re principale du syst√®me"""
    print(f"""
{Colors.CYAN}{Colors.BOLD}
+==============================================================================+
|                [EPITA] DEMONSTRATION - Intelligence Symbolique              |
|                        Architecture Modulaire v2.0                         |
+==============================================================================+
{Colors.ENDC}""")

def afficher_menu_categories(config: Dict[str, Any]) -> None:
    """Affiche le menu cat√©goris√© principal"""
    print(f"\n{Colors.BOLD}{'=' * 47}{Colors.ENDC}")
    
    if 'categories' not in config:
        print(f"{Colors.FAIL}Configuration des cat√©gories non trouv√©e{Colors.ENDC}")
        return
    
    categories = config['categories']
    categories_triees = sorted(categories.items(), key=lambda x: x[1]['id'])
    
    for cat_id, cat_info in categories_triees:
        icon = cat_info.get('icon', '‚Ä¢')
        nom = cat_info.get('nom', cat_id)
        description = cat_info.get('description', '')
        id_num = cat_info.get('id', 0)
        
        print(f"{Colors.CYAN}{icon} {id_num}. {nom}{Colors.ENDC} ({description})")
    
    print(f"\n{Colors.WARNING}S√©lectionnez une cat√©gorie (1-6) ou 'q' pour quitter:{Colors.ENDC}")

def charger_et_executer_module(nom_module: str, mode_interactif: bool = False) -> bool:
    """Charge et ex√©cute dynamiquement un module de d√©monstration"""
    try:
        module_path = modules_path / f"{nom_module}.py"
        if not module_path.exists():
            print(f"{Colors.FAIL}{Symbols.CROSS} Module {nom_module} non trouv√©{Colors.ENDC}")
            return False
        
        # Chargement dynamique du module
        spec = importlib.util.spec_from_file_location(nom_module, module_path)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        
        # Ex√©cution selon le mode
        if mode_interactif and hasattr(module, 'run_demo_interactive'):
            return module.run_demo_interactive()
        elif hasattr(module, 'run_demo_rapide'):
            return module.run_demo_rapide()
        else:
            print(f"{Colors.WARNING}Fonction de d√©monstration non trouv√©e dans {nom_module}{Colors.ENDC}")
            return False
            
    except Exception as e:
        print(f"{Colors.FAIL}{Symbols.CROSS} Erreur lors de l'ex√©cution de {nom_module}: {e}{Colors.ENDC}")
        return False

def mode_menu_interactif(config: Dict[str, Any]) -> None:
    """Mode menu interactif principal"""
    logger = DemoLogger("menu_principal")
    
    while True:
        afficher_banniere_principale()
        afficher_menu_categories(config)
        
        try:
            choix = input(f"\n{Colors.CYAN}> {Colors.ENDC}").strip().lower()
            
            if choix == 'q' or choix == 'quit':
                logger.info("Au revoir !")
                break
            
            # Conversion en entier pour la s√©lection
            if choix.isdigit():
                num_choix = int(choix)
                
                # Trouver la cat√©gorie correspondante
                categories = config.get('categories', {})
                cat_selectionnee = None
                
                for cat_id, cat_info in categories.items():
                    if cat_info.get('id') == num_choix:
                        cat_selectionnee = (cat_id, cat_info)
                        break
                
                if cat_selectionnee:
                    cat_id, cat_info = cat_selectionnee
                    nom_module = cat_info.get('module', '')
                    nom_cat = cat_info.get('nom', cat_id)
                    
                    logger.header(f"{Symbols.ROCKET} Lancement de : {nom_cat}")
                    
                    if confirmer_action(f"Ex√©cuter la d√©monstration '{nom_cat}' ?"):
                        succes = charger_et_executer_module(nom_module, mode_interactif=True)
                        
                        if succes:
                            logger.success(f"{Symbols.CHECK} D√©monstration '{nom_cat}' termin√©e avec succ√®s !")
                        else:
                            logger.error(f"{Symbols.CROSS} √âchec de la d√©monstration '{nom_cat}'")
                        
                        pause_interactive("Appuyez sur Entr√©e pour revenir au menu principal...")
                else:
                    print(f"{Colors.FAIL}Choix invalide : {num_choix}{Colors.ENDC}")
                    pause_interactive()
            else:
                print(f"{Colors.FAIL}Veuillez entrer un num√©ro (1-6) ou 'q'{Colors.ENDC}")
                pause_interactive()
                
        except KeyboardInterrupt:
            logger.info("\nInterruption utilisateur - Au revoir !")
            break
        except Exception as e:
            logger.error(f"Erreur inattendue : {e}")
            pause_interactive()

def mode_quick_start() -> None:
    """Mode Quick Start pour les √©tudiants"""
    logger = DemoLogger("quick_start")
    afficher_banniere_principale()
    logger.header(f"{Symbols.ROCKET} MODE QUICK-START - D√©monstration rapide")
    
    # Charger la configuration
    config = charger_config_categories()
    if not config:
        return
    
    # Ex√©cuter une d√©mo rapide de chaque cat√©gorie
    categories = config.get('categories', {})
    
    for cat_id, cat_info in categories.items():
        module_name = cat_info.get('module')
        if module_name:
            try:
                print(f"\n{Colors.CYAN}{cat_info.get('icon', '[INFO]')} {cat_info.get('nom', 'Cat√©gorie')}{Colors.ENDC}")
                succes = charger_et_executer_module(module_name, mode_interactif=False)
                if succes:
                    print(f"{Colors.GREEN}  [OK] Termin√©{Colors.ENDC}")
                else:
                    print(f"{Colors.FAIL}  [FAIL] Erreur{Colors.ENDC}")
                time.sleep(0.5)
            except Exception as e:
                logger.error(f"Erreur module {module_name}: {e}")
    
    print(f"\n{Colors.GREEN}{Symbols.CHECK} Quick-start termin√© !{Colors.ENDC}")

def mode_metrics_only(config: Dict[str, Any]) -> None:
    """Affiche uniquement les m√©triques du projet"""
    afficher_banniere_principale()
    
    config_global = config.get('config', {})
    taux_succes = config_global.get('taux_succes_tests', 99.7)
    architecture = config_global.get('architecture', 'Python + Java (JPype)')
    domaines = config_global.get('domaines', [])
    
    print(f"\n{Colors.BOLD}{Symbols.CHART} M√âTRIQUES DU PROJET{Colors.ENDC}")
    print(f"{Colors.CYAN}{'=' * 50}{Colors.ENDC}")
    print(f"{Colors.GREEN}{Symbols.CHECK} Taux de succ√®s des tests : {taux_succes}%{Colors.ENDC}")
    print(f"{Colors.BLUE}{Symbols.GEAR} Architecture : {architecture}{Colors.ENDC}")
    print(f"{Colors.CYAN}{Symbols.BRAIN} Domaines couverts :{Colors.ENDC}")
    for domaine in domaines:
        print(f"  ‚Ä¢ {domaine}")
    
    print(f"\n{Colors.BOLD}Modules disponibles :{Colors.ENDC}")
    categories = config.get('categories', {})
    for cat_info in sorted(categories.values(), key=lambda x: x.get('id', 0)):
        icon = cat_info.get('icon', '‚Ä¢')
        nom = cat_info.get('nom', 'Module')
        print(f"  {icon} {nom}")

def mode_execution_legacy() -> None:
    """Ex√©cute le comportement legacy pour compatibilit√©"""
    print(f"{Colors.WARNING}{Symbols.WARNING} Mode legacy - Chargement du script original...{Colors.ENDC}")
    
    try:
        # Import et ex√©cution du script legacy
        legacy_path = Path(__file__).parent / "demonstration_epita_legacy.py"
        spec = importlib.util.spec_from_file_location("legacy", legacy_path)
        legacy_module = importlib.util.module_from_spec(spec)
        
        # Simuler les arguments pour le mode normal
        import sys
        original_argv = sys.argv.copy()
        sys.argv = ['demonstration_epita_legacy.py']  # Mode normal
        
        try:
            spec.loader.exec_module(legacy_module)
        finally:
            sys.argv = original_argv
            
    except Exception as e:
        print(f"{Colors.FAIL}Erreur lors de l'ex√©cution du mode legacy : {e}{Colors.ENDC}")

def execute_all_categories_non_interactive(config: Dict[str, Any]) -> None:
    """Ex√©cute toutes les cat√©gories de tests en mode non-interactif avec trace compl√®te."""
    logger = DemoLogger("all_tests")
    
    # Banni√®re pour le mode all-tests
    print(f"""
{Colors.CYAN}{Colors.BOLD}
+==============================================================================+
|              [EPITA] MODE --ALL-TESTS - Trace Compl√®te Non-Interactive     |
|                     Ex√©cution de toutes les cat√©gories                     |
+==============================================================================+
{Colors.ENDC}""")
    
    start_time = time.time()
    categories = config.get('categories', {})
    categories_triees = sorted(categories.items(), key=lambda x: x[1]['id'])
    
    logger.info(f"{Symbols.ROCKET} D√©but de l'ex√©cution compl√®te - {len(categories_triees)} cat√©gories √† traiter")
    logger.info(f"[TIME] Timestamp de d√©marrage : {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Statistiques globales
    total_categories = len(categories_triees)
    categories_reussies = 0
    categories_echouees = 0
    resultats_detailles = []
    
    for i, (cat_id, cat_info) in enumerate(categories_triees, 1):
        nom_module = cat_info.get('module', '')
        nom_cat = cat_info.get('nom', cat_id)
        icon = cat_info.get('icon', '‚Ä¢')
        description = cat_info.get('description', '')
        
        print(f"\n{Colors.BOLD}{'=' * 80}{Colors.ENDC}")
        print(f"{Colors.CYAN}{icon} CAT√âGORIE {i}/{total_categories} : {nom_cat}{Colors.ENDC}")
        print(f"{Colors.BLUE}Description : {description}{Colors.ENDC}")
        print(f"{Colors.WARNING}Module : {nom_module}{Colors.ENDC}")
        print(f"{'=' * 80}")
        
        cat_start_time = time.time()
        
        try:
            # Ex√©cution non-interactive du module
            logger.info(f"[CAT] D√©but ex√©cution cat√©gorie : {nom_cat}")
            succes = charger_et_executer_module(nom_module, mode_interactif=False)
            cat_end_time = time.time()
            cat_duration = cat_end_time - cat_start_time
            
            if succes:
                categories_reussies += 1
                status = "SUCC√àS"
                color = Colors.GREEN
                symbol = Symbols.CHECK
                logger.success(f"{Symbols.CHECK} Cat√©gorie '{nom_cat}' termin√©e avec succ√®s en {cat_duration:.2f}s")
            else:
                categories_echouees += 1
                status = "√âCHEC"
                color = Colors.FAIL
                symbol = Symbols.CROSS
                logger.error(f"[FAIL] √âchec de la cat√©gorie '{nom_cat}' apr√®s {cat_duration:.2f}s")
            
            resultats_detailles.append({
                'categorie': nom_cat,
                'module': nom_module,
                'status': status,
                'duration': cat_duration,
                'index': i
            })
            
            print(f"\n{color}{symbol} Statut : {status} (dur√©e: {cat_duration:.2f}s){Colors.ENDC}")
            
        except Exception as e:
            categories_echouees += 1
            cat_end_time = time.time()
            cat_duration = cat_end_time - cat_start_time
            
            logger.error(f"[ERROR] Erreur critique dans la cat√©gorie '{nom_cat}': {e}")
            print(f"\n{Colors.FAIL}{Symbols.CROSS} ERREUR CRITIQUE : {e}{Colors.ENDC}")
            
            resultats_detailles.append({
                'categorie': nom_cat,
                'module': nom_module,
                'status': 'ERREUR',
                'duration': cat_duration,
                'index': i,
                'erreur': str(e)
            })
    
    # Rapport final
    end_time = time.time()
    total_duration = end_time - start_time
    taux_reussite = (categories_reussies / total_categories) * 100 if total_categories > 0 else 0
    
    print(f"\n{Colors.BOLD}{'=' * 80}{Colors.ENDC}")
    print(f"{Colors.CYAN}{Colors.BOLD}           RAPPORT FINAL - EX√âCUTION COMPL√àTE{Colors.ENDC}")
    print(f"{'=' * 80}")
    
    print(f"\n{Colors.BOLD}[STATS] STATISTIQUES G√âN√âRALES :{Colors.ENDC}")
    print(f"   [TIME] Timestamp de fin : {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   [TIME] Dur√©e totale : {total_duration:.2f} secondes")
    print(f"   [INFO] Total cat√©gories : {total_categories}")
    print(f"   [OK] Cat√©gories r√©ussies : {categories_reussies}")
    print(f"   [FAIL] Cat√©gories √©chou√©es : {categories_echouees}")
    print(f"   [CHART] Taux de r√©ussite : {taux_reussite:.1f}%")
    
    print(f"\n{Colors.BOLD}[INFO] D√âTAILS PAR CAT√âGORIE :{Colors.ENDC}")
    for resultat in resultats_detailles:
        status_color = Colors.GREEN if resultat['status'] == 'SUCC√àS' else Colors.FAIL
        status_symbol = '[OK]' if resultat['status'] == 'SUCC√àS' else '[FAIL]'
        
        print(f"   {status_symbol} {resultat['index']:2d}. {resultat['categorie']:<30} "
              f"{status_color}[{resultat['status']}]{Colors.ENDC} "
              f"({resultat['duration']:.2f}s)")
        
        if 'erreur' in resultat:
            print(f"      [ERROR] Erreur: {resultat['erreur']}")
    
    # M√©triques techniques
    print(f"\n{Colors.BOLD}[TECH] M√âTRIQUES TECHNIQUES :{Colors.ENDC}")
    print(f"   [PYTHON] Architecture : {config.get('config', {}).get('architecture', 'Python + Java (JPype)')}")
    print(f"   [VERSION] Version : {config.get('config', {}).get('version', '2.0.0')}")
    print(f"   [TARGET] Taux succ√®s tests : {config.get('config', {}).get('taux_succes_tests', 99.7)}%")
    
    domaines = config.get('config', {}).get('domaines', [])
    if domaines:
        print(f"   [BRAIN] Domaines couverts :")
        for domaine in domaines:
            print(f"      ‚Ä¢ {domaine}")
    
    # Message final
    if categories_echouees == 0:
        final_color = Colors.GREEN
        final_message = f"[SUCCESS] EX√âCUTION COMPL√àTE R√âUSSIE - Tous les tests ont √©t√© ex√©cut√©s avec succ√®s !"
        logger.success(final_message)
    else:
        final_color = Colors.WARNING
        final_message = f"[WARNING] EX√âCUTION TERMIN√âE AVEC {categories_echouees} √âCHEC(S)"
        logger.warning(final_message)
    
    print(f"\n{final_color}{Colors.BOLD}{final_message}{Colors.ENDC}")
    print(f"{'=' * 80}")

def mode_validation_custom_data(config: Dict[str, Any]) -> None:
    """Mode validation avec donn√©es d√©di√©es pour d√©tecter mocks vs r√©el."""
    logger = DemoLogger("validation_custom")
    
    print(f"""
{Colors.CYAN}{Colors.BOLD}
+==============================================================================+
|              [EPITA] VALIDATION AVEC DONN√âES D√âDI√âES                        |
|                   D√©tection Mocks vs Traitement R√©el                        |
+==============================================================================+
{Colors.ENDC}""")
    
    validator = EpitaValidator()
    datasets = validator.create_custom_datasets()
    
    logger.info(f"üß™ Cr√©ation de {len(datasets)} datasets de test personnalis√©s")
    
    # Tester chaque cat√©gorie avec les datasets custom
    categories = config.get('categories', {})
    categories_triees = sorted(categories.items(), key=lambda x: x[1]['id'])
    
    all_results = []
    
    for cat_id, cat_info in categories_triees:
        nom_module = cat_info.get('module', '')
        nom_cat = cat_info.get('nom', cat_id)
        
        print(f"\n{Colors.BOLD}{'=' * 60}{Colors.ENDC}")
        print(f"{Colors.CYAN}üîç VALIDATION MODULE: {nom_cat}{Colors.ENDC}")
        print(f"{'=' * 60}")
        
        for dataset in datasets:
            print(f"\n{Colors.WARNING}üìä Test avec dataset: {dataset.name}{Colors.ENDC}")
            print(f"   Marqueur: {dataset.marker}")
            print(f"   Objectif: {dataset.test_purpose}")
            
            try:
                # Charger le module et tester avec le dataset
                module_path = modules_path / f"{nom_module}.py"
                if module_path.exists():
                    spec = importlib.util.spec_from_file_location(nom_module, module_path)
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    
                    # Trouver la fonction de d√©mo appropri√©e
                    demo_func = None
                    if hasattr(module, 'run_demo_rapide'):
                        demo_func = module.run_demo_rapide
                    elif hasattr(module, 'run_demo_interactive'):
                        demo_func = module.run_demo_interactive
                    
                    if demo_func:
                        result = validator.validate_with_dataset(dataset, demo_func, nom_cat)
                        all_results.append(result)
                        
                        # Afficher les r√©sultats
                        if result.success:
                            print(f"   {Colors.GREEN}‚úÖ Ex√©cution: SUCC√àS{Colors.ENDC}")
                        else:
                            print(f"   {Colors.FAIL}‚ùå Ex√©cution: √âCHEC{Colors.ENDC}")
                        
                        if result.custom_data_processed:
                            print(f"   {Colors.GREEN}üìù Donn√©es custom: TRAIT√âES{Colors.ENDC}")
                        else:
                            print(f"   {Colors.WARNING}üìù Donn√©es custom: NON D√âTECT√âES{Colors.ENDC}")
                        
                        if result.real_processing_detected:
                            print(f"   {Colors.GREEN}üîß Traitement r√©el: D√âTECT√â{Colors.ENDC}")
                        else:
                            print(f"   {Colors.WARNING}üîß Traitement r√©el: NON D√âTECT√â{Colors.ENDC}")
                        
                        if result.mock_detected:
                            print(f"   {Colors.FAIL}üé≠ Mocks d√©tect√©s: OUI{Colors.ENDC}")
                        else:
                            print(f"   {Colors.GREEN}üé≠ Mocks d√©tect√©s: NON{Colors.ENDC}")
                        
                        print(f"   ‚è±Ô∏è Temps d'ex√©cution: {result.execution_time:.3f}s")
                        
                        if result.error:
                            print(f"   {Colors.FAIL}üí• Erreur: {result.error}{Colors.ENDC}")
                    else:
                        print(f"   {Colors.WARNING}‚ö†Ô∏è Aucune fonction de d√©mo trouv√©e{Colors.ENDC}")
                else:
                    print(f"   {Colors.FAIL}‚ùå Module non trouv√©: {module_path}{Colors.ENDC}")
                    
            except Exception as e:
                print(f"   {Colors.FAIL}üí• Erreur lors du test: {e}{Colors.ENDC}")
    
    # Rapport final de validation
    print(f"\n{Colors.BOLD}{'=' * 80}{Colors.ENDC}")
    print(f"{Colors.CYAN}{Colors.BOLD}           RAPPORT FINAL - VALIDATION DONN√âES CUSTOM{Colors.ENDC}")
    print(f"{'=' * 80}")
    
    if all_results:
        total_tests = len(all_results)
        success_tests = sum(1 for r in all_results if r.success)
        real_processing_tests = sum(1 for r in all_results if r.real_processing_detected)
        custom_data_tests = sum(1 for r in all_results if r.custom_data_processed)
        mock_detected_tests = sum(1 for r in all_results if r.mock_detected)
        
        print(f"\n{Colors.BOLD}üìä STATISTIQUES G√âN√âRALES:{Colors.ENDC}")
        print(f"   Total tests effectu√©s: {total_tests}")
        print(f"   Tests r√©ussis: {success_tests}/{total_tests} ({success_tests/total_tests*100:.1f}%)")
        print(f"   Traitement r√©el d√©tect√©: {real_processing_tests}/{total_tests} ({real_processing_tests/total_tests*100:.1f}%)")
        print(f"   Donn√©es custom trait√©es: {custom_data_tests}/{total_tests} ({custom_data_tests/total_tests*100:.1f}%)")
        print(f"   Mocks d√©tect√©s: {mock_detected_tests}/{total_tests} ({mock_detected_tests/total_tests*100:.1f}%)")
        
        print(f"\n{Colors.BOLD}üéØ √âVALUATION CAPACIT√âS:{Colors.ENDC}")
        if custom_data_tests > total_tests * 0.7:
            print(f"   {Colors.GREEN}‚úÖ EXCELLENTE acceptation des donn√©es custom{Colors.ENDC}")
        elif custom_data_tests > total_tests * 0.4:
            print(f"   {Colors.WARNING}‚ö†Ô∏è MOD√âR√âE acceptation des donn√©es custom{Colors.ENDC}")
        else:
            print(f"   {Colors.FAIL}‚ùå FAIBLE acceptation des donn√©es custom{Colors.ENDC}")
        
        if real_processing_tests > total_tests * 0.6:
            print(f"   {Colors.GREEN}‚úÖ TRAITEMENT R√âEL pr√©dominant{Colors.ENDC}")
        else:
            print(f"   {Colors.WARNING}‚ö†Ô∏è MOCKS ou simulations d√©tect√©s{Colors.ENDC}")
        
        # Sauvegarder le rapport d√©taill√©
        rapport_path = Path("logs") / f"validation_epita_custom_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        rapport_path.parent.mkdir(exist_ok=True)
        
        with open(rapport_path, 'w', encoding='utf-8') as f:
            json.dump([result.__dict__ for result in all_results], f, indent=2, ensure_ascii=False)
        
        print(f"\n{Colors.BLUE}üìÑ Rapport d√©taill√© sauvegard√©: {rapport_path}{Colors.ENDC}")
    else:
        print(f"{Colors.FAIL}‚ùå Aucun r√©sultat de validation g√©n√©r√©{Colors.ENDC}")

def mode_custom_data_test(custom_text: str, config: Dict[str, Any]) -> None:
    """Test avec des donn√©es custom sp√©cifiques fournies par l'utilisateur."""
    logger = DemoLogger("custom_data_test")
    
    print(f"""
{Colors.CYAN}{Colors.BOLD}
+==============================================================================+
|              [EPITA] TEST AVEC DONN√âES CUSTOM SP√âCIFIQUES                   |
|                        Texte fourni par l'utilisateur                       |
+==============================================================================+
{Colors.ENDC}""")
    
    print(f"\n{Colors.BOLD}üìù DONN√âES √Ä TESTER:{Colors.ENDC}")
    print(f"   Longueur: {len(custom_text)} caract√®res")
    print(f"   Hash: {hashlib.md5(custom_text.encode()).hexdigest()[:8]}...")
    print(f"   Aper√ßu: {custom_text[:100]}{'...' if len(custom_text) > 100 else ''}")
    
    # Cr√©er un dataset custom avec les donn√©es utilisateur
    timestamp = int(time.time())
    marker = f"USER_DATA_{timestamp}"
    custom_dataset = CustomTestDataset(
        name="user_provided_data",
        content=f"[{marker}] {custom_text}",
        content_hash=hashlib.md5(custom_text.encode()).hexdigest(),
        expected_indicators=["analyse", "traitement", "r√©sultat"],
        test_purpose="Test avec donn√©es utilisateur sp√©cifiques",
        marker=marker
    )
    
    validator = EpitaValidator()
    categories = config.get('categories', {})
    
    print(f"\n{Colors.BOLD}üîç TEST SUR TOUTES LES CAT√âGORIES:{Colors.ENDC}")
    
    results = []
    for cat_id, cat_info in sorted(categories.items(), key=lambda x: x[1]['id']):
        nom_module = cat_info.get('module', '')
        nom_cat = cat_info.get('nom', cat_id)
        
        print(f"\n{Colors.CYAN}üìä {nom_cat}:{Colors.ENDC}")
        
        try:
            module_path = modules_path / f"{nom_module}.py"
            if module_path.exists():
                spec = importlib.util.spec_from_file_location(nom_module, module_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                
                demo_func = getattr(module, 'run_demo_rapide', None) or getattr(module, 'run_demo_interactive', None)
                
                if demo_func:
                    result = validator.validate_with_dataset(custom_dataset, demo_func, nom_cat)
                    results.append(result)
                    
                    status = "‚úÖ SUCC√àS" if result.success else "‚ùå √âCHEC"
                    data_processed = "üìù TRAIT√âES" if result.custom_data_processed else "üìù NON D√âTECT√âES"
                    real_processing = "üîß R√âEL" if result.real_processing_detected else "üîß SIMUL√â"
                    
                    print(f"   {status} | {data_processed} | {real_processing} | ‚è±Ô∏è {result.execution_time:.3f}s")
                else:
                    print(f"   {Colors.WARNING}‚ö†Ô∏è Fonction de d√©mo non trouv√©e{Colors.ENDC}")
            else:
                print(f"   {Colors.FAIL}‚ùå Module non trouv√©{Colors.ENDC}")
        except Exception as e:
            print(f"   {Colors.FAIL}üí• Erreur: {str(e)[:50]}...{Colors.ENDC}")
    
    # R√©sum√© final
    if results:
        success_rate = sum(1 for r in results if r.success) / len(results) * 100
        processing_rate = sum(1 for r in results if r.custom_data_processed) / len(results) * 100
        real_rate = sum(1 for r in results if r.real_processing_detected) / len(results) * 100
        
        print(f"\n{Colors.BOLD}üìà R√âSUM√â VALIDATION DONN√âES CUSTOM:{Colors.ENDC}")
        print(f"   Taux de succ√®s: {success_rate:.1f}%")
        print(f"   Taux de traitement des donn√©es: {processing_rate:.1f}%")
        print(f"   Taux de traitement r√©el: {real_rate:.1f}%")
        
        if processing_rate > 70:
            print(f"   {Colors.GREEN}üéØ CONCLUSION: Les donn√©es custom sont bien accept√©es et trait√©es{Colors.ENDC}")
        elif processing_rate > 30:
            print(f"   {Colors.WARNING}üéØ CONCLUSION: Acceptation partielle des donn√©es custom{Colors.ENDC}")
        else:
            print(f"   {Colors.FAIL}üéØ CONCLUSION: Les donn√©es custom ne semblent pas √™tre trait√©es{Colors.ENDC}")

def parse_arguments():
    """Parse les arguments de ligne de commande"""
    parser = argparse.ArgumentParser(
        description="Script de d√©monstration EPITA - Architecture Modulaire v2.1",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Modes disponibles :
  [d√©faut]           Menu interactif cat√©goris√©
  --interactive      Mode interactif avec pauses p√©dagogiques
  --quick-start      Mode Quick Start pour √©tudiants
  --metrics          Affichage des m√©triques uniquement
  --all-tests        Ex√©cution compl√®te non-interactive de toutes les cat√©gories
  --validate-custom  Validation avec datasets d√©di√©s pour d√©tecter mocks vs r√©el
  --custom-data      Test avec des donn√©es custom sp√©cifiques
  --legacy           Ex√©cution du script original (compatibilit√©)
        """
    )
    
    parser.add_argument('--interactive', '-i', action='store_true',
                       help='Mode interactif avec pauses p√©dagogiques')
    parser.add_argument('--quick-start', '-q', action='store_true',
                       help='Mode Quick Start pour √©tudiants')
    parser.add_argument('--metrics', '-m', action='store_true',
                       help='Affichage des m√©triques uniquement')
    parser.add_argument('--legacy', '-l', action='store_true',
                       help='Ex√©cution du script original (compatibilit√©)')
    parser.add_argument('--all-tests', action='store_true',
                       help='Ex√©cute tous les tests de toutes les cat√©gories en mode non-interactif')
    parser.add_argument('--validate-custom', action='store_true',
                       help='Mode validation avec donn√©es d√©di√©es pour d√©tecter mocks vs traitement r√©el')
    parser.add_argument('--custom-data', type=str, metavar='TEXT',
                       help='Test avec des donn√©es custom sp√©cifiques fournies en param√®tre')
    
    return parser.parse_args()

def main():
    """Fonction principale"""
    # Validation de l'environnement
    if not valider_environnement():
        print(f"{Colors.FAIL}Environnement non valide. Ex√©cutez depuis la racine du projet.{Colors.ENDC}")
        sys.exit(1)
    
    # Parse des arguments
    args = parse_arguments()
    
    # Chargement de la configuration
    config = charger_config_categories()
    if not config:
        print(f"{Colors.FAIL}Impossible de charger la configuration. Ex√©cution en mode legacy.{Colors.ENDC}")
        mode_execution_legacy()
        return
    
    # S√©lection du mode d'ex√©cution
    if args.validate_custom:
        mode_validation_custom_data(config)
    elif args.custom_data:
        mode_custom_data_test(args.custom_data, config)
    elif args.all_tests:
        execute_all_categories_non_interactive(config)
    elif args.quick_start:
        mode_quick_start()
    elif args.metrics:
        mode_metrics_only(config)
    elif args.legacy:
        mode_execution_legacy()
    elif args.interactive:
        # Mode interactif avanc√© - ex√©cution s√©quentielle des modules
        logger = DemoLogger("demo_complet")
        logger.header("[EPITA] D√âMONSTRATION COMPL√àTE - MODE INTERACTIF")
        
        categories = config.get('categories', {})
        categories_triees = sorted(categories.items(), key=lambda x: x[1]['id'])
        
        for i, (cat_id, cat_info) in enumerate(categories_triees, 1):
            nom_module = cat_info.get('module', '')
            nom_cat = cat_info.get('nom', cat_id)
            
            afficher_progression(i, len(categories_triees), f"Module : {nom_cat}")
            
            if confirmer_action(f"Ex√©cuter '{nom_cat}' ?"):
                charger_et_executer_module(nom_module, mode_interactif=True)
            
            if i < len(categories_triees):
                pause_interactive()
        
        logger.success("üéì D√©monstration compl√®te termin√©e !")
    else:
        # Mode menu interactif par d√©faut
        mode_menu_interactif(config)

if __name__ == "__main__":
    main()