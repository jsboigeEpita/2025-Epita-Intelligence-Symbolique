conda.exe : ERROR conda.cli.main_run:execute(41): `conda run pytest tests/` failed. (See above for error)
Au caractère C:\Tools\miniconda3\shell\condabin\Conda.psm1:187 : 17
+ ...             & $Env:CONDA_EXE $Env:_CE_M $Env:_CE_CONDA $Command @Othe ...
+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (ERROR conda.cli...bove for error):String) [], RemoteException
    + FullyQualifiedErrorId : NativeCommandError
 
============================= test session starts =============================

platform win32 -- Python 3.10.18, pytest-8.4.0, pluggy-1.6.0

rootdir: D:\2025-Epita-Intelligence-Symbolique-4

configfile: pytest.ini

plugins: anyio-4.9.0, asyncio-1.0.0, cov-6.1.1, mock-3.14.1

asyncio: mode=auto, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function

collected 939 items



tests\agents\core\informal\test_informal_agent.py .............          [  1%]

tests\agents\core\informal\test_informal_agent_creation.py ...           [  1%]

tests\agents\core\informal\test_informal_analysis_methods.py ....        [  2%]

tests\agents\core\informal\test_informal_definitions.py Fsss.            [  2%]

tests\agents\core\informal\test_informal_error_handling.py ....          [  3%]

tests\agents\core\logic\test_abstract_logic_agent.py ........            [  3%]

tests\agents\core\logic\test_belief_set.py ..........                    [  5%]

tests\agents\core\logic\test_examples.py .....                           [  5%]

tests\agents\core\logic\test_first_order_logic_agent.py ............     [  6%]

tests\agents\core\logic\test_logic_factory.py ........                   [  7%]

tests\agents\core\logic\test_modal_logic_agent.py ............           [  8%]

tests\agents\core\logic\test_propositional_logic_agent.py FFFFF...FF.F.  [ 10%]

tests\agents\core\logic\test_query_executor.py .........                 [ 11%]

tests\agents\core\logic\test_tweety_bridge.py ............               [ 12%]

tests\agents\core\logic\test_watson_logic_assistant.py FFFFF             [ 13%]

tests\agents\core\pm\test_sherlock_enquete_agent.py F..FF                [ 13%]

tests\agents\tools\analysis\enhanced\test_enhanced_complex_fallacy_analyzer.py . [ 13%]

.........                                                                [ 14%]

tests\agents\tools\analysis\enhanced\test_enhanced_contextual_fallacy_analyzer.py . [ 14%]

.........                                                                [ 15%]

tests\agents\tools\analysis\enhanced\test_enhanced_fallacy_severity_evaluator.py . [ 15%]

............                                                             [ 17%]

tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py EEE..F [ 17%]

..                                                                       [ 17%]

tests\core\test_enquete_states.py .......................                [ 20%]

tests\environment_checks\test_core_dependencies.py .......F.F.........   [ 22%]

tests\environment_checks\test_project_module_imports.py ..........       [ 23%]

tests\minimal_jvm_pytest_test.py .                                       [ 23%]

tests\mocks\test_jpype_mock.py ..                                        [ 23%]

tests\mocks\test_numpy_mock.py .                                         [ 23%]

tests\orchestration\hierarchical\operational\adapters\test_extract_agent_adapter.py . [ 24%]

...................                                                      [ 26%]

tests\orchestration\hierarchical\tactical\test_coordinator.py .......... [ 27%]

...........                                                              [ 28%]

tests\orchestration\plugins\test_enquete_state_manager_plugin.py ....... [ 29%]

.                                                                        [ 29%]

tests\orchestration\tactical\test_tactical_coordinator.py .......        [ 29%]

tests\orchestration\tactical\test_tactical_coordinator_advanced.py ..... [ 30%]

..                                                                       [ 30%]

tests\orchestration\tactical\test_tactical_coordinator_coverage.py ..... [ 31%]

                                                                         [ 31%]

tests\orchestration\tactical\test_tactical_monitor.py .........          [ 32%]

tests\orchestration\tactical\test_tactical_monitor_advanced.py .......   [ 32%]

tests\orchestration\test_cluedo_orchestrator.py F                        [ 33%]

tests\project_core\dev_utils\test_verification_utils.py .......          [ 33%]

tests\project_core\service_setup\test_core_services.py .                 [ 33%]

tests\test_extract_agent_adapter.py sssssssss                            [ 34%]

tests\test_numpy_rec_mock.py ..FF.                                       [ 35%]

tests\test_tactical_resolver.py ................                         [ 37%]

tests\test_tactical_resolver_advanced.py .......                         [ 37%]

tests\test_tactical_state.py ......................                      [ 40%]

tests\test_validation_errors.py .....                                    [ 40%]

tests\ui\test_extract_definition_persistence.py ....Fs..F                [ 41%]

tests\ui\test_utils.py .....................................             [ 45%]

tests\unit\argumentation_analysis\analytics\test_stats_calculator.py F.. [ 45%]

F.FF                                                                     [ 46%]

tests\unit\argumentation_analysis\analytics\test_text_analyzer.py ....   [ 46%]

tests\unit\argumentation_analysis\mocks\test_advanced_tools.py ......    [ 47%]

tests\unit\argumentation_analysis\mocks\test_argument_mining.py ........ [ 48%]

F.FF.F.FF                                                                [ 49%]

tests\unit\argumentation_analysis\mocks\test_bias_detection.py .....F... [ 50%]

......                                                                   [ 50%]

tests\unit\argumentation_analysis\mocks\test_claim_mining.py .....FF..F. [ 51%]

...                                                                      [ 52%]

tests\unit\argumentation_analysis\mocks\test_clarity_scoring.py ....FFFF [ 53%]

F..F                                                                     [ 53%]

tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py ...FF [ 54%]

FFFF.                                                                    [ 54%]

tests\unit\argumentation_analysis\mocks\test_emotional_tone_analysis.py . [ 54%]

....FF.F..                                                               [ 55%]

tests\unit\argumentation_analysis\mocks\test_engagement_analysis.py .... [ 56%]

..FF...F.                                                                [ 57%]

tests\unit\argumentation_analysis\mocks\test_evidence_detection.py ..... [ 57%]

..F.....FF                                                               [ 58%]

tests\unit\argumentation_analysis\mocks\test_fallacy_categorization.py . [ 58%]

..........                                                               [ 59%]

tests\unit\argumentation_analysis\mocks\test_fallacy_detection.py ...... [ 60%]

.                                                                        [ 60%]

tests\unit\argumentation_analysis\mocks\test_rhetorical_analysis.py .... [ 61%]

..F.F..                                                                  [ 61%]

tests\unit\argumentation_analysis\nlp\test_embedding_utils.py ....F...F  [ 62%]

tests\unit\argumentation_analysis\orchestration\test_advanced_analyzer.py F [ 62%]

.....                                                                    [ 63%]

tests\unit\argumentation_analysis\pipelines\test_advanced_rhetoric.py F. [ 63%]

...                                                                      [ 64%]

tests\unit\argumentation_analysis\pipelines\test_analysis_pipeline.py .. [ 64%]

...                                                                      [ 64%]

tests\unit\argumentation_analysis\reporting\test_summary_generator.py .. [ 64%]

                                                                         [ 64%]

tests\unit\argumentation_analysis\service_setup\test_analysis_services.py . [ 64%]

......                                                                   [ 65%]

tests\unit\argumentation_analysis\utils\core_utils\test_cli_utils.py ... [ 65%]

.....................                                                    [ 68%]

tests\unit\argumentation_analysis\utils\core_utils\test_crypto_utils.py . [ 68%]

............                                                             [ 69%]

tests\unit\argumentation_analysis\utils\core_utils\test_file_utils.py .. [ 69%]

..........................................................               [ 75%]

tests\unit\argumentation_analysis\utils\core_utils\test_logging_utils.py . [ 75%]

.........                                                                [ 76%]

tests\unit\argumentation_analysis\utils\core_utils\test_network_utils.py . [ 76%]

.......                                                                  [ 77%]

tests\unit\argumentation_analysis\utils\core_utils\test_reporting_utils.py . [ 77%]

................                                                         [ 79%]

tests\unit\argumentation_analysis\utils\core_utils\test_system_utils.py . [ 79%]

.......                                                                  [ 80%]

tests\unit\argumentation_analysis\utils\core_utils\test_text_utils.py .. [ 80%]

..................                                                       [ 82%]

tests\unit\argumentation_analysis\utils\dev_tools\test_code_formatting_utils.py . [ 82%]

.....                                                                    [ 83%]

tests\unit\argumentation_analysis\utils\dev_tools\test_code_validation.py . [ 83%]

....s..                                                                  [ 84%]

tests\unit\argumentation_analysis\utils\dev_tools\test_encoding_utils.py . [ 84%]

.......                                                                  [ 84%]

tests\unit\argumentation_analysis\utils\dev_tools\test_env_checks.py ... [ 85%]

.F................                                                       [ 87%]

tests\unit\argumentation_analysis\utils\dev_tools\test_format_utils.py . [ 87%]

.....                                                                    [ 87%]

tests\unit\argumentation_analysis\utils\dev_tools\test_mock_utils.py ... [ 88%]

....                                                                     [ 88%]

tests\unit\argumentation_analysis\utils\test_analysis_comparison.py ...F [ 88%]

F...                                                                     [ 89%]

tests\unit\argumentation_analysis\utils\test_data_generation.py ........ [ 90%]

..                                                                       [ 90%]

tests\unit\argumentation_analysis\utils\test_data_loader.py .....        [ 90%]

tests\unit\argumentation_analysis\utils\test_data_processing_utils.py F. [ 91%]

FF.F.                                                                    [ 91%]

tests\unit\argumentation_analysis\utils\test_error_estimation.py F...    [ 92%]

tests\unit\argumentation_analysis\utils\test_metrics_aggregation.py .F.  [ 92%]

tests\unit\argumentation_analysis\utils\test_metrics_extraction.py ..... [ 92%]

..............FF..                                                       [ 94%]

tests\unit\argumentation_analysis\utils\test_report_generator.py ...     [ 95%]

tests\unit\argumentation_analysis\utils\test_text_processing.py .......F [ 96%]

....                                                                     [ 96%]

tests\unit\argumentation_analysis\utils\test_visualization_generator.py . [ 96%]

s.                                                                       [ 96%]

tests\utils\test_crypto_utils.py ..................                      [ 98%]

tests\utils\test_fetch_service_errors.py ............                    [100%]



=================================== ERRORS ====================================

__ ERROR at setup of test_run_extract_repair_pipeline_successful_run_no_save __

file D:\2025-Epita-Intelligence-Symbolique-4\tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py, line 68

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.create_llm_service")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.DefinitionService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.CryptoService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.CacheService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.ExtractService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.FetchService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.repair_extract_markers", new_callable=AsyncMock)

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.generate_marker_repair_report")

  @pytest.mark.asyncio

  async def test_run_extract_repair_pipeline_successful_run_no_save(

      mock_generate_marker_repair_report: MagicMock,

      mock_repair_extract_markers: AsyncMock,

      MockFetchService: MagicMock,

      MockExtractService: MagicMock,

      MockCacheService: MagicMock,

      MockCryptoService: MagicMock,

      MockDefinitionService: MagicMock,

      mock_create_llm_service: MagicMock,

      mock_project_root: Path,

      mock_llm_service: MagicMock, # Fixture pour configurer le retour de create_llm_service

      # mock_core_services: Dict[str, MagicMock], # Moins utile maintenant

      mock_definition_service_fixture: MagicMock # Fixture pour configurer le comportement de l'instance mockée de DefinitionService

  ):

      """Teste une exécution réussie du pipeline sans sauvegarde."""

      mock_create_llm_service.return_value = mock_llm_service



      # Configurer les instances mockées retournées par les classes patchées

      mock_crypto_instance = MagicMock()

      MockCryptoService.return_value = mock_crypto_instance



      mock_cache_instance = MagicMock()

      MockCacheService.return_value = mock_cache_instance



      mock_extract_instance = MagicMock()

      MockExtractService.return_value = mock_extract_instance



      mock_fetch_instance = MagicMock()

      MockFetchService.return_value = mock_fetch_instance



      # Utiliser la configuration de la fixture mock_definition_service_fixture pour l'instance mockée

      # mock_definition_service_fixture est déjà configurée avec load_definitions, save_definitions etc.

      MockDefinitionService.return_value = mock_definition_service_fixture



      # Assurer que load_definitions retourne un tuple

      sample_source = SourceDefinition(source_name="Test Source", source_type="text", schema="file", host_parts=[], path="", extracts=[])

      sample_defs = ExtractDefinitions(sources=[sample_source])

      mock_definition_service_fixture.load_definitions.return_value = (sample_defs, None)





      mock_repair_extract_markers.return_value = (sample_defs, [{"some_result": "data"}])



      output_report_path = str(mock_project_root / "repair_report.html")



      await run_extract_repair_pipeline(

          project_root_dir=mock_project_root,

          output_report_path_str=output_report_path,

          save_changes=False,

          hitler_only=False,

          custom_input_path_str=None,

          output_json_path_str=None

      )



      mock_create_llm_service.assert_called_once()



      # Vérifier que les constructeurs des services ont été appelés (une fois chacun)

      MockDefinitionService.assert_called_once()

      MockCryptoService.assert_called_once()

      MockCacheService.assert_called_once()

      MockExtractService.assert_called_once()

      MockFetchService.assert_called_once()



      # Vérifier les appels sur les instances mockées

      mock_definition_service_fixture.load_definitions.assert_called_once()

      mock_repair_extract_markers.assert_called_once()

      # Vérifier les arguments de repair_extract_markers (le premier est extract_definitions)

      assert isinstance(mock_repair_extract_markers.call_args[0][0], ExtractDefinitions)

      assert mock_repair_extract_markers.call_args[0][1] == mock_llm_service # llm_service

      assert mock_repair_extract_markers.call_args[0][2] == mock_fetch_instance # fetch_service instance

      assert mock_repair_extract_markers.call_args[0][3] == mock_extract_instance # extract_service instance



      mock_generate_marker_repair_report.assert_called_once()

      assert mock_generate_marker_repair_report.call_args[0][0] == [{"some_result": "data"}] # results

      assert mock_generate_marker_repair_report.call_args[0][1] == output_report_path # output_file_str



      mock_definition_service_fixture.save_definitions.assert_not_called()

      mock_definition_service_fixture.export_definitions_to_json.assert_not_called()

E       fixture 'mock_definition_service_fixture' not found

>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_core_services, mock_definition_service, mock_extract_service, mock_fetch_service, mock_llm_service, mock_project_root, mock_sk_kernel, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_extracts_for_repair, session_mocker, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory

>       use 'pytest --fixtures [testpath]' for help on them.



D:\2025-Epita-Intelligence-Symbolique-4\tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py:68

_ ERROR at setup of test_run_extract_repair_pipeline_with_save_and_json_export _

file D:\2025-Epita-Intelligence-Symbolique-4\tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py, line 156

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.create_llm_service")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.DefinitionService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.CryptoService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.CacheService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.ExtractService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.FetchService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.repair_extract_markers", new_callable=AsyncMock)

  @patch("argumentation_analysis.utils.extract_repair.marker_repair_logic.generate_report") # Cible l'emplacement original

  @pytest.mark.asyncio

  async def test_run_extract_repair_pipeline_with_save_and_json_export(

      mock_generate_marker_repair_report: MagicMock, # Renommé pour correspondre au nom importé

      mock_repair_extract_markers: AsyncMock,  # Renommé pour correspondre au nom importé

      MockFetchService: MagicMock,

      MockExtractService: MagicMock,

      MockCacheService: MagicMock,

      MockCryptoService: MagicMock,

      MockDefinitionService: MagicMock,

      mock_create_llm_service: MagicMock, # Renommé pour correspondre au nom importé

      mock_project_root: Path,

      mock_llm_service: MagicMock, # Fixture

      mock_definition_service_fixture: MagicMock # Fixture

  ):

      """Teste le pipeline avec sauvegarde et export JSON."""

      mock_create_llm_service.return_value = mock_llm_service



      # Configurer les instances mockées

      MockCryptoService.return_value = MagicMock()

      MockCacheService.return_value = MagicMock()

      MockExtractService.return_value = MagicMock()

      MockFetchService.return_value = MagicMock()

      MockDefinitionService.return_value = mock_definition_service_fixture # Utiliser la fixture configurée



      updated_defs_mock = ExtractDefinitions(sources=[SourceDefinition(source_name="Updated", source_type="text", schema="file", host_parts=[], path="", extracts=[])])

      # Assurer que load_definitions retourne un tuple correct

      mock_definition_service_fixture.load_definitions.return_value = (updated_defs_mock, None)

      mock_repair_extract_markers.return_value = (updated_defs_mock, [])



      output_report_path = str(mock_project_root / "report.html")

      output_json_path = str(mock_project_root / "updated.json")



      await run_extract_repair_pipeline(

          project_root_dir=mock_project_root,

          output_report_path_str=output_report_path,

          save_changes=True,

          hitler_only=False,

          custom_input_path_str=None,

          output_json_path_str=output_json_path

      )



      mock_definition_service_fixture.save_definitions.assert_called_once_with(updated_defs_mock)

      mock_definition_service_fixture.export_definitions_to_json.assert_called_once_with(

          updated_defs_mock, Path(output_json_path)

      )

      mock_generate_marker_repair_report.assert_called_once() # Vérifier qu'il est appelé

E       fixture 'mock_definition_service_fixture' not found

>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_core_services, mock_definition_service, mock_extract_service, mock_fetch_service, mock_llm_service, mock_project_root, mock_sk_kernel, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_extracts_for_repair, session_mocker, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory

>       use 'pytest --fixtures [testpath]' for help on them.



D:\2025-Epita-Intelligence-Symbolique-4\tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py:156

____ ERROR at setup of test_run_extract_repair_pipeline_hitler_only_filter ____

file D:\2025-Epita-Intelligence-Symbolique-4\tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py, line 212

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.create_llm_service")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.DefinitionService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.CryptoService") # Ajouter les autres patchs de service

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.CacheService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.ExtractService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.FetchService")

  @patch("argumentation_analysis.utils.dev_tools.repair_utils.repair_extract_markers", new_callable=AsyncMock)

  @pytest.mark.asyncio

  async def test_run_extract_repair_pipeline_hitler_only_filter(

      mock_repair_extract_markers: AsyncMock, # Renommé

      MockFetchService: MagicMock,

      MockExtractService: MagicMock,

      MockCacheService: MagicMock,

      MockCryptoService: MagicMock,

      MockDefinitionService: MagicMock,

      mock_create_llm_service: MagicMock, # Renommé

      mock_project_root: Path,

      mock_llm_service: MagicMock, # Fixture

      mock_definition_service_fixture: MagicMock # Fixture

  ):

      """Teste le filtrage --hitler-only."""

      mock_create_llm_service.return_value = mock_llm_service



      # Configurer les instances mockées

      MockCryptoService.return_value = MagicMock()

      MockCacheService.return_value = MagicMock()

      MockExtractService.return_value = MagicMock()

      MockFetchService.return_value = MagicMock()

      MockDefinitionService.return_value = mock_definition_service_fixture



      # Configurer mock_definition_service_fixture pour retourner plusieurs sources

      sources_data = [

          SourceDefinition(source_name="Discours d'Hitler 1", source_type="text", schema="file", host_parts=[], path="", extracts=[]),

          SourceDefinition(source_name="Autre Discours", source_type="text", schema="file", host_parts=[], path="", extracts=[]),

          SourceDefinition(source_name="Texte Hitler sur la fin", source_type="text", schema="file", host_parts=[], path="", extracts=[])

      ]

      mock_definition_service_fixture.load_definitions.return_value = (ExtractDefinitions(sources=sources_data), None)



      mock_repair_extract_markers.return_value = (ExtractDefinitions(sources=[]), []) # Peu importe le retour ici



      await run_extract_repair_pipeline(

          project_root_dir=mock_project_root,

          output_report_path_str=str(mock_project_root / "report.html"),

          save_changes=False,

          hitler_only=True, # Activer le filtre

          custom_input_path_str=None,

          output_json_path_str=None

      )



      mock_repair_extract_markers.assert_called_once()

      # Vérifier que les définitions passées à repair_extract_markers sont filtrées

      called_with_definitions = mock_repair_markers.call_args[0][0]

      assert isinstance(called_with_definitions, ExtractDefinitions)

      assert len(called_with_definitions.sources) == 2 # Seules les sources "Hitler"

      assert called_with_definitions.sources[0].source_name == "Discours d'Hitler 1"

      assert called_with_definitions.sources[1].source_name == "Texte Hitler sur la fin"

E       fixture 'mock_definition_service_fixture' not found

>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_core_services, mock_definition_service, mock_extract_service, mock_fetch_service, mock_llm_service, mock_project_root, mock_sk_kernel, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_extracts_for_repair, session_mocker, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory

>       use 'pytest --fixtures [testpath]' for help on them.



D:\2025-Epita-Intelligence-Symbolique-4\tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py:212

================================== FAILURES ===================================

_________________ TestInformalDefinitions.test_initialization _________________



self = <tests.agents.core.informal.test_informal_definitions.TestInformalDefinitions object at 0x000001C5C5103250>

informal_analysis_plugin_instance = <argumentation_analysis.agents.core.informal.informal_definitions.InformalAnalysisPlugin object at 0x000001C5D3775750>



    def test_initialization(self, informal_analysis_plugin_instance):

        plugin = informal_analysis_plugin_instance

        assert plugin is not None

        assert plugin._logger is not None # pylint: disable=protected-access

        # Vérifier que le chemin du fichier de taxonomie est défini et que le DataFrame est chargé

        assert hasattr(plugin, '_current_taxonomy_path') # pylint: disable=protected-access

        assert plugin._current_taxonomy_path is not None # pylint: disable=protected-access

>       assert plugin._get_taxonomy_dataframe() is not None # pylint: disable=protected-access



tests\agents\core\informal\test_informal_definitions.py:40: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

argumentation_analysis\agents\core\informal\informal_definitions.py:216: in _get_taxonomy_dataframe

    self._taxonomy_df_cache = self._internal_load_and_prepare_dataframe()

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



self = <argumentation_analysis.agents.core.informal.informal_definitions.InformalAnalysisPlugin object at 0x000001C5D3775750>



    def _internal_load_and_prepare_dataframe(self) -> pd.DataFrame:

        """

        Charge et prépare le DataFrame de taxonomie des sophismes à partir d'un fichier CSV.

    

        Utilise le chemin `self._current_taxonomy_path` déterminé lors de l'initialisation.

        L'index du DataFrame est défini sur la colonne 'PK' et converti en entier si possible.

    

        :return: Un DataFrame pandas contenant la taxonomie des sophismes.

        :rtype: pd.DataFrame

        :raises Exception: Si une erreur survient pendant le chargement ou la préparation.

        """

        self._logger.info(f"Chargement et préparation du DataFrame de taxonomie depuis: {self._current_taxonomy_path}...")

    

        try:

            # Charger le fichier CSV en utilisant load_csv_file de project_core

            df = load_csv_file(self._current_taxonomy_path)

    

            if df is None:

                self._logger.error(f"Échec du chargement du fichier CSV depuis {self._current_taxonomy_path}. load_csv_file a retourné None.")

>               raise Exception(f"Impossible de charger la taxonomie depuis {self._current_taxonomy_path}")

E               Exception: Impossible de charger la taxonomie depuis C:\Users\MYIA\AppData\Local\Temp\pytest-of-MYIA\pytest-309\test_initialization0\test_data\test_taxonomy.csv



argumentation_analysis\agents\core\informal\informal_definitions.py:134: Exception

------------------------------ Captured log call ------------------------------

ERROR    App.ProjectCore.FileLoaders:file_loaders.py:132 ❌ Erreur inattendue lors du chargement du fichier CSV C:\Users\MYIA\AppData\Local\Temp\pytest-of-MYIA\pytest-309\test_initialization0\test_data\test_taxonomy.csv: No module named 'numpy.rec'; 'numpy' is not a package

Traceback (most recent call last):

  File "D:\2025-Epita-Intelligence-Symbolique-4\argumentation_analysis\utils\core_utils\file_loaders.py", line 122, in load_csv_file

    df = pd.read_csv(file_path, encoding='utf-8')

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv

    return _read(filepath_or_buffer, kwds)

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\pandas\io\parsers\readers.py", line 626, in _read

    return parser.read(nrows)

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\pandas\io\parsers\readers.py", line 1968, in read

    df = DataFrame(

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\pandas\core\frame.py", line 778, in __init__

    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\pandas\core\internals\construction.py", line 444, in dict_to_mgr

    missing = arrays.isna()

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\pandas\core\series.py", line 5775, in isna

    return NDFrame.isna(self)

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\pandas\core\generic.py", line 8754, in isna

    return isna(self).__finalize__(self, method="isna")

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\pandas\core\dtypes\missing.py", line 178, in isna

    return _isna(obj)

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\pandas\core\dtypes\missing.py", line 216, in _isna

    result = _isna_array(obj._values, inf_as_na=inf_as_na)

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\pandas\core\dtypes\missing.py", line 288, in _isna_array

    elif isinstance(values, np.rec.recarray):

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\numpy\__init__.py", line 367, in __getattr__

    import numpy.rec as rec

ModuleNotFoundError: No module named 'numpy.rec'; 'numpy' is not a package

ERROR    InformalAnalysisPlugin:informal_definitions.py:133 Échec du chargement du fichier CSV depuis C:\Users\MYIA\AppData\Local\Temp\pytest-of-MYIA\pytest-309\test_initialization0\test_data\test_taxonomy.csv. load_csv_file a retourné None.

ERROR    InformalAnalysisPlugin:informal_definitions.py:202 Erreur lors du chargement ou de la préparation de la taxonomie depuis C:\Users\MYIA\AppData\Local\Temp\pytest-of-MYIA\pytest-309\test_initialization0\test_data\test_taxonomy.csv: Impossible de charger la taxonomie depuis C:\Users\MYIA\AppData\Local\Temp\pytest-of-MYIA\pytest-309\test_initialization0\test_data\test_taxonomy.csv

___________ TestPropositionalLogicAgent.test_execute_query_accepted ___________



self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_execute_query_accepted>



    def test_execute_query_accepted(self):

        """Test de l'exécution d'une requête acceptée."""

        belief_set_obj = PropositionalBeliefSet("a => b")

>       if not self.use_real_jpype:

E       AttributeError: 'TestPropositionalLogicAgent' object has no attribute 'use_real_jpype'



tests\agents\core\logic\test_propositional_logic_agent.py:181: AttributeError

_________ TestPropositionalLogicAgent.test_execute_query_error_tweety _________



self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_execute_query_error_tweety>



    def test_execute_query_error_tweety(self):

        """Test de l'exécution d'une requête avec erreur de Tweety."""

        belief_set_obj = PropositionalBeliefSet("a => b")

>       if not self.use_real_jpype:

E       AttributeError: 'TestPropositionalLogicAgent' object has no attribute 'use_real_jpype'



tests\agents\core\logic\test_propositional_logic_agent.py:219: AttributeError

_______ TestPropositionalLogicAgent.test_execute_query_invalid_formula ________



self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_execute_query_invalid_formula>



    def test_execute_query_invalid_formula(self):

        """Test de l'exécution d'une requête avec une formule invalide."""

        belief_set_obj = PropositionalBeliefSet("a => b")

>       if not self.use_real_jpype:

E       AttributeError: 'TestPropositionalLogicAgent' object has no attribute 'use_real_jpype'



tests\agents\core\logic\test_propositional_logic_agent.py:239: AttributeError

___________ TestPropositionalLogicAgent.test_execute_query_rejected ___________



self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_execute_query_rejected>



    def test_execute_query_rejected(self):

        """Test de l'exécution d'une requête rejetée."""

        belief_set_obj = PropositionalBeliefSet("a => b")

>       if not self.use_real_jpype:

E       AttributeError: 'TestPropositionalLogicAgent' object has no attribute 'use_real_jpype'



tests\agents\core\logic\test_propositional_logic_agent.py:201: AttributeError

______________ TestPropositionalLogicAgent.test_generate_queries ______________



args = (<test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_generate_queries>,)

kwargs = {}



    def wrapper(*args, **kwargs):

>       asyncio.run(f(*args, **kwargs))



tests\agents\core\logic\test_propositional_logic_agent.py:303: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\asyncio\runners.py:44: in run

    return loop.run_until_complete(main)

C:\Users\MYIA\.conda\envs\projet-is\lib\asyncio\base_events.py:649: in run_until_complete

    return future.result()

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_generate_queries>



    async def test_generate_queries(self):

        """Test de la génération de requêtes."""

        mock_sk_result = MagicMock()

        mock_sk_result.__str__.return_value = "a\nb\na => b"

        self.kernel.invoke.return_value = mock_sk_result

>       if not self.use_real_jpype:

E       AttributeError: 'TestPropositionalLogicAgent' object has no attribute 'use_real_jpype'



tests\agents\core\logic\test_propositional_logic_agent.py:139: AttributeError

______ TestPropositionalLogicAgent.test_text_to_belief_set_empty_result _______



args = (<test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_text_to_belief_set_empty_result>,)

kwargs = {}



    def wrapper(*args, **kwargs):

>       asyncio.run(f(*args, **kwargs))



tests\agents\core\logic\test_propositional_logic_agent.py:303: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\asyncio\runners.py:44: in run

    return loop.run_until_complete(main)

C:\Users\MYIA\.conda\envs\projet-is\lib\asyncio\base_events.py:649: in run_until_complete

    return future.result()

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_text_to_belief_set_empty_result>



    async def test_text_to_belief_set_empty_result(self):

        """Test de la conversion de texte en ensemble de croyances avec résultat vide."""

        mock_sk_result = MagicMock()

        mock_sk_result.__str__.return_value = ""

        self.kernel.invoke.return_value = mock_sk_result

    

        belief_set, message = await self.agent.text_to_belief_set("Texte de test")

    

        self.kernel.invoke.assert_called_once()

>       if not self.use_real_jpype:

E       AttributeError: 'TestPropositionalLogicAgent' object has no attribute 'use_real_jpype'



tests\agents\core\logic\test_propositional_logic_agent.py:102: AttributeError

------------------------------ Captured log call ------------------------------

ERROR    agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:169 La conversion a produit un ensemble de croyances vide.

___ TestPropositionalLogicAgent.test_text_to_belief_set_invalid_belief_set ____



args = (<test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_text_to_belief_set_invalid_belief_set>,)

kwargs = {}



    def wrapper(*args, **kwargs):

>       asyncio.run(f(*args, **kwargs))



tests\agents\core\logic\test_propositional_logic_agent.py:303: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\asyncio\runners.py:44: in run

    return loop.run_until_complete(main)

C:\Users\MYIA\.conda\envs\projet-is\lib\asyncio\base_events.py:649: in run_until_complete

    return future.result()

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_text_to_belief_set_invalid_belief_set>



    async def test_text_to_belief_set_invalid_belief_set(self):

        """Test de la conversion de texte en ensemble de croyances avec ensemble invalide."""

        mock_sk_result = MagicMock()

        mock_sk_result.__str__.return_value = "invalid_pl_syntax {"

        self.kernel.invoke.return_value = mock_sk_result

>       if not self.use_real_jpype:

E       AttributeError: 'TestPropositionalLogicAgent' object has no attribute 'use_real_jpype'



tests\agents\core\logic\test_propositional_logic_agent.py:114: AttributeError

__________ TestPropositionalLogicAgent.test_validate_formula_invalid __________



self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_validate_formula_invalid>



    def test_validate_formula_invalid(self):

        """Test de la validation d'une formule invalide."""

>       if not self.use_real_jpype:

E       AttributeError: 'TestPropositionalLogicAgent' object has no attribute 'use_real_jpype'



tests\agents\core\logic\test_propositional_logic_agent.py:289: AttributeError

__________________ test_watson_logic_assistant_instanciation __________________



mock_kernel = <MagicMock spec='Kernel' id='1949182410272'>

mock_tweety_bridge = <MagicMock id='1949178330992'>

mocker = <pytest_mock.plugin.MockerFixture object at 0x000001C5D41671C0>



    def test_watson_logic_assistant_instanciation(mock_kernel: MagicMock, mock_tweety_bridge: MagicMock, mocker: MagicMock) -> None:

        """

        Teste l'instanciation correcte de WatsonLogicAssistant et vérifie l'appel au constructeur parent.

        """

        # Espionner le constructeur de la classe parente PropositionalLogicAgent

        spy_super_init = mocker.spy(PropositionalLogicAgent, "__init__")

    

        # Patcher l'initialisation de TweetyBridge dans PropositionalLogicAgent pour utiliser notre mock

        # Cela est crucial car PropositionalLogicAgent.__init__ appelle self.setup_agent_components,

        # qui à son tour initialise self._tweety_bridge.

        with patch('argumentation_analysis.agents.core.logic.propositional_logic_agent.TweetyBridge', return_value=mock_tweety_bridge):

            # Instancier WatsonLogicAssistant

>           agent = WatsonLogicAssistant(kernel=mock_kernel, agent_name=TEST_AGENT_NAME)



tests\agents\core\logic\test_watson_logic_assistant.py:38: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



self = <argumentation_analysis.agents.core.logic.watson_logic_assistant.WatsonLogicAssistant object at 0x000001C5D41956C0>

kernel = <MagicMock spec='Kernel' id='1949182410272'>

agent_name = 'TestWatsonAssistant'

system_prompt = "Vous êtes le Dr. John Watson, un logicien rigoureux et l'assistant de confiance de Sherlock Holmes.\nVotre rôle est d...tation en langage naturel dans l'état via `add_deduction_result`.\n4. Marquez la tâche correspondante comme complétée."



    def __init__(self, kernel: Kernel, agent_name: str = "WatsonLogicAssistant", system_prompt: Optional[str] = WATSON_LOGIC_ASSISTANT_SYSTEM_PROMPT):

        """

        Initialise une instance de WatsonLogicAssistant.

    

        Args:

            kernel: Le kernel Semantic Kernel à utiliser.

            agent_name: Le nom de l'agent.

            system_prompt: Le prompt système pour guider l'agent.

            # tweety_bridge_instance: Une instance de TweetyBridge si elle doit être

            #                         partagée ou configurée spécifiquement.

            #                         La classe parente PropositionalLogicAgent gère

            #                         sa propre instance de TweetyBridge par défaut.

        """

        super().__init__(kernel, agent_name=agent_name)

        self.kernel = kernel  # Stocker la référence au kernel

>       self.logger = logging.getLogger(agent_name) # Assurer un logger spécifique

E       AttributeError: can't set attribute 'logger'



argumentation_analysis\agents\core\logic\watson_logic_assistant.py:47: AttributeError

________ test_watson_logic_assistant_instanciation_with_custom_prompt _________



mock_kernel = <MagicMock spec='Kernel' id='1949178615392'>

mock_tweety_bridge = <MagicMock id='1949178611216'>

mocker = <pytest_mock.plugin.MockerFixture object at 0x000001C5D420E0B0>



    def test_watson_logic_assistant_instanciation_with_custom_prompt(mock_kernel: MagicMock, mock_tweety_bridge: MagicMock, mocker: MagicMock) -> None:

        """

        Teste l'instanciation de WatsonLogicAssistant avec un prompt système personnalisé.

        """

        custom_prompt = "Instructions personnalisées pour Watson."

        spy_super_init = mocker.spy(PropositionalLogicAgent, "__init__")

    

        with patch('argumentation_analysis.agents.core.logic.propositional_logic_agent.TweetyBridge', return_value=mock_tweety_bridge):

>           agent = WatsonLogicAssistant(kernel=mock_kernel, agent_name=TEST_AGENT_NAME, system_prompt=custom_prompt)



tests\agents\core\logic\test_watson_logic_assistant.py:70: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



self = <argumentation_analysis.agents.core.logic.watson_logic_assistant.WatsonLogicAssistant object at 0x000001C5D4211630>

kernel = <MagicMock spec='Kernel' id='1949178615392'>

agent_name = 'TestWatsonAssistant'

system_prompt = 'Instructions personnalisées pour Watson.'



    def __init__(self, kernel: Kernel, agent_name: str = "WatsonLogicAssistant", system_prompt: Optional[str] = WATSON_LOGIC_ASSISTANT_SYSTEM_PROMPT):

        """

        Initialise une instance de WatsonLogicAssistant.

    

        Args:

            kernel: Le kernel Semantic Kernel à utiliser.

            agent_name: Le nom de l'agent.

            system_prompt: Le prompt système pour guider l'agent.

            # tweety_bridge_instance: Une instance de TweetyBridge si elle doit être

            #                         partagée ou configurée spécifiquement.

            #                         La classe parente PropositionalLogicAgent gère

            #                         sa propre instance de TweetyBridge par défaut.

        """

        super().__init__(kernel, agent_name=agent_name)

        self.kernel = kernel  # Stocker la référence au kernel

>       self.logger = logging.getLogger(agent_name) # Assurer un logger spécifique

E       AttributeError: can't set attribute 'logger'



argumentation_analysis\agents\core\logic\watson_logic_assistant.py:47: AttributeError

_____________ test_watson_logic_assistant_default_name_and_prompt _____________



mock_kernel = <MagicMock spec='Kernel' id='1949179203872'>

mock_tweety_bridge = <MagicMock id='1949179206944'>

mocker = <pytest_mock.plugin.MockerFixture object at 0x000001C5D41F16C0>



    def test_watson_logic_assistant_default_name_and_prompt(mock_kernel: MagicMock, mock_tweety_bridge: MagicMock, mocker: MagicMock) -> None:

        """

        Teste l'instanciation de WatsonLogicAssistant avec le nom et le prompt par défaut.

        """

        spy_super_init = mocker.spy(PropositionalLogicAgent, "__init__")

    

        with patch('argumentation_analysis.agents.core.logic.propositional_logic_agent.TweetyBridge', return_value=mock_tweety_bridge):

>           agent = WatsonLogicAssistant(kernel=mock_kernel)



tests\agents\core\logic\test_watson_logic_assistant.py:93: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



self = <argumentation_analysis.agents.core.logic.watson_logic_assistant.WatsonLogicAssistant object at 0x000001C5D4211510>

kernel = <MagicMock spec='Kernel' id='1949179203872'>

agent_name = 'WatsonLogicAssistant'

system_prompt = "Vous êtes le Dr. John Watson, un logicien rigoureux et l'assistant de confiance de Sherlock Holmes.\nVotre rôle est d...tation en langage naturel dans l'état via `add_deduction_result`.\n4. Marquez la tâche correspondante comme complétée."



    def __init__(self, kernel: Kernel, agent_name: str = "WatsonLogicAssistant", system_prompt: Optional[str] = WATSON_LOGIC_ASSISTANT_SYSTEM_PROMPT):

        """

        Initialise une instance de WatsonLogicAssistant.

    

        Args:

            kernel: Le kernel Semantic Kernel à utiliser.

            agent_name: Le nom de l'agent.

            system_prompt: Le prompt système pour guider l'agent.

            # tweety_bridge_instance: Une instance de TweetyBridge si elle doit être

            #                         partagée ou configurée spécifiquement.

            #                         La classe parente PropositionalLogicAgent gère

            #                         sa propre instance de TweetyBridge par défaut.

        """

        super().__init__(kernel, agent_name=agent_name)

        self.kernel = kernel  # Stocker la référence au kernel

>       self.logger = logging.getLogger(agent_name) # Assurer un logger spécifique

E       AttributeError: can't set attribute 'logger'



argumentation_analysis\agents\core\logic\watson_logic_assistant.py:47: AttributeError

______________________ test_get_agent_belief_set_content ______________________



mock_kernel = <MagicMock spec='Kernel' id='1949179047376'>

mock_tweety_bridge = <MagicMock id='1949179050160'>



    @pytest.mark.asyncio

    async def test_get_agent_belief_set_content(mock_kernel: MagicMock, mock_tweety_bridge: MagicMock) -> None:

        """

        Teste la méthode get_agent_belief_set_content de WatsonLogicAssistant.

        """

        with patch('argumentation_analysis.agents.core.logic.propositional_logic_agent.TweetyBridge', return_value=mock_tweety_bridge):

>           agent = WatsonLogicAssistant(kernel=mock_kernel, agent_name=TEST_AGENT_NAME)



tests\agents\core\logic\test_watson_logic_assistant.py:116: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



self = <argumentation_analysis.agents.core.logic.watson_logic_assistant.WatsonLogicAssistant object at 0x000001C5D4140190>

kernel = <MagicMock spec='Kernel' id='1949179047376'>

agent_name = 'TestWatsonAssistant'

system_prompt = "Vous êtes le Dr. John Watson, un logicien rigoureux et l'assistant de confiance de Sherlock Holmes.\nVotre rôle est d...tation en langage naturel dans l'état via `add_deduction_result`.\n4. Marquez la tâche correspondante comme complétée."



    def __init__(self, kernel: Kernel, agent_name: str = "WatsonLogicAssistant", system_prompt: Optional[str] = WATSON_LOGIC_ASSISTANT_SYSTEM_PROMPT):

        """

        Initialise une instance de WatsonLogicAssistant.

    

        Args:

            kernel: Le kernel Semantic Kernel à utiliser.

            agent_name: Le nom de l'agent.

            system_prompt: Le prompt système pour guider l'agent.

            # tweety_bridge_instance: Une instance de TweetyBridge si elle doit être

            #                         partagée ou configurée spécifiquement.

            #                         La classe parente PropositionalLogicAgent gère

            #                         sa propre instance de TweetyBridge par défaut.

        """

        super().__init__(kernel, agent_name=agent_name)

        self.kernel = kernel  # Stocker la référence au kernel

>       self.logger = logging.getLogger(agent_name) # Assurer un logger spécifique

E       AttributeError: can't set attribute 'logger'



argumentation_analysis\agents\core\logic\watson_logic_assistant.py:47: AttributeError

________________________ test_add_new_deduction_result ________________________



mock_kernel = <MagicMock spec='Kernel' id='1949178277280'>

mock_tweety_bridge = <MagicMock id='1949178270752'>



    @pytest.mark.asyncio

    async def test_add_new_deduction_result(mock_kernel: MagicMock, mock_tweety_bridge: MagicMock) -> None:

        """

        Teste la méthode add_new_deduction_result de WatsonLogicAssistant.

        """

        with patch('argumentation_analysis.agents.core.logic.propositional_logic_agent.TweetyBridge', return_value=mock_tweety_bridge):

>           agent = WatsonLogicAssistant(kernel=mock_kernel, agent_name=TEST_AGENT_NAME)



tests\agents\core\logic\test_watson_logic_assistant.py:190: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



self = <argumentation_analysis.agents.core.logic.watson_logic_assistant.WatsonLogicAssistant object at 0x000001C5D4210400>

kernel = <MagicMock spec='Kernel' id='1949178277280'>

agent_name = 'TestWatsonAssistant'

system_prompt = "Vous êtes le Dr. John Watson, un logicien rigoureux et l'assistant de confiance de Sherlock Holmes.\nVotre rôle est d...tation en langage naturel dans l'état via `add_deduction_result`.\n4. Marquez la tâche correspondante comme complétée."



    def __init__(self, kernel: Kernel, agent_name: str = "WatsonLogicAssistant", system_prompt: Optional[str] = WATSON_LOGIC_ASSISTANT_SYSTEM_PROMPT):

        """

        Initialise une instance de WatsonLogicAssistant.

    

        Args:

            kernel: Le kernel Semantic Kernel à utiliser.

            agent_name: Le nom de l'agent.

            system_prompt: Le prompt système pour guider l'agent.

            # tweety_bridge_instance: Une instance de TweetyBridge si elle doit être

            #                         partagée ou configurée spécifiquement.

            #                         La classe parente PropositionalLogicAgent gère

            #                         sa propre instance de TweetyBridge par défaut.

        """

        super().__init__(kernel, agent_name=agent_name)

        self.kernel = kernel  # Stocker la référence au kernel

>       self.logger = logging.getLogger(agent_name) # Assurer un logger spécifique

E       AttributeError: can't set attribute 'logger'



argumentation_analysis\agents\core\logic\watson_logic_assistant.py:47: AttributeError

__________________ test_sherlock_enquete_agent_instanciation __________________



mock_kernel = <MagicMock spec='Kernel' id='1949179001632'>

mocker = <pytest_mock.plugin.MockerFixture object at 0x000001C5D41F3160>



    def test_sherlock_enquete_agent_instanciation(mock_kernel: MagicMock, mocker: MagicMock) -> None:

        """

        Teste l'instanciation correcte de SherlockEnqueteAgent et vérifie l'appel au constructeur parent.

        """

        # Espionner le constructeur de la classe parente ProjectManagerAgent

        spy_super_init = mocker.spy(ProjectManagerAgent, "__init__")

    

        # Instancier SherlockEnqueteAgent

        agent = SherlockEnqueteAgent(kernel=mock_kernel, agent_name=TEST_AGENT_NAME)

    

        # Vérifier que l'agent est une instance de SherlockEnqueteAgent

        assert isinstance(agent, SherlockEnqueteAgent)

        assert agent.name == TEST_AGENT_NAME

        # Vérifier que le logger a été configuré avec le nom de l'agent

>       assert agent.logger.name == TEST_AGENT_NAME

E       AssertionError: assert 'agent.Sherlo...SherlockAgent' == 'TestSherlockAgent'

E         

E         - TestSherlockAgent

E         + agent.SherlockEnqueteAgent.TestSherlockAgent



tests\agents\core\pm\test_sherlock_enquete_agent.py:32: AssertionError

______________________ test_get_current_case_description ______________________



mock_kernel = <MagicMock spec='Kernel' id='1949178620864'>



    @pytest.mark.asyncio

    async def test_get_current_case_description(mock_kernel: MagicMock) -> None:

        """

        Teste la méthode get_current_case_description de SherlockEnqueteAgent.

        """

        agent = SherlockEnqueteAgent(kernel=mock_kernel, agent_name=TEST_AGENT_NAME)

    

        # Cas 1: invoke retourne un objet avec un attribut 'value'

        expected_description_value_attr = "Description de l'affaire (via value)"

        mock_invoke_result_value_attr = MagicMock()

        mock_invoke_result_value_attr.value = expected_description_value_attr

        mock_kernel.invoke = MagicMock(return_value=mock_invoke_result_value_attr) # Simule une coroutine

    

        description = await agent.get_current_case_description()

    

        mock_kernel.invoke.assert_called_once_with(

            plugin_name="EnqueteStatePlugin",

            function_name="get_case_description"

        )

>       assert description == expected_description_value_attr

E       assert "Erreur: Impo...de l'affaire." == 'Description ...e (via value)'

E         

E         - Description de l'affaire (via value)

E         + Erreur: Impossible de récupérer la description de l'affaire.



tests\agents\core\pm\test_sherlock_enquete_agent.py:101: AssertionError

------------------------------ Captured log call ------------------------------

ERROR    agent.SherlockEnqueteAgent.TestSherlockAgent:sherlock_enquete_agent.py:70 Erreur lors de la récupération de la description de l'affaire: object MagicMock can't be used in 'await' expression

___________________________ test_add_new_hypothesis ___________________________



mock_kernel = <MagicMock spec='Kernel' id='1949179122192'>



    @pytest.mark.asyncio

    async def test_add_new_hypothesis(mock_kernel: MagicMock) -> None:

        """

        Teste la méthode add_new_hypothesis de SherlockEnqueteAgent.

        """

        agent = SherlockEnqueteAgent(kernel=mock_kernel, agent_name=TEST_AGENT_NAME)

        hypothesis_text = "Le coupable est le Colonel Moutarde."

        confidence_score = 0.75

        expected_invoke_result = {"id": "hyp_123", "text": hypothesis_text, "confidence": confidence_score}

    

        # Cas 1: invoke retourne un objet avec un attribut 'value'

        mock_invoke_result_value_attr = MagicMock()

        mock_invoke_result_value_attr.value = expected_invoke_result

        mock_kernel.invoke = MagicMock(return_value=mock_invoke_result_value_attr)

    

>       result = await agent.add_new_hypothesis(hypothesis_text, confidence_score)

E       AttributeError: 'SherlockEnqueteAgent' object has no attribute 'add_new_hypothesis'



tests\agents\core\pm\test_sherlock_enquete_agent.py:150: AttributeError

________________________ test_setup_agents_successful _________________________



self = <MagicMock name='mock.add_service' id='1949183873696'>

args = (<MagicMock id='1949183672432'>,), kwargs = {}

msg = "Expected 'add_service' to be called once. Called 0 times."



    def assert_called_once_with(self, /, *args, **kwargs):

        """assert that the mock was called exactly once and that that call was

        with the specified arguments."""

        if not self.call_count == 1:

            msg = ("Expected '%s' to be called once. Called %s times.%s"

                   % (self._mock_name or 'mock',

                      self.call_count,

                      self._calls_repr()))

>           raise AssertionError(msg)

E           AssertionError: Expected 'add_service' to be called once. Called 0 times.



C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\mock.py:940: AssertionError



During handling of the above exception, another exception occurred:



mock_llm_service = <MagicMock id='1949183672432'>

mock_sk_kernel = <MagicMock spec='Kernel' id='1949183873648'>



    @pytest.mark.asyncio

    async def test_setup_agents_successful(mock_llm_service: MagicMock, mock_sk_kernel: MagicMock):

        """Teste la configuration réussie des agents."""

        mock_llm_service.service_id = "test_service_id" # Nécessaire pour get_prompt_execution_settings

    

        # ChatCompletionAgent n'est plus utilisé directement par setup_agents dans repair_utils.py

        # La fonction setup_agents retourne (None, None)

        # with patch("project_core.dev_utils.repair_utils.ChatCompletionAgent", spec=ChatCompletionAgent) as MockAgent:

            # repair_agent_instance = MagicMock()

            # validation_agent_instance = MagicMock()

            # MockAgent.side_effect = [repair_agent_instance, validation_agent_instance]

    

        repair_agent, validation_agent = await setup_agents(mock_llm_service, mock_sk_kernel)

    

>       mock_sk_kernel.add_service.assert_called_once_with(mock_llm_service)

E       AssertionError: Expected 'add_service' to be called once. Called 0 times.



tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py:351: AssertionError

------------------------------ Captured log call ------------------------------

WARNING  argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:62 setup_agents: ChatCompletionAgent est temporairement désactivé. Retour de (None, None).

________________ test_parametrized_dependency_import[sklearn] _________________



dependency = 'sklearn'



    @pytest.mark.use_real_numpy

    @pytest.mark.parametrize("dependency", PROJECT_DEPENDENCIES_PARAMETRIZED)

    def test_parametrized_dependency_import(dependency):

        """Vérifie que chaque dépendance majeure paramétrée peut être importée."""

        try:

>           importlib.import_module(dependency)



tests\environment_checks\test_core_dependencies.py:80: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\importlib\__init__.py:126: in import_module

    return _bootstrap._gcd_import(name[level:], package, level)

<frozen importlib._bootstrap>:1050: in _gcd_import

    ???

<frozen importlib._bootstrap>:1027: in _find_and_load

    ???

<frozen importlib._bootstrap>:1006: in _find_and_load_unlocked

    ???

<frozen importlib._bootstrap>:688: in _load_unlocked

    ???

<frozen importlib._bootstrap_external>:883: in exec_module

    ???

<frozen importlib._bootstrap>:241: in _call_with_frames_removed

    ???

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\sklearn\__init__.py:73: in <module>

    from .base import clone  # noqa: E402

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\sklearn\base.py:19: in <module>

    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\sklearn\utils\__init__.py:15: in <module>

    from ._chunking import gen_batches, gen_even_slices

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\sklearn\utils\_chunking.py:11: in <module>

    from ._param_validation import Interval, validate_params

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\sklearn\utils\_param_validation.py:14: in <module>

    from scipy.sparse import csr_matrix, issparse

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\scipy\sparse\__init__.py:300: in <module>

    from ._base import *

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\scipy\sparse\_base.py:5: in <module>

    from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\scipy\sparse\_sputils.py:10: in <module>

    from scipy._lib._util import np_long, np_ulong

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\scipy\_lib\_util.py:13: in <module>

    from scipy._lib._array_api import array_namespace, is_numpy, xp_size

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



    """Utility functions to use Python Array API compatible libraries.

    

    For the context about the Array API see:

    https://data-apis.org/array-api/latest/purpose_and_scope.html

    

    The SciPy use case of the Array API is described on the following page:

    https://data-apis.org/array-api/latest/use_cases.html#use-case-scipy

    """

    import os

    

    from types import ModuleType

    from typing import Any, Literal, TypeAlias

    

    import numpy as np

>   import numpy.typing as npt

E   ModuleNotFoundError: No module named 'numpy.typing'; 'numpy' is not a package



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\scipy\_lib\_array_api.py:15: ModuleNotFoundError



During handling of the above exception, another exception occurred:



dependency = 'sklearn'



    @pytest.mark.use_real_numpy

    @pytest.mark.parametrize("dependency", PROJECT_DEPENDENCIES_PARAMETRIZED)

    def test_parametrized_dependency_import(dependency):

        """Vérifie que chaque dépendance majeure paramétrée peut être importée."""

        try:

            importlib.import_module(dependency)

            # Optionnel: afficher la version si disponible

            # module = importlib.import_module(dependency)

            # print(f"{dependency} version: {getattr(module, '__version__', 'N/A')}")

        except ImportError as e:

>           pytest.fail(f"Échec de l'importation de la dépendance '{dependency}': {e}")

E           Failed: Échec de l'importation de la dépendance 'sklearn': No module named 'numpy.typing'; 'numpy' is not a package



tests\environment_checks\test_core_dependencies.py:85: Failed

_________________ test_parametrized_dependency_import[spacy] __________________



dependency = 'spacy'



    @pytest.mark.use_real_numpy

    @pytest.mark.parametrize("dependency", PROJECT_DEPENDENCIES_PARAMETRIZED)

    def test_parametrized_dependency_import(dependency):

        """Vérifie que chaque dépendance majeure paramétrée peut être importée."""

        try:

>           importlib.import_module(dependency)



tests\environment_checks\test_core_dependencies.py:80: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\importlib\__init__.py:126: in import_module

    return _bootstrap._gcd_import(name[level:], package, level)

<frozen importlib._bootstrap>:1050: in _gcd_import

    ???

<frozen importlib._bootstrap>:1027: in _find_and_load

    ???

<frozen importlib._bootstrap>:1006: in _find_and_load_unlocked

    ???

<frozen importlib._bootstrap>:688: in _load_unlocked

    ???

<frozen importlib._bootstrap_external>:883: in exec_module

    ???

<frozen importlib._bootstrap>:241: in _call_with_frames_removed

    ???

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\spacy\__init__.py:6: in <module>

    from .errors import setup_default_warnings

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\spacy\errors.py:3: in <module>

    from .compat import Literal

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\spacy\compat.py:39: in <module>

    from thinc.api import Optimizer  # noqa: F401

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\thinc\api.py:1: in <module>

    from .backends import (

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\thinc\backends\__init__.py:17: in <module>

    from .cupy_ops import CupyOps

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\thinc\backends\cupy_ops.py:16: in <module>

    from .numpy_ops import NumpyOps

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



>   ???

E   ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 32 from PyObject



thinc\\backends\\numpy_ops.pyx:1: ValueError



During handling of the above exception, another exception occurred:



dependency = 'spacy'



    @pytest.mark.use_real_numpy

    @pytest.mark.parametrize("dependency", PROJECT_DEPENDENCIES_PARAMETRIZED)

    def test_parametrized_dependency_import(dependency):

        """Vérifie que chaque dépendance majeure paramétrée peut être importée."""

        try:

            importlib.import_module(dependency)

            # Optionnel: afficher la version si disponible

            # module = importlib.import_module(dependency)

            # print(f"{dependency} version: {getattr(module, '__version__', 'N/A')}")

        except ImportError as e:

            pytest.fail(f"Échec de l'importation de la dépendance '{dependency}': {e}")

        except Exception as e:

>           pytest.fail(f"Erreur inattendue lors de l'importation de '{dependency}': {e}")

E           Failed: Erreur inattendue lors de l'importation de 'spacy': numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 32 from PyObject



tests\environment_checks\test_core_dependencies.py:87: Failed

_______________________ test_cluedo_orchestration_flow ________________________



thing = <module 'argumentation_analysis.orchestration' from 'D:\\2025-Epita-Intelligence-Symbolique-4\\argumentation_analysis\\orchestration\\__init__.py'>

comp = 'cluedo_orchestrator'

import_path = 'argumentation_analysis.orchestration.cluedo_orchestrator'



    def _dot_lookup(thing, comp, import_path):

        try:

>           return getattr(thing, comp)

E           AttributeError: module 'argumentation_analysis.orchestration' has no attribute 'cluedo_orchestrator'



C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\mock.py:1248: AttributeError



During handling of the above exception, another exception occurred:



    @pytest.mark.asyncio

    async def test_cluedo_orchestration_flow():

        """

        Teste le flux d'orchestration de base de cluedo_orchestrator.py.

        - Mock les agents pour retourner des réponses prédéfinies.

        - Vérifie que les agents sont appelés.

        - Vérifie que l'historique de la conversation contient des messages des agents.

        """

    

        # Mocker EnqueteCluedoState pour contrôler la solution et éviter la génération aléatoire si besoin

        mock_enquete_state_instance = MagicMock()

        mock_enquete_state_instance.nom_enquete_cluedo = "Test Case"

        mock_enquete_state_instance.elements_jeu_cluedo = {

            "suspects": ["A", "B"], "armes": ["X", "Y"], "lieux": ["1", "2"]

        }

        mock_enquete_state_instance.description_cas = "A test case."

        mock_enquete_state_instance.solution_enquete = {"suspect": "A", "arme": "X", "lieu": "1"} # Solution fixe

    

        # Mocker les instances des agents

        # SherlockEnqueteAgent

        mock_sherlock_instance = AsyncMock()

        mock_sherlock_instance.name = "Sherlock" # Important pour AgentGroupChat

        # Simuler la méthode que AgentGroupChat pourrait appeler (ex: invoke ou process_chat)

        # La méthode exacte dépend de l'implémentation de Agent et AgentGroupChat

        # Pour l'instant, on suppose une méthode `invoke` compatible avec le wrapper ou l'agent lui-même.

        async def sherlock_invoke_side_effect(prompt, **kwargs):

            # Simule une réponse de Sherlock

            return [{"role": "assistant", "name": "Sherlock", "content": "Sherlock: C'est élémentaire."}]

        mock_sherlock_instance.invoke = AsyncMock(side_effect=sherlock_invoke_side_effect)

        # Si les agents ont une méthode spécifique pour être appelés par AgentGroupChat, il faut la mocker.

        # Par exemple, si AgentGroupChat appelle directement une méthode comme `generate_reply(history)`

        # mock_sherlock_instance.generate_reply = AsyncMock(return_value="Sherlock: C'est élémentaire.")

    

    

        # WatsonLogicAssistant

        mock_watson_instance = AsyncMock()

        mock_watson_instance.name = "Watson" # Important pour AgentGroupChat

        async def watson_invoke_side_effect(prompt, **kwargs):

            # Simule une réponse de Watson

            return [{"role": "assistant", "name": "Watson", "content": "Watson: En effet, Sherlock."}]

        mock_watson_instance.invoke = AsyncMock(side_effect=watson_invoke_side_effect)

        # mock_watson_instance.generate_reply = AsyncMock(return_value="Watson: En effet, Sherlock.")

    

        # Mocker AgentGroupChat

        mock_group_chat_instance = AsyncMock()

        # La méthode `invoke` de AgentGroupChat retourne l'historique

        # On simule un historique simple basé sur les réponses mockées des agents

        simulated_history = [

            {"role": "user", "name": "User", "content": "L'enquête sur le meurtre au Manoir Tudor commence maintenant. Qui a des premières pistes ?"},

            {"role": "assistant", "name": "Sherlock", "content": "Sherlock: C'est élémentaire."},

            {"role": "assistant", "name": "Watson", "content": "Watson: En effet, Sherlock."},

            # Potentiellement un autre tour si max_turns=2 pour BalancedParticipationStrategy

            # et max_messages le permet.

            {"role": "assistant", "name": "Sherlock", "content": "Sherlock: Le coupable est évident."},

        ]

        mock_group_chat_instance.invoke = AsyncMock(return_value=simulated_history)

    

        # Patch des constructeurs et autres dépendances

>       with patch(f"{ORCHESTRATOR_MODULE_PATH}.Kernel", MagicMock()) as mock_kernel_constructor, \

             patch(f"{ORCHESTRATOR_MODULE_PATH}.EnqueteCluedoState", return_value=mock_enquete_state_instance) as mock_enquete_state_constructor, \

             patch(f"{ORCHESTRATOR_MODULE_PATH}.EnqueteStateManagerPlugin", MagicMock()) as mock_plugin_constructor, \

             patch(f"{ORCHESTRATOR_MODULE_PATH}.SherlockEnqueteAgent", return_value=mock_sherlock_instance) as mock_sherlock_constructor, \

             patch(f"{ORCHESTRATOR_MODULE_PATH}.WatsonLogicAssistant", return_value=mock_watson_instance) as mock_watson_constructor, \

             patch(f"{ORCHESTRATOR_MODULE_PATH}.AgentGroupChat", return_value=mock_group_chat_instance) as mock_group_chat_constructor, \

             patch(f"{ORCHESTRATOR_MODULE_PATH}.BalancedParticipationStrategy", MagicMock()) as mock_balanced_strat_constructor, \

             patch(f"{ORCHESTRATOR_MODULE_PATH}.SimpleTerminationStrategy", MagicMock()) as mock_simple_term_strat_constructor, \

             patch("builtins.print") as mock_print: # Mocker print pour éviter les sorties console pendant le test



tests\orchestration\test_cluedo_orchestrator.py:70: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\mock.py:1431: in __enter__

    self.target = self.getter()

C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\mock.py:1618: in <lambda>

    getter = lambda: _importer(target)

C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\mock.py:1261: in _importer

    thing = _dot_lookup(thing, comp, import_path)

C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\mock.py:1250: in _dot_lookup

    __import__(import_path)

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



    import asyncio

    import semantic_kernel as sk

    from semantic_kernel.kernel import Kernel

    

    from argumentation_analysis.core.enquete_states import EnqueteCluedoState

    from argumentation_analysis.orchestration.plugins.enquete_state_manager_plugin import EnqueteStateManagerPlugin

    from argumentation_analysis.agents.core.pm.sherlock_enquete_agent import SherlockEnqueteAgent

    from argumentation_analysis.agents.core.logic.watson_logic_assistant import WatsonLogicAssistant

    

    # Correction du chemin d'import pour AgentGroupChat et les stratégies

>   from semantic_kernel.experimental.group_chat import (

        AgentGroupChat,

        BalancedParticipationStrategy,

        SimpleTerminationStrategy,

        Agent,

    )

E   ModuleNotFoundError: No module named 'semantic_kernel.experimental'



argumentation_analysis\orchestration\cluedo_orchestrator.py:11: ModuleNotFoundError

___________ TestNumpyRecMock.test_numpy_rec_recarray_instantiation ____________



self = <tests.test_numpy_rec_mock.TestNumpyRecMock testMethod=test_numpy_rec_recarray_instantiation>



    def test_numpy_rec_recarray_instantiation(self):

        """Test que numpy.rec.recarray peut être instancié."""

        import numpy

    

        # Test avec shape et formats requis

        arr1 = numpy.rec.recarray((2, 2), formats=['i4', 'f8'], names=['id', 'value'])

        self.assertIsNotNone(arr1)

>       self.assertEqual(arr1.shape, (2, 2))

E       AssertionError: Tuples differ: (2,) != (2, 2)

E       

E       Second tuple contains 1 additional elements.

E       First extra element 1:

E       2

E       

E       - (2,)

E       + (2, 2)

E       ?    ++



tests\test_numpy_rec_mock.py:46: AssertionError

_____________ TestNumpyRecMock.test_numpy_rec_recarray_properties _____________



self = <tests.test_numpy_rec_mock.TestNumpyRecMock testMethod=test_numpy_rec_recarray_properties>



    def test_numpy_rec_recarray_properties(self):

        """Test que les propriétés de recarray fonctionnent."""

        import numpy

    

        arr = numpy.rec.recarray(3, formats=['i4', 'f8'], names=['id', 'value'])

    

        # Test des propriétés

>       self.assertEqual(arr.names, ['id', 'value'])

E       AssertionError: ('id', 'value') != ['id', 'value']



tests\test_numpy_rec_mock.py:66: AssertionError

__________________ test_load_definitions_encrypted_wrong_key __________________



test_env = {'crypto_service': <argumentation_analysis.services.crypto_service.CryptoService object at 0x000001C5D43D5270>, 'defin...ypte2/test_definitions_dir/extract_definitions.json.enc'), 'key': b'BeMR7CqING-uwSq1t_WbTiBYOFg_uNfzHAxwXPtyy_4=', ...}



    def test_load_definitions_encrypted_wrong_key(test_env):

        wrong_key = test_env['crypto_service'].generate_key() # bytes

    

>       with pytest.raises(InvalidToken):

E       Failed: DID NOT RAISE <class 'cryptography.fernet.InvalidToken'>



tests\ui\test_extract_definition_persistence.py:108: Failed

----------------------------- Captured log setup ------------------------------

WARNING  Services.CryptoService:crypto_service.py:42 Service de chiffrement initialisé sans clé. Le chiffrement est désactivé.

---------------------------- Captured stderr call -----------------------------

04:23:32 [ERROR] argumentation_analysis.utils.core_utils.crypto_utils: Erreur déchiffrement Fernet (InvalidToken/Signature): 

------------------------------ Captured log call ------------------------------

ERROR    argumentation_analysis.utils.core_utils.crypto_utils:crypto_utils.py:159 Erreur déchiffrement Fernet (InvalidToken/Signature): 

WARNING  App.UI.Utils:file_operations.py:78 ⚠️ Échec du déchiffrement pour 'C:\Users\MYIA\AppData\Local\Temp\pytest-of-MYIA\pytest-309\test_load_definitions_encrypte2\test_definitions_dir\extract_definitions.json.enc' (decrypt_data_with_fernet a retourné None). Utilisation des définitions par défaut.

__________________________ test_load_malformed_json ___________________________



test_env = {'crypto_service': <argumentation_analysis.services.crypto_service.CryptoService object at 0x000001C5F35B7D00>, 'defin...json0/test_definitions_dir/extract_definitions.json.enc'), 'key': b'2TRAIUVy62f99KAAn78BHR7kQ5hC-LFOcDdMtvispPM=', ...}



    def test_load_malformed_json(test_env):

        malformed_json_file = test_env['test_dir'] / "malformed.json"

    

        with open(malformed_json_file, 'w') as f:

            f.write("{'sources': [}")

    

        with pytest.raises(json.JSONDecodeError):

             load_extract_definitions(config_file=malformed_json_file, b64_derived_key=None)

    

        malformed_encrypted_file = test_env['test_dir'] / "malformed_encrypted.json.enc"

        with open(malformed_encrypted_file, 'wb') as f:

            f.write(b"this is not encrypted data")

    

>       with pytest.raises(InvalidToken):

E       Failed: DID NOT RAISE <class 'cryptography.fernet.InvalidToken'>



tests\ui\test_extract_definition_persistence.py:169: Failed

----------------------------- Captured log setup ------------------------------

WARNING  Services.CryptoService:crypto_service.py:42 Service de chiffrement initialisé sans clé. Le chiffrement est désactivé.

---------------------------- Captured stderr call -----------------------------

04:23:32 [ERROR] argumentation_analysis.utils.core_utils.crypto_utils: Erreur déchiffrement Fernet (InvalidToken/Signature): 

------------------------------ Captured log call ------------------------------

ERROR    App.UI.Utils:file_operations.py:100 ❌ Erreur décodage JSON pour 'C:\Users\MYIA\AppData\Local\Temp\pytest-of-MYIA\pytest-309\test_load_malformed_json0\test_definitions_dir\malformed.json': Expecting property name enclosed in double quotes: line 1 column 2 (char 1). L'exception sera relancée.

ERROR    argumentation_analysis.utils.core_utils.crypto_utils:crypto_utils.py:159 Erreur déchiffrement Fernet (InvalidToken/Signature): 

WARNING  App.UI.Utils:file_operations.py:78 ⚠️ Échec du déchiffrement pour 'C:\Users\MYIA\AppData\Local\Temp\pytest-of-MYIA\pytest-309\test_load_malformed_json0\test_definitions_dir\malformed_encrypted.json.enc' (decrypt_data_with_fernet a retourné None). Utilisation des définitions par défaut.

_________________ test_calculate_average_scores_nominal_case __________________



sample_grouped_results = {'CorpusA': [{'confidence_score': 0.8, 'id': 'doc1', 'length': 100, 'richness_score': 0.9}, {'confidence_score': 0.7, ...pty': [], 'CorpusD_NoNumeric': [{'id': 'doc5', 'source': 'web', 'text': 'bla'}, {'comment': 'foo', 'id': 'doc6'}], ...}



    def test_calculate_average_scores_nominal_case(sample_grouped_results):

        """Teste le calcul des scores moyens dans un cas nominal."""

        averages = calculate_average_scores(sample_grouped_results)

    

        assert "CorpusA" in averages

        assert "average_confidence_score" in averages["CorpusA"]

        assert "average_richness_score" in averages["CorpusA"]

        assert "average_length" in averages["CorpusA"]

        assert "average_non_numeric" not in averages["CorpusA"] # Doit ignorer les non-numériques

>       assert averages["CorpusA"]["average_confidence_score"] == pytest.approx(0.75)  # (0.8 + 0.7) / 2



tests\unit\argumentation_analysis\analytics\test_stats_calculator.py:46: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.75



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

_____________ test_calculate_average_scores_single_result_corpus ______________



sample_grouped_results = {'CorpusA': [{'confidence_score': 0.8, 'id': 'doc1', 'length': 100, 'richness_score': 0.9}, {'confidence_score': 0.7, ...pty': [], 'CorpusD_NoNumeric': [{'id': 'doc5', 'source': 'web', 'text': 'bla'}, {'comment': 'foo', 'id': 'doc6'}], ...}



    def test_calculate_average_scores_single_result_corpus(sample_grouped_results):

        """Teste le cas d'un corpus avec un seul résultat."""

        averages = calculate_average_scores(sample_grouped_results)

        assert "CorpusE_Single" in averages

        assert "average_confidence_score" in averages["CorpusE_Single"]

>       assert averages["CorpusE_Single"]["average_confidence_score"] == pytest.approx(0.6)



tests\unit\argumentation_analysis\analytics\test_stats_calculator.py:77: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.6



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

______________ test_calculate_average_scores_mixed_numeric_types ______________



    def test_calculate_average_scores_mixed_numeric_types():

        """Teste avec des types numériques mixtes (int, float)."""

        grouped_results = {

            "MixedCorpus": [

                {"score_a": 10, "score_b": 5.5},

                {"score_a": 20, "score_b": 4.5, "score_c": 100.0},

            ]

        }

        averages = calculate_average_scores(grouped_results)

        assert "MixedCorpus" in averages

>       assert averages["MixedCorpus"]["average_score_a"] == pytest.approx(15.0) # (10 + 20) / 2



tests\unit\argumentation_analysis\analytics\test_stats_calculator.py:94: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 15.0



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

_______________ test_calculate_average_scores_results_not_dicts _______________



    def test_calculate_average_scores_results_not_dicts():

        """

        Teste le comportement lorsque les éléments dans la liste de résultats ne sont pas tous des dictionnaires.

        La fonction actuelle devrait ignorer les éléments non-dictionnaires.

        """

        grouped_results = {

            "CorpusWithInvalid": [

                {"id": "doc1", "score": 0.8},

                "not a dict",

                None,

                {"id": "doc2", "score": 0.7},

                123

            ]

        }

        # La fonction actuelle ne lève pas d'erreur mais les ignore.

        # Si une gestion d'erreur stricte était ajoutée, ce test devrait être adapté.

>       averages = calculate_average_scores(grouped_results)



tests\unit\argumentation_analysis\analytics\test_stats_calculator.py:114: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



grouped_results = {'CorpusWithInvalid': [{'id': 'doc1', 'score': 0.8}, 'not a dict', None, {'id': 'doc2', 'score': 0.7}, 123]}



    def calculate_average_scores(grouped_results: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Dict[str, float]]:

        """

        Calcule les scores moyens pour chaque corpus à partir des résultats groupés.

    

        Cette fonction prend un dictionnaire de résultats groupés par corpus et calcule

        les scores moyens pour différentes métriques (par exemple, score de confiance,

        richesse contextuelle, etc.) pour chaque corpus. Les scores non numériques

        sont ignorés.

    

        :param grouped_results: Un dictionnaire où les clés sont les noms des corpus

                                et les valeurs sont des listes de dictionnaires.

                                Chaque dictionnaire dans la liste représente un

                                résultat d'analyse pour un extrait et peut contenir

                                diverses métriques et scores.

                                Exemple:

                                {

                                    "CorpusA": [

                                        {"id": "doc1", "confidence_score": 0.8, "richness_score": 0.9},

                                        {"id": "doc2", "confidence_score": 0.7, "richness_score": 0.85},

                                    ],

                                    "CorpusB": [

                                        {"id": "doc3", "confidence_score": 0.9, "richness_score": 0.95},

                                    ]

                                }

        :type grouped_results: Dict[str, List[Dict[str, Any]]]

        :return: Un dictionnaire où les clés sont les noms des corpus et les valeurs sont

                 des dictionnaires contenant les scores moyens calculés pour ce corpus.

                 Les clés internes du dictionnaire de scores moyens dépendront des métriques

                 numériques présentes dans les données d'entrée et seront préfixées par "average_".

                 Si un corpus n'a aucun résultat ou aucun score numérique, son dictionnaire

                 de moyennes sera vide.

                 Exemple de retour:

                 {

                     "CorpusA": {

                         "average_confidence_score": 0.75,

                         "average_richness_score": 0.875

                     },

                     "CorpusB": {

                         "average_confidence_score": 0.9,

                         "average_richness_score": 0.95

                     }

                 }

        :rtype: Dict[str, Dict[str, float]]

        :raises TypeError: Si `grouped_results` n'est pas un dictionnaire ou si les

                           valeurs ne sont pas des listes de dictionnaires comme attendu.

                           (Note: la gestion explicite des erreurs n'est pas implémentée

                           dans cette version, mais des erreurs de type peuvent survenir

                           avec des entrées malformées.)

        """

        average_scores_by_corpus: Dict[str, Dict[str, float]] = {}

    

        # Itération sur chaque corpus fourni dans les résultats groupés

        for corpus_name, results_list in grouped_results.items():

            if not results_list:

                # Si un corpus n'a pas de résultats, on lui assigne un dictionnaire vide de moyennes

                average_scores_by_corpus[corpus_name] = {}

                continue

    

            score_sums: Dict[str, float] = {} # Pour stocker la somme des scores pour chaque métrique

            score_counts: Dict[str, int] = {} # Pour compter le nombre d'occurrences de chaque métrique

    

            # Itération sur chaque élément de résultat (par exemple, analyse d'un document) dans le corpus

            for result_item in results_list:

>               for key, value in result_item.items():

E               AttributeError: 'str' object has no attribute 'items'



argumentation_analysis\analytics\stats_calculator.py:76: AttributeError

__________ test_mine_arguments_explicit_premise_conclusion_too_short __________



miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x000001C5D537F910>



    def test_mine_arguments_explicit_premise_conclusion_too_short(miner_default: MockArgumentMiner):

        """Teste prémisse/conclusion explicites mais contenu trop court."""

        text = "Prémisse: A. Conclusion: B." # "A." (len 2) et "B." (len 2) < min_length 10

        result = miner_default.mine_arguments(text)

        # Devrait tomber sur Affirmation Simple si le texte global est assez long

        assert len(result) == 1

        assert result[0]["type"] == "Affirmation Simple (Mock)"

    

        text_custom_miner = "Prémisse: Un. Conclusion: Deux." # "Un." (len 3), "Deux." (len 5)

        # Pour miner_custom_config (min_length 5), "Deux." est OK, "Un." ne l'est pas.

        # Donc, l'argument explicite ne devrait pas être formé.

>       result_custom = miner_custom_config.mine_arguments(text_custom_miner)

E       AttributeError: 'FixtureFunctionDefinition' object has no attribute 'mine_arguments'



tests\unit\argumentation_analysis\mocks\test_argument_mining.py:93: AttributeError

_________________ test_mine_arguments_implicit_par_consequent _________________



miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x000001C5D539DF90>



    def test_mine_arguments_implicit_par_consequent(miner_default: MockArgumentMiner):

        """Teste argument implicite avec 'par conséquent'."""

        text = "Les études le montrent clairement. Par conséquent, nous devons agir."

        result = miner_default.mine_arguments(text)

        assert len(result) == 1

        arg = result[0]

        assert arg["type"] == "Argument Implicite (Mock - par conséquent)"

        assert arg["premise"] == "Les études le montrent clairement"

>       assert arg["conclusion"] == "nous devons agir"

E       AssertionError: assert ', nous devons agir' == 'nous devons agir'

E         

E         - nous devons agir

E         + , nous devons agir

E         ? ++



tests\unit\argumentation_analysis\mocks\test_argument_mining.py:124: AssertionError

_____________________ test_mine_arguments_implicit_ainsi ______________________



miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x000001C5D53928F0>



    def test_mine_arguments_implicit_ainsi(miner_default: MockArgumentMiner):

        """Teste argument implicite avec 'ainsi'."""

        text = "Le budget est limité. Ainsi, certains projets seront reportés."

        result = miner_default.mine_arguments(text)

        assert len(result) == 1

        arg = result[0]

        assert arg["type"] == "Argument Implicite (Mock - ainsi)"

        assert arg["premise"] == "Le budget est limité"

>       assert arg["conclusion"] == "certains projets seront reportés"

E       AssertionError: assert ', certains p...ront reportés' == 'certains pro...ront reportés'

E         

E         - certains projets seront reportés

E         + , certains projets seront reportés

E         ? ++



tests\unit\argumentation_analysis\mocks\test_argument_mining.py:134: AssertionError

_________________ test_mine_arguments_explicit_over_implicit __________________



miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x000001C5D436F160>



    def test_mine_arguments_explicit_over_implicit(miner_default: MockArgumentMiner):

        """Teste que l'explicite a priorité et évite doublon avec implicite."""

        text = "Prémisse: C'est un fait. Conclusion: Il faut l'accepter. C'est un fait, donc il faut l'accepter."

        # L'explicite devrait trouver "C'est un fait." et "Il faut l'accepter."

        # L'implicite (avec "donc") trouverait "C'est un fait" et "il faut l'accepter."

        # Le mock devrait éviter ce doublon.

        result = miner_default.mine_arguments(text)

>       assert len(result) == 1 # Un seul argument, l'explicite

E       assert 2 == 1

E        +  where 2 = len([{'conclusion': "Il faut l'accepter.", 'confidence': 0.85, 'details': 'Prémisse et conclusion explicitement marquées.'...epter", 'confidence': 0.7, 'details': "Conclusion inférée par l'indicateur 'donc'.", 'premise': "C'est un fait,", ...}])



tests\unit\argumentation_analysis\mocks\test_argument_mining.py:151: AssertionError

_________________ test_mine_arguments_complex_scenario_mixed __________________



miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x000001C5F36D5D50>



    def test_mine_arguments_complex_scenario_mixed(miner_default: MockArgumentMiner):

        """Teste un scénario mixte avec explicite et implicite."""

        text = (

            "Prémisse: Le ciel est bleu. Conclusion: C'est une belle journée. "

            "Le soleil brille fortement, par conséquent il fait chaud."

        )

        result = miner_default.mine_arguments(text)

        assert len(result) == 2

    

        found_explicit = False

        found_implicit = False

        for arg in result:

            if arg["type"] == "Argument Explicite (Mock)":

                assert arg["premise"] == "Le ciel est bleu."

                assert arg["conclusion"] == "C'est une belle journée."

                found_explicit = True

            elif arg["type"] == "Argument Implicite (Mock - par conséquent)":

>               assert arg["premise"] == "Le soleil brille fortement" # Dernière phrase avant "par conséquent"

E               AssertionError: assert 'Le soleil brille fortement,' == 'Le soleil brille fortement'

E                 

E                 - Le soleil brille fortement

E                 + Le soleil brille fortement,

E                 ?                           +



tests\unit\argumentation_analysis\mocks\test_argument_mining.py:193: AssertionError

____________________ test_conclusion_delimitation_explicit ____________________



miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x000001C5D436E890>



    def test_conclusion_delimitation_explicit(miner_default: MockArgumentMiner):

        """Teste la délimitation de la conclusion pour un argument explicite."""

        text_with_period = "Prémisse: P1. Conclusion: C1 est vraie. Ceci est une autre phrase."

        result = miner_default.mine_arguments(text_with_period)

        assert len(result) == 1

>       assert result[0]["conclusion"] == "C1 est vraie." # Doit s'arrêter au point.

E       KeyError: 'conclusion'



tests\unit\argumentation_analysis\mocks\test_argument_mining.py:205: KeyError

________________________ test_detect_bias_confirmation ________________________



detector_default = <argumentation_analysis.mocks.bias_detection.MockBiasDetector object at 0x000001C5D53974C0>



    def test_detect_bias_confirmation(detector_default: MockBiasDetector):

        text = "Il est évident que cette solution est la meilleure pour tout le monde."

        result = detector_default.detect_biases(text)

        assert len(result) >= 1

        found_bias = any(b["bias_type"] == "Biais de Confirmation (Mock)" and b["detected_pattern"] == r"il est évident que" for b in result)

        assert found_bias

        # Vérifier le contexte (le mock prend 30 chars avant/après)

        bias_entry = next(b for b in result if b["bias_type"] == "Biais de Confirmation (Mock)")

        assert "Il est évident que cette solut" in bias_entry["context_snippet"] # Début du contexte

>       assert "est la meilleure pour tout le" in bias_entry["context_snippet"] # Fin du contexte

E       AssertionError: assert 'est la meilleure pour tout le' in 'Il est évident que cette solution est la meilleu'



tests\unit\argumentation_analysis\mocks\test_bias_detection.py:71: AssertionError

__________________ test_extract_claims_global_claim_success ___________________



miner_default = <argumentation_analysis.mocks.claim_mining.MockClaimMiner object at 0x000001C5D537F910>



    def test_extract_claims_global_claim_success(miner_default: MockClaimMiner):

        """Teste le cas d'une revendication globale (pas de mots-clés, pas de phrases assertives distinctes)."""

        text = "Ceci est une affirmation unique sans rien de spécial." # > 8 chars

        result = miner_default.extract_claims(text)

        assert len(result) == 1

        claim = result[0]

>       assert claim["type"] == "Revendication Globale (Mock)"

E       AssertionError: assert 'Revendicatio...ertive (Mock)' == 'Revendication Globale (Mock)'

E         

E         - Revendication Globale (Mock)

E         ?               ^^^^^^

E         + Revendication Assertive (Mock)

E         ?               ^^^ +++++



tests\unit\argumentation_analysis\mocks\test_claim_mining.py:66: AssertionError

_________________ test_extract_claims_global_claim_too_short __________________



miner_custom_config = <argumentation_analysis.mocks.claim_mining.MockClaimMiner object at 0x000001C5F34E2380>



    def test_extract_claims_global_claim_too_short(miner_custom_config: MockClaimMiner):

        """Teste qu'une revendication globale trop courte n'est pas retournée."""

        # min_claim_length est 5 pour miner_custom_config

        assert miner_custom_config.extract_claims("Test") == [] # len 4 < 5

    

        text_just_long_enough = "Assez" # len 5

        result = miner_custom_config.extract_claims(text_just_long_enough)

        assert len(result) == 1

>       assert result[0]["type"] == "Revendication Globale (Mock)"

E       AssertionError: assert 'Revendicatio...ertive (Mock)' == 'Revendication Globale (Mock)'

E         

E         - Revendication Globale (Mock)

E         ?               ^^^^^^

E         + Revendication Assertive (Mock)

E         ?               ^^^ +++++



tests\unit\argumentation_analysis\mocks\test_claim_mining.py:79: AssertionError

_________ test_extract_claims_by_keyword_text_too_short_after_keyword _________



miner_default = <argumentation_analysis.mocks.claim_mining.MockClaimMiner object at 0x000001C5D5394130>



    def test_extract_claims_by_keyword_text_too_short_after_keyword(miner_default: MockClaimMiner):

        """Teste mot-clé mais texte suivant trop court."""

        text = "Finalement, il faut noter A." # "A." (len 2) < min_claim_length 8

        result = miner_default.extract_claims(text)

        # Devrait tomber sur Revendication Globale si le texte entier est assez long

        assert len(result) == 1

>       assert result[0]["type"] == "Revendication Globale (Mock)"

E       AssertionError: assert 'Revendicatio...ertive (Mock)' == 'Revendication Globale (Mock)'

E         

E         - Revendication Globale (Mock)

E         ?               ^^^^^^

E         + Revendication Assertive (Mock)

E         ?               ^^^ +++++



tests\unit\argumentation_analysis\mocks\test_claim_mining.py:112: AssertionError

______________________ test_score_clarity_long_sentences ______________________



scorer_default = <argumentation_analysis.mocks.clarity_scoring.MockClarityScorer object at 0x000001C5F34E3A30>



    def test_score_clarity_long_sentences(scorer_default: MockClarityScorer):

        """Teste l'impact des phrases longues."""

        # 30 mots / 1 phrase = 30. > 25. Pénalité -0.1

        text = "Ceci est une phrase exceptionnellement et particulièrement longue conçue dans le but unique de tester la fonctionnalité de détection de phrases longues de notre analyseur de clarté."

        result = scorer_default.score_clarity(text)

>       assert result["clarity_score"] == pytest.approx(1.0 - 0.1)



tests\unit\argumentation_analysis\mocks\test_clarity_scoring.py:78: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.9



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

______________________ test_score_clarity_complex_words _______________________



scorer_default = <argumentation_analysis.mocks.clarity_scoring.MockClarityScorer object at 0x000001C5D5393AC0>



    def test_score_clarity_complex_words(scorer_default: MockClarityScorer):

        """Teste l'impact des mots complexes."""

        # "constitutionnellement", "anticonstitutionnellement" (2 mots > 9 lettres)

        # Total 6 mots. Ratio = 2/6 = 0.33. > 0.1. Pénalité -0.15

        text = "Le mot constitutionnellement est long. Anticonstitutionnellement aussi."

        result = scorer_default.score_clarity(text)

>       assert result["clarity_score"] == pytest.approx(1.0 - 0.15)



tests\unit\argumentation_analysis\mocks\test_clarity_scoring.py:88: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.85



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

_________________ test_score_clarity_passive_voice_simulation _________________



scorer_default = <argumentation_analysis.mocks.clarity_scoring.MockClarityScorer object at 0x000001C5CF336A10>



    def test_score_clarity_passive_voice_simulation(scorer_default: MockClarityScorer):

        """Teste l'impact simulé de la voix passive."""

        # "Le chat est chassé. La souris est mangée. Une action fut entreprise." (3 passifs / 3 phrases = 1.0)

        # Ratio 1.0 > 0.2. Pénalité -0.05

        text = "Le chat est chassé par le chien. La souris est mangée par le chat. Une action fut entreprise par le comité."

        result = scorer_default.score_clarity(text)

>       assert result["clarity_score"] == pytest.approx(1.0 - 0.05)



tests\unit\argumentation_analysis\mocks\test_clarity_scoring.py:98: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.95



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

__________________________ test_score_clarity_jargon __________________________



scorer_default = <argumentation_analysis.mocks.clarity_scoring.MockClarityScorer object at 0x000001C5F35C1F00>



    def test_score_clarity_jargon(scorer_default: MockClarityScorer):

        """Teste l'impact du jargon."""

        # "synergie", "paradigm shift" (2 jargons). Pénalité -0.2 * 2 = -0.4

        text = "Nous devons optimiser la synergie pour un paradigm shift efficient."

        result = scorer_default.score_clarity(text)

>       assert result["clarity_score"] == pytest.approx(1.0 - 0.2 * 2)



tests\unit\argumentation_analysis\mocks\test_clarity_scoring.py:107: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.6



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

________________________ test_score_clarity_ambiguity _________________________



scorer_default = <argumentation_analysis.mocks.clarity_scoring.MockClarityScorer object at 0x000001C5D53961A0>



    def test_score_clarity_ambiguity(scorer_default: MockClarityScorer):

        """Teste l'impact des mots ambigus."""

        # "peut-être", "possiblement", "certains" (3 ambigus). Pénalité -0.1 * 3 = -0.3

        text = "Peut-être que cela fonctionnera. Possiblement demain. Certains pensent ainsi."

        result = scorer_default.score_clarity(text)

>       assert result["clarity_score"] == pytest.approx(1.0 - 0.1 * 3)



tests\unit\argumentation_analysis\mocks\test_clarity_scoring.py:116: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.7



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

______________________ test_score_clarity_custom_jargon _______________________



scorer_custom_config = <argumentation_analysis.mocks.clarity_scoring.MockClarityScorer object at 0x000001C5F34DC400>



    def test_score_clarity_custom_jargon(scorer_custom_config: MockClarityScorer):

        """Teste avec une liste de jargon personnalisée et pénalité modifiée."""

        # Jargon: "customjargon" (x1). Pénalité custom = -0.5

        text = "Ce texte utilise notre customjargon spécifique."

>       result = scorer_custom_config.score_clarity(text)



tests\unit\argumentation_analysis\mocks\test_clarity_scoring.py:155: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



self = <argumentation_analysis.mocks.clarity_scoring.MockClarityScorer object at 0x000001C5F34DC400>

text = 'Ce texte utilise notre customjargon spécifique.'



    def score_clarity(self, text: str) -> Dict[str, Any]:

        """

        Simule l'évaluation de la clarté du texte.

    

        Args:

            text: Le texte à évaluer.

    

        Returns:

            Un dictionnaire contenant le score de clarté simulé et les facteurs l'influençant.

        """

        if not isinstance(text, str) or not text.strip():

            logger.warning("MockClarityScorer.score_clarity a reçu une entrée non textuelle ou vide.")

            return {"error": "Entrée non textuelle ou vide", "clarity_score": 0.0, "factors": {}}

    

        logger.info("MockClarityScorer évalue le texte : %s...", text[:100])

    

        clarity_score: float = 1.0 # Score de base parfait

        factors: Dict[str, Any] = {penalty: 0 for penalty in self.clarity_penalties}

        text_lower = text.lower()

    

        words = re.findall(r'\b\w+\b', text_lower)

        num_words = len(words)

        if num_words == 0:

            return {"clarity_score": 0.0, "factors": factors, "interpretation": self._interpret_score(0.0)}

    

        sentences = [s.strip() for s in text.split('.') if s.strip()] # Simpliste

        num_sentences = len(sentences) if len(sentences) > 0 else 1

    

        # Longueur moyenne des phrases

        avg_sentence_length = num_words / num_sentences

        if avg_sentence_length > self.max_avg_sentence_length:

            clarity_score += self.clarity_penalties["long_sentences_avg"]

            factors["long_sentences_avg"] = round(avg_sentence_length,1)

    

        # Mots complexes (simplifié: mots > 9 lettres comme proxy pour >3 syllabes)

        complex_words = [w for w in words if len(w) > 9]

        complex_word_ratio = len(complex_words) / num_words

        if complex_word_ratio > self.max_complex_word_ratio:

>           clarity_score += self.clarity_penalties["complex_words_ratio"]

E           KeyError: 'complex_words_ratio'



argumentation_analysis\mocks\clarity_scoring.py:76: KeyError

______________________ test_analyze_coherence_ideal_text ______________________



analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x000001C5D537F8E0>



    def test_analyze_coherence_ideal_text(analyzer_default: MockCoherenceAnalyzer):

        """Teste un texte idéalement cohérent."""

        # Beaucoup de mots de transition, répétition de mots-clés, pas de contradictions.

        # Score de base 0.5

        # Transition words: "donc", "ainsi", "de plus" (3) / ~20 mots. Ratio > 0.02. Score += 0.2

        # Repeated keywords: "cohérence" (x2), "texte" (x2). Count = 2. Score += 0.15

        # Pronoun referencing (texte > 50 mots): Score += 0.1

        # Total attendu = 0.5 + 0.2 + 0.15 + 0.1 = 0.95

        text = (

            "Ce texte est un exemple de cohérence. Donc, il suit une logique claire. "

            "De plus, les idées sont bien liées. Ainsi, la cohérence du texte est assurée. "

            "La structure du texte aide aussi."

        ) # 30 mots

        result = analyzer_default.analyze_coherence(text)

>       assert result["coherence_score"] == pytest.approx(0.5 + 0.2 + 0.15 + 0.1) # 0.95



tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:72: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.95



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

___________________ test_analyze_coherence_transition_words ___________________



analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x000001C5D458A8F0>



    def test_analyze_coherence_transition_words(analyzer_default: MockCoherenceAnalyzer):

        """Teste l'impact des mots de transition."""

        # Score de base 0.5. Pronoms +0.1 (car > 50 mots). Total 0.6

        # 5 mots de transition / ~30 mots. Ratio > 0.02. Score += 0.2. Total = 0.8

        text_good = "Le premier point est important. Donc, nous devons le considérer. De plus, il y a un autre aspect. Par conséquent, la conclusion est évidente. Finalement, c'est clair."

        result_good = analyzer_default.analyze_coherence(text_good)

        assert result_good["factors"]["transition_words_ratio"] > 0

        assert result_good["coherence_score"] > 0.5 + 0.1 # Plus que base + pronoms

    

        text_bad = "Point un. Point deux. Point trois. Point quatre. Point cinq. Point six. Point sept." # Peu de transitions

        result_bad = analyzer_default.analyze_coherence(text_bad)

        assert result_bad["factors"]["transition_words_ratio"] == 0

        # Score = 0.5 (base) + 0.1 (pronoms, car > 50 mots) = 0.6

>       assert result_bad["coherence_score"] == pytest.approx(0.5 + 0.1)



tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:91: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.6



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

__________________ test_analyze_coherence_repeated_keywords ___________________



analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x000001C5CF334A30>



    def test_analyze_coherence_repeated_keywords(analyzer_default: MockCoherenceAnalyzer):

        """Teste l'impact de la répétition de mots-clés."""

        # Score de base 0.5. Pronoms +0.1. Total 0.6

        # "analyse" (x3), "pertinente" (x2). Count = 2. Score += 0.15. Total = 0.75

        text = "Cette analyse est une analyse très pertinente. L'analyse des données est pertinente."

        result = analyzer_default.analyze_coherence(text)

        assert result["factors"]["repeated_keywords_bonus"] >= 2

>       assert result["coherence_score"] == pytest.approx(0.5 + 0.15 + 0.1)



tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:101: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.75



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

____________________ test_analyze_coherence_contradictions ____________________



analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x000001C5D539CB80>



    def test_analyze_coherence_contradictions(analyzer_default: MockCoherenceAnalyzer):

        """Teste l'impact des contradictions."""

        # Score de base 0.5. Pronoms +0.1. Total 0.6

        # "j'aime" et "je n'aime pas". Contradiction x1. Score -= 0.4. Total = 0.2

        text = "J'aime le chocolat. Mais parfois, je n'aime pas le chocolat du tout."

        result = analyzer_default.analyze_coherence(text)

        assert result["factors"]["contradiction_penalty"] == 1

>       assert result["coherence_score"] == pytest.approx(0.5 - 0.4 + 0.1) # 0.2



tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:111: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.19999999999999998



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

_________________ test_analyze_coherence_abrupt_topic_change __________________



analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x000001C5D537C9D0>



    def test_analyze_coherence_abrupt_topic_change(analyzer_default: MockCoherenceAnalyzer):

        """Teste l'impact d'un changement de sujet abrupt simulé."""

        # Score de base 0.5. Pronoms +0.1. Total 0.6

        # Changement abrupt. Score -= 0.2. Total = 0.4

        text = "Les pommes sont rouges et délicieuses. Les voitures vont vite."

        prev_summary = "Discussion sur les fruits et leurs couleurs."

        result = analyzer_default.analyze_coherence(text, prev_summary)

>       assert result["factors"]["abrupt_topic_change_penalty"] == 1

E       assert 0 == 1



tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:121: AssertionError

____________ test_analyze_coherence_multiple_factors_and_clamping _____________



analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x000001C5D436FCD0>



    def test_analyze_coherence_multiple_factors_and_clamping(analyzer_default: MockCoherenceAnalyzer):

        """Teste le cumul de facteurs et le clampage."""

        # Base 0.5. Pronoms +0.1.

        # Transitions: "donc", "cependant" (x2) / ~20 mots. Ratio > 0.02. Score += 0.2

        # Répétition: "test" (x2). Score += 0.15

        # Contradiction: "j'aime", "je n'aime pas". Score -= 0.4

        # Total = 0.5 + 0.1 + 0.2 + 0.15 - 0.4 = 0.55

        text = "J'aime ce test. Donc, c'est un bon test. Cependant, je n'aime pas toujours ce test."

        result = analyzer_default.analyze_coherence(text)

>       assert result["coherence_score"] == pytest.approx(0.55)



tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:140: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.55



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

____________________ test_analyze_tone_single_emotion_joy _____________________



analyzer_default = <argumentation_analysis.mocks.emotional_tone_analysis.MockEmotionalToneAnalyzer object at 0x000001C5D4588A90>



    def test_analyze_tone_single_emotion_joy(analyzer_default: MockEmotionalToneAnalyzer):

        """Teste la détection de la joie."""

        text = "Je suis tellement heureux et content aujourd'hui, c'est une journée joyeuse !"

        # "heureux": 0.3+0.1=0.4

        # "content": 0.3+0.1=0.4 (cumulatif sur le count, puis min(1.0, 0.3*count+0.1))

        # "joyeux": 0.3+0.1=0.4

        # count = 3. score = min(1.0, 0.3*3 + 0.1) = min(1.0, 1.0) = 1.0

        result = analyzer_default.analyze_tone(text)

>       assert result["emotions_scores"]["Joie (Mock)"] == 1.0

E       assert 0.7 == 1.0



tests\unit\argumentation_analysis\mocks\test_emotional_tone_analysis.py:74: AssertionError

___________ test_analyze_tone_single_emotion_sadness_strong_keyword ___________



analyzer_default = <argumentation_analysis.mocks.emotional_tone_analysis.MockEmotionalToneAnalyzer object at 0x000001C5F36F1720>



    def test_analyze_tone_single_emotion_sadness_strong_keyword(analyzer_default: MockEmotionalToneAnalyzer):

        """Teste la détection de la tristesse avec un mot-clé fort."""

        text = "Il se sentait complètement déprimé et abattu."

        # "déprimé": 0.3*1+0.1 = 0.4. Mot fort -> +0.2. Total = 0.6

        # "abattu": 0.3*1+0.1 = 0.4. (le score est par émotion, pas par mot-clé individuel avant cumul)

        # count = 2. score_base = min(1.0, 0.3*2+0.1) = 0.7. Mot fort "déprimé" présent -> +0.2. Total = 0.9

        result = analyzer_default.analyze_tone(text)

>       assert result["emotions_scores"]["Tristesse (Mock)"] == 0.9

E       assert 0.8999999999999999 == 0.9



tests\unit\argumentation_analysis\mocks\test_emotional_tone_analysis.py:91: AssertionError

________ test_analyze_tone_mixed_emotions_one_dominant_above_threshold ________



analyzer_default = <argumentation_analysis.mocks.emotional_tone_analysis.MockEmotionalToneAnalyzer object at 0x000001C5D4588640>



    def test_analyze_tone_mixed_emotions_one_dominant_above_threshold(analyzer_default: MockEmotionalToneAnalyzer):

        """Teste émotions mixtes avec une clairement dominante."""

        text = "Je suis très très heureux, ravi même ! Un peu triste aussi, mais surtout joyeux."

        # Joie: "heureux" (x2), "ravi", "joyeux". count = 4.

        # score_base = min(1.0, 0.3*4+0.1) = 1.0. "ravi" est fort -> +0.2. Total = 1.0 (capé)

        # Tristesse: "triste". count = 1. score = 0.3*1+0.1 = 0.4.

        result = analyzer_default.analyze_tone(text)

        assert result["emotions_scores"]["Joie (Mock)"] == 1.0

>       assert result["emotions_scores"]["Tristesse (Mock)"] == 0.4

E       assert 0.6000000000000001 == 0.4



tests\unit\argumentation_analysis\mocks\test_emotional_tone_analysis.py:118: AssertionError

____________________ test_analyze_engagement_appel_action _____________________



analyzer_default = <argumentation_analysis.mocks.engagement_analysis.MockEngagementAnalyzer object at 0x000001C5F36F1840>



    def test_analyze_engagement_appel_action(analyzer_default: MockEngagementAnalyzer):

        text = "Cliquez ici pour en savoir plus et rejoignez notre communauté !"

        # "cliquez", "rejoignez"

        # score += 0.3

        result = analyzer_default.analyze_engagement(text)

        assert result["signals_detected"]["appels_action"] == 2

        assert result["engagement_score"] >= 0.3

>       assert result["interpretation"] == "Engageant (Mock)" # 0.3

E       AssertionError: assert 'Peu engageant (Mock)' == 'Engageant (Mock)'

E         

E         - Engageant (Mock)

E         ? ^

E         + Peu engageant (Mock)

E         ? ^^^^^



tests\unit\argumentation_analysis\mocks\test_engagement_analysis.py:93: AssertionError

__________________ test_analyze_engagement_pronoms_inclusifs __________________



analyzer_default = <argumentation_analysis.mocks.engagement_analysis.MockEngagementAnalyzer object at 0x000001C5D458BB50>



    def test_analyze_engagement_pronoms_inclusifs(analyzer_default: MockEngagementAnalyzer):

        text = "Ensemble, nous pouvons faire la différence pour notre futur. Votre aide est précieuse."

        # "ensemble", "nous", "notre", "votre"

        # count = 4. score += 0.1 * min(4, 5) = 0.4

        result = analyzer_default.analyze_engagement(text)

>       assert result["signals_detected"]["pronoms_inclusifs"] >= 4

E       assert 0 >= 4



tests\unit\argumentation_analysis\mocks\test_engagement_analysis.py:100: AssertionError

______________ test_analyze_engagement_combination_and_clamping _______________



analyzer_default = <argumentation_analysis.mocks.engagement_analysis.MockEngagementAnalyzer object at 0x000001C5D436C580>



    def test_analyze_engagement_combination_and_clamping(analyzer_default: MockEngagementAnalyzer):

        text = (

            "Qu'en pensez-vous ? Cliquez ici ! Ensemble, nous ferons des choses incroyables. "

            "C'est une opportunité révolutionnaire. Rejoignez-nous maintenant. Votre avis compte. "

            "N'est-ce pas merveilleux ? " + ("bla " * 30) # Pour bonus longueur

        )

        # Questions: "Qu'en pensez-vous ?", "N'est-ce pas ?" -> count=2. score_q = 0.2 * 2 = 0.4

        # Action: "Cliquez", "Rejoignez" -> count=2. score_a = 0.3

        # Inclusifs: "Ensemble", "nous", "Votre" -> count=3. score_i = 0.1 * 3 = 0.3

        # Positif: "incroyables", "révolutionnaire", "merveilleux" -> count=3. score_p = 0.15

        # Longueur: > 200 -> score_l = 0.05

        # Total = 0.4 + 0.3 + 0.3 + 0.15 + 0.05 = 1.2. Devrait être clampé à 1.0

        result = analyzer_default.analyze_engagement(text)

        assert result["engagement_score"] == 1.0

        assert result["interpretation"] == "Très engageant (Mock)"

        assert result["signals_detected"]["questions_directes"] >= 2

        assert result["signals_detected"]["appels_action"] >= 2

>       assert result["signals_detected"]["pronoms_inclusifs"] >= 3

E       assert 0 >= 3



tests\unit\argumentation_analysis\mocks\test_engagement_analysis.py:147: AssertionError

________ test_detect_evidence_by_keyword_text_too_short_after_keyword _________



detector_default = <argumentation_analysis.mocks.evidence_detection.MockEvidenceDetector object at 0x000001C5F35C3130>



    def test_detect_evidence_by_keyword_text_too_short_after_keyword(detector_default: MockEvidenceDetector):

        """Teste mot-clé mais texte suivant trop court."""

        text = "Il a été prouvé que A." # "A." (len 2) < min_evidence_length 15

        result = detector_default.detect_evidence(text)

        assert result == [] # Ne devrait rien trouver car trop court, et pas de fallback global dans ce mock

    

        # Avec config custom (min_length 10)

        text_custom = "Preuve: B." # "B." (len 2) < min_evidence_length 10

>       result_custom = detector_custom_config.detect_evidence(text_custom)

E       AttributeError: 'FixtureFunctionDefinition' object has no attribute 'detect_evidence'



tests\unit\argumentation_analysis\mocks\test_evidence_detection.py:92: AttributeError

_____________________ test_detect_evidence_priority_order _____________________



detector_default = <argumentation_analysis.mocks.evidence_detection.MockEvidenceDetector object at 0x000001C5D5390AF0>



    def test_detect_evidence_priority_order(detector_default: MockEvidenceDetector):

        """Teste l'ordre de priorité: Mot-clé > Factuel > Citation."""

        # 1. Mot-clé a priorité

        text_keyword_and_factual = "Selon l'étude, 50% des cas sont résolus. Il y a aussi 20 chiffres."

        result1 = detector_default.detect_evidence(text_keyword_and_factual)

        assert len(result1) == 1

        assert result1[0]["type"] == "Preuve par Mot-Clé (Mock)"

>       assert result1[0]["evidence_text"] == "50% des cas sont résolus." # Texte après mot-clé

E       AssertionError: assert ', 50% des cas sont résolus.' == '50% des cas sont résolus.'

E         

E         - 50% des cas sont résolus.

E         + , 50% des cas sont résolus.

E         ? ++



tests\unit\argumentation_analysis\mocks\test_evidence_detection.py:164: AssertionError

___________________ test_detect_evidence_multiple_keywords ____________________



detector_default = <argumentation_analysis.mocks.evidence_detection.MockEvidenceDetector object at 0x000001C5F36E50F0>



    def test_detect_evidence_multiple_keywords(detector_default: MockEvidenceDetector):

        """Teste la détection de plusieurs mots-clés."""

        text = (

            "Selon l'étude, les faits sont têtus. "

            "Par exemple, les chiffres ne mentent pas."

        )

        result = detector_default.detect_evidence(text)

        assert len(result) == 2

        keywords_found = {ev["keyword_used"] for ev in result}

        assert "selon l'étude" in keywords_found

        assert "par exemple" in keywords_found

    

        evidence_texts = {ev["evidence_text"] for ev in result}

>       assert "les faits sont têtus." in evidence_texts

E       AssertionError: assert 'les faits sont têtus.' in {', les chiffres ne mentent pas.', ', les faits sont têtus.'}



tests\unit\argumentation_analysis\mocks\test_evidence_detection.py:193: AssertionError

__________________________ test_analyze_ironic_tone ___________________________



analyzer = <argumentation_analysis.mocks.rhetorical_analysis.MockRhetoricalAnalyzer object at 0x000001C5D5392740>



    def test_analyze_ironic_tone(analyzer: MockRhetoricalAnalyzer):

        """Teste la détection de 'tonalité ironique'."""

        text = "Oh, quelle merveilleuse journée pluvieuse, c'est une tonalité ironique."

        result = analyzer.analyze(text)

    

        # La tonalité ironique peut ajouter une figure de style par défaut si aucune autre n'est trouvée.

>       assert len(result["figures_de_style"]) >= 1

E       assert 0 >= 1

E        +  where 0 = len([])



tests\unit\argumentation_analysis\mocks\test_rhetorical_analysis.py:90: AssertionError

____________________ test_analyze_combination_all_keywords ____________________



analyzer = <argumentation_analysis.mocks.rhetorical_analysis.MockRhetoricalAnalyzer object at 0x000001C5D436FB50>



    def test_analyze_combination_all_keywords(analyzer: MockRhetoricalAnalyzer):

        """Teste la combinaison de tous les mots-clés."""

        text = "Un exemple de métaphore, une question rhétorique, et une tonalité ironique."

        result = analyzer.analyze(text)

    

        assert len(result["figures_de_style"]) == 2 # Métaphore et Question

        types = {f["type"] for f in result["figures_de_style"]}

        assert "Métaphore (Mock)" in types

        assert "Question Rhétorique (Mock)" in types

    

        # La tonalité ironique est la dernière vérifiée et devrait s'appliquer

        assert result["tonalite_globale"] == "Ironique (Mock)"

        # Métaphore: +0.3, Question: +0.4, Ironie: -0.2. Total = 0.5

>       assert result["score_engagement_simule"] == 0.5

E       assert 0.49999999999999994 == 0.5



tests\unit\argumentation_analysis\mocks\test_rhetorical_analysis.py:126: AssertionError

____________________ test_get_embeddings_openai_api_error _____________________



mock_openai_constructor = <MagicMock name='OpenAI' id='1949704209072'>

sample_text_chunks = ['Ceci est un texte.', 'Un autre texte ici.']



    @patch(OPENAI_CLIENT_PATH) # Mock pour éviter l'erreur d'import si OpenAI n'est pas là

    def test_get_embeddings_openai_api_error(mock_openai_constructor, sample_text_chunks):

        """Teste la gestion d'une APIError d'OpenAI."""

        # Simuler que openai.APIError est défini (même si on ne l'importe pas directement ici)

        # Cela est nécessaire car la fonction testée y fait référence.

        # On peut le faire en patchant la référence dans le module testé si besoin,

        # ou en s'assurant que le mock d'OpenAI est suffisant.

        # Pour ce test, on simule que client.embeddings.create lève l'erreur.

    

        # Mock de l'APIError d'OpenAI

        # Normalement, on importerait APIError d'OpenAI, mais pour le test unitaire,

        # on peut le simuler si on ne veut pas dépendre de l'installation d'OpenAI pour les tests.

        # Cependant, la fonction testée fait `from openai import OpenAI, APIError`.

        # Si OpenAI est None, APIError sera aussi None.

        # Si OpenAI est mocké, il faut s'assurer que APIError est aussi gérable.

        # Le plus simple est de supposer qu'OpenAI est importable et de mocker son comportement.

    

        mock_client_instance = MagicMock()

        # Simuler que APIError est une classe d'exception

        # Dans un environnement de test, si openai n'est pas installé, APIError sera None.

        # La fonction get_embeddings_for_chunks a un `try...except APIError`.

        # Si APIError est None, ce `except` ne fonctionnera pas comme prévu.

        # Pour ce test, nous allons supposer que l'import d'OpenAI a réussi.

    

        # On importe APIError pour le test si disponible, sinon on crée une classe factice

        try:

            from openai import APIError as ActualAPIError

        except ImportError:

            class ActualAPIError(Exception): pass # Classe factice si openai n'est pas là

    

>       mock_client_instance.embeddings.create.side_effect = ActualAPIError("Test API Error")

E       TypeError: APIError.__init__() missing 1 required positional argument: 'request'



tests\unit\argumentation_analysis\nlp\test_embedding_utils.py:119: TypeError

____________________ test_save_embeddings_data_mkdir_fails ____________________



tmp_path = WindowsPath('C:/Users/MYIA/AppData/Local/Temp/pytest-of-MYIA/pytest-309/test_save_embeddings_data_mkdi0')

sample_embeddings_data = {'embeddings': [[0.1], [0.2]], 'model': 'test-model', 'texts': ['a', 'b']}

caplog = <_pytest.logging.LogCaptureFixture object at 0x000001C5F366A170>



    def test_save_embeddings_data_mkdir_fails(tmp_path, sample_embeddings_data, caplog):

        """Teste le cas où la création du répertoire parent échoue."""

        output_file = tmp_path / "dir_fail" / "embeddings.json"

    

        with patch(MKDIR_PATH, side_effect=OSError("Cannot create directory")) as mock_mkdir, \

             patch(OPEN_BUILTIN_PATH, mock_open()): # open ne sera pas appelé si mkdir échoue avant

    

            success = save_embeddings_data(sample_embeddings_data, output_file)

    

            assert success is False

            mock_mkdir.assert_called_once()

            # L'erreur est capturée par le `except Exception` générique dans save_embeddings_data

>           assert "Erreur inattendue lors de la sauvegarde" in caplog.text

E           assert 'Erreur inattendue lors de la sauvegarde' in 'ERROR    argumentation_analysis.nlp.embedding_utils:embedding_utils.py:181 ❌ Erreur d\'E/S lors de la sauvegarde des ...et-is\\lib\\unittest\\mock.py", line 1173, in _execute_mock_call\n    raise effect\nOSError: Cannot create directory\n'

E            +  where 'ERROR    argumentation_analysis.nlp.embedding_utils:embedding_utils.py:181 ❌ Erreur d\'E/S lors de la sauvegarde des ...et-is\\lib\\unittest\\mock.py", line 1173, in _execute_mock_call\n    raise effect\nOSError: Cannot create directory\n' = <_pytest.logging.LogCaptureFixture object at 0x000001C5F366A170>.text



tests\unit\argumentation_analysis\nlp\test_embedding_utils.py:204: AssertionError

------------------------------ Captured log call ------------------------------

ERROR    argumentation_analysis.nlp.embedding_utils:embedding_utils.py:181 ❌ Erreur d'E/S lors de la sauvegarde des embeddings dans C:\Users\MYIA\AppData\Local\Temp\pytest-of-MYIA\pytest-309\test_save_embeddings_data_mkdi0\dir_fail\embeddings.json: Cannot create directory

Traceback (most recent call last):

  File "D:\2025-Epita-Intelligence-Symbolique-4\argumentation_analysis\nlp\embedding_utils.py", line 169, in save_embeddings_data

    output_path.parent.mkdir(parents=True, exist_ok=True)

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\mock.py", line 1114, in __call__

    return self._mock_call(*args, **kwargs)

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\mock.py", line 1118, in _mock_call

    return self._execute_mock_call(*args, **kwargs)

  File "C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\mock.py", line 1173, in _execute_mock_call

    raise effect

OSError: Cannot create directory

________________ test_analyze_extract_advanced_successful_run _________________



sample_extract_definition = {'context': {'domain': 'general_test'}, 'extract_name': 'Test Extrait 1', 'extract_text': 'Ceci est le premier argument. Et voici un second argument.'}

mock_tools = {'complex_fallacy_analyzer': <argumentation_analysis.mocks.advanced_tools.MockEnhancedComplexFallacyAnalyzer object at...yzer': <argumentation_analysis.mocks.advanced_tools.MockEnhancedRhetoricalResultAnalyzer object at 0x000001C5F368B8B0>}

sample_base_result = {'analyses': {'argument_coherence': {'score': 0.7}, 'contextual_fallacies': {'base_found': True, 'count': 1}, 'semantic_analysis': {'sentiment': 'neutral'}}, 'extract_name': 'Test Extrait 1', 'source_name': 'TestSource'}



    def test_analyze_extract_advanced_successful_run(

        sample_extract_definition: Dict[str, Any],

        mock_tools: Dict[str, Any],

        sample_base_result: Optional[Dict[str, Any]]

    ):

        """Teste un déroulement réussi de l'analyse avancée."""

        source_name = "TestSource"

    

        results = analyze_extract_advanced(

            sample_extract_definition,

            source_name,

            sample_base_result,

            mock_tools

        )

    

        assert results["extract_name"] == sample_extract_definition["extract_name"]

        assert results["source_name"] == source_name

        assert results["argument_count"] > 0 # split_text_into_arguments devrait trouver 2 args

        assert "timestamp" in results

    

        analyses = results["analyses"]

        assert "complex_fallacies" in analyses

        assert "contextual_fallacies" in analyses

        assert "fallacy_severity" in analyses

        assert "rhetorical_results" in analyses

        assert "comparison_with_base" in results # Car sample_base_result est fourni

    

        # Vérifier que les mocks ont produit quelque chose (pas d'erreur)

        assert "error" not in analyses["complex_fallacies"]

        assert "error" not in analyses["contextual_fallacies"]

        assert "error" not in analyses["fallacy_severity"]

        assert "error" not in analyses["rhetorical_results"]

>       assert "error" not in results["comparison_with_base"]

E       assert 'error' not in {'error': "'float' object has no attribute 'get'"}



tests\unit\argumentation_analysis\orchestration\test_advanced_analyzer.py:81: AssertionError

------------------------------ Captured log call ------------------------------

ERROR    argumentation_analysis.orchestration.advanced_analyzer:advanced_analyzer.py:135 Erreur lors de la comparaison avec l'analyse de base (orchestrateur): 'float' object has no attribute 'get'

Traceback (most recent call last):

  File "D:\2025-Epita-Intelligence-Symbolique-4\argumentation_analysis\orchestration\advanced_analyzer.py", line 132, in analyze_extract_advanced

    comparison = compare_rhetorical_analyses(results, base_result) # Nécessite compare_rhetorical_analyses

  File "D:\2025-Epita-Intelligence-Symbolique-4\argumentation_analysis\utils\analysis_comparison.py", line 136, in compare_rhetorical_analyses

    adv_coherence_score = adv_coherence_score_data.get("score", adv_coherence.get("score", 0.0)) # Fallback

AttributeError: 'float' object has no attribute 'get'

_________________ test_run_advanced_rhetoric_pipeline_success _________________



self = <MagicMock name='tqdm' id='1949703765584'>, args = ()

kwargs = {'desc': "Pipeline d'analyse avancée", 'total': 3, 'unit': 'extrait'}

msg = "Expected 'tqdm' to be called once. Called 0 times."



    def assert_called_once_with(self, /, *args, **kwargs):

        """assert that the mock was called exactly once and that that call was

        with the specified arguments."""

        if not self.call_count == 1:

            msg = ("Expected '%s' to be called once. Called %s times.%s"

                   % (self._mock_name or 'mock',

                      self.call_count,

                      self._calls_repr()))

>           raise AssertionError(msg)

E           AssertionError: Expected 'tqdm' to be called once. Called 0 times.



C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\mock.py:940: AssertionError



During handling of the above exception, another exception occurred:



mock_tqdm = <MagicMock name='tqdm' id='1949703765584'>

mock_json_dump = <MagicMock name='dump' id='1949703754208'>

mock_open = <MagicMock name='open' id='1949703821280'>

mock_create_mocks = <MagicMock name='create_mock_advanced_rhetorical_tools' id='1949703975552'>

mock_analyze_single_extract = <MagicMock name='analyze_extract_advanced' id='1949703972240'>

sample_extract_definitions = [{'extracts': [{'extract_name': 'Ext1.1', 'extract_text': "Texte de l'extrait 1.1"}, {'extract_name': 'Ext1.2', 'extra...rce1'}, {'extracts': [{'extract_name': 'Ext2.1', 'extract_text': "Texte de l'extrait 2.1"}], 'source_name': 'Source2'}]

sample_base_results = [{'analyses': {'coherence': 0.8}, 'extract_name': 'Ext1.1', 'source_name': 'Source1'}, {'analyses': {'coherence': 0.7}...t1.2', 'source_name': 'Source1'}, {'analyses': {'coherence': 0.9}, 'extract_name': 'Ext2.1', 'source_name': 'Source2'}]

temp_output_file = WindowsPath('C:/Users/MYIA/AppData/Local/Temp/pytest-of-MYIA/pytest-309/test_run_advanced_rhetoric_pip0/advanced_results.json')



    @patch("argumentation_analysis.pipelines.advanced_rhetoric.analyze_extract_advanced")

    @patch("argumentation_analysis.pipelines.advanced_rhetoric.create_mock_advanced_rhetorical_tools")

    @patch("builtins.open") # Pour mocker l'écriture du fichier

    @patch("json.dump")

    @patch("tqdm.tqdm") # Pour mocker la barre de progression

    def test_run_advanced_rhetoric_pipeline_success(

        mock_tqdm: MagicMock,

        mock_json_dump: MagicMock,

        mock_open: MagicMock,

        mock_create_mocks: MagicMock,

        mock_analyze_single_extract: MagicMock,

        sample_extract_definitions: List[Dict[str, Any]],

        sample_base_results: List[Dict[str, Any]],

        temp_output_file: Path

    ):

        """Teste une exécution réussie du pipeline."""

    

        # Configurer les mocks

        mock_progress_bar_instance = MagicMock()

        mock_tqdm.return_value = mock_progress_bar_instance

    

        mock_tools_dict = {"mock_tool": "un outil"}

        mock_create_mocks.return_value = mock_tools_dict

    

        # Simuler les résultats de l'analyse d'un seul extrait

        def analyze_single_side_effect(extract_def, source_name, base_res, tools):

            return {"analyzed": True, "extract_name": extract_def["extract_name"], "source_name": source_name}

        mock_analyze_single_extract.side_effect = analyze_single_side_effect

    

        run_advanced_rhetoric_pipeline(sample_extract_definitions, sample_base_results, temp_output_file)

    

        # Vérifications

        mock_create_mocks.assert_called_once() # Doit utiliser les mocks par défaut

    

        assert mock_analyze_single_extract.call_count == 3 # 2 extraits pour Source1, 1 pour Source2

    

        # Vérifier les appels à analyze_extract_advanced avec les bons arguments

        calls = [

            call(sample_extract_definitions[0]["extracts"][0], "Source1", sample_base_results[0], mock_tools_dict),

            call(sample_extract_definitions[0]["extracts"][1], "Source1", sample_base_results[1], mock_tools_dict),

            call(sample_extract_definitions[1]["extracts"][0], "Source2", sample_base_results[2], mock_tools_dict),

        ]

        mock_analyze_single_extract.assert_has_calls(calls, any_order=False) # L'ordre est important ici

    

>       mock_tqdm.assert_called_once_with(total=3, desc="Pipeline d'analyse avancée", unit="extrait")

E       AssertionError: Expected 'tqdm' to be called once. Called 0 times.



tests\unit\argumentation_analysis\pipelines\test_advanced_rhetoric.py:90: AssertionError

---------------------------- Captured stderr call -----------------------------


Pipeline d'analyse avancée:   0%|          | 0/3 [00:00<?, ?extrait/s]
Pipeline d'analyse avancée: 100%|██████████| 3/3 [00:00<?, ?extrait/s]

_________ test_check_java_environment_java_version_fails_filenotfound _________



mock_os_environ_java = environ({'PYTEST_CURRENT_TEST': 'tests/unit/argumentation_analysis/utils/dev_tools/test_env_checks.py::test_check_java_environment_java_version_fails_filenotfound (call)', 'JAVA_HOME': '/opt/java'})

mock_path_java = <MagicMock name='MockPathInstance_1' spec='WindowsPath' id='1949179697024'>

mock_run_command_java = <MagicMock name='_run_command' id='1949180005632'>

caplog = <_pytest.logging.LogCaptureFixture object at 0x000001C5D52D8730>



    def test_check_java_environment_java_version_fails_filenotfound(mock_os_environ_java, mock_path_java, mock_run_command_java, caplog):

        mock_os_environ_java["JAVA_HOME"] = "/opt/java"

        mock_path_java.configure_java_exe_path(is_dir_java_home=True, java_exe_exists_in_bin=True, java_exe_is_file_in_bin=True)

        mock_run_command_java.return_value = (-1, "", "FileNotFoundError: java") # Simulating _run_command's FileNotFoundError case

    

        assert check_java_environment() is False

        # assert "Échec de l'exécution de 'java -version'." in caplog.text # This comes from _run_command via logger

>       assert "Java n'est pas trouvé dans le PATH ou n'est pas exécutable." in caplog.text # This is from check_java_environment

E       assert "Java n'est pas trouvé dans le PATH ou n'est pas exécutable." in "INFO     argumentation_analysis.utils.dev_tools.env_checks:env_checks.py:79 ☕ Vérification de l'environnement Java......tils.dev_tools.env_checks:env_checks.py:166 ❌ L'environnement Java n'est pas considéré comme correctement configuré.\n"

E        +  where "INFO     argumentation_analysis.utils.dev_tools.env_checks:env_checks.py:79 ☕ Vérification de l'environnement Java......tils.dev_tools.env_checks:env_checks.py:166 ❌ L'environnement Java n'est pas considéré comme correctement configuré.\n" = <_pytest.logging.LogCaptureFixture object at 0x000001C5D52D8730>.text



tests\unit\argumentation_analysis\utils\dev_tools\test_env_checks.py:187: AssertionError

---------------------------- Captured stderr call -----------------------------

04:23:35 [INFO] [argumentation_analysis.utils.dev_tools.env_checks] ☕ Vérification de l'environnement Java...

04:23:35 [INFO] [argumentation_analysis.utils.dev_tools.env_checks]     JAVA_HOME est défini : /opt/java

04:23:35 [INFO] [argumentation_analysis.utils.dev_tools.env_checks]     JAVA_HOME pointe vers un répertoire Java valide (/mocked/java_home/bin/java).

04:23:35 [ERROR] [argumentation_analysis.utils.dev_tools.env_checks]     Échec de l'exécution de 'java -version'. Code de retour : -1

04:23:35 [ERROR] [argumentation_analysis.utils.dev_tools.env_checks]     Stderr: FileNotFoundError: java

04:23:35 [WARNING] [argumentation_analysis.utils.dev_tools.env_checks]     Java n'est pas trouvé dans le PATH (FileNotFoundError).

04:23:35 [ERROR] [argumentation_analysis.utils.dev_tools.env_checks] ❌ Des problèmes ont été détectés avec l'environnement Java.

04:23:35 [ERROR] [argumentation_analysis.utils.dev_tools.env_checks] ❌ L'environnement Java n'est pas considéré comme correctement configuré.

------------------------------ Captured log call ------------------------------

INFO     argumentation_analysis.utils.dev_tools.env_checks:env_checks.py:79 ☕ Vérification de l'environnement Java...

INFO     argumentation_analysis.utils.dev_tools.env_checks:env_checks.py:86     JAVA_HOME est défini : /opt/java

INFO     argumentation_analysis.utils.dev_tools.env_checks:env_checks.py:92     JAVA_HOME pointe vers un répertoire Java valide (/mocked/java_home/bin/java).

ERROR    argumentation_analysis.utils.dev_tools.env_checks:env_checks.py:120     Échec de l'exécution de 'java -version'. Code de retour : -1

ERROR    argumentation_analysis.utils.dev_tools.env_checks:env_checks.py:122     Stderr: FileNotFoundError: java

WARNING  argumentation_analysis.utils.dev_tools.env_checks:env_checks.py:131     Java n'est pas trouvé dans le PATH (FileNotFoundError).

ERROR    argumentation_analysis.utils.dev_tools.env_checks:env_checks.py:147 ❌ Des problèmes ont été détectés avec l'environnement Java.

ERROR    argumentation_analysis.utils.dev_tools.env_checks:env_checks.py:166 ❌ L'environnement Java n'est pas considéré comme correctement configuré.

________________ test_compare_rhetorical_analyses_empty_inputs ________________



    def test_compare_rhetorical_analyses_empty_inputs():

        """Teste avec des dictionnaires d'entrée vides."""

        comparison = compare_rhetorical_analyses({}, {})

    

        fdc = comparison["fallacy_detection_comparison"]

        assert fdc["advanced_fallacy_count"] == 0

>       assert fdc["base_fallacy_count"] == 0

E       assert None == 0



tests\unit\argumentation_analysis\utils\test_analysis_comparison.py:129: AssertionError

------------------------------ Captured log call ------------------------------

INFO     argumentation_analysis.utils.analysis_comparison:analysis_comparison.py:37 Début de la comparaison des analyses pour l'extrait: N/A

_______________ test_compare_rhetorical_analyses_invalid_inputs _______________



    def test_compare_rhetorical_analyses_invalid_inputs():

        """Teste avec des entrées qui ne sont pas des dictionnaires."""

        comparison_list_adv = compare_rhetorical_analyses([], {"test": 1})

        assert "error" in comparison_list_adv

>       assert comparison_list_adv["error"] == "Entrées invalides pour la comparaison."

E       assert "Résultats av...s pour 'N/A'." == 'Entrées inva... comparaison.'

E         

E         - Entrées invalides pour la comparaison.

E         + Résultats avancés invalides pour 'N/A'.



tests\unit\argumentation_analysis\utils\test_analysis_comparison.py:146: AssertionError

------------------------------ Captured log call ------------------------------

INFO     argumentation_analysis.utils.analysis_comparison:analysis_comparison.py:37 Début de la comparaison des analyses pour l'extrait: N/A

ERROR    argumentation_analysis.utils.analysis_comparison:analysis_comparison.py:40 Résultats avancés invalides pour 'N/A'. Attendu: dict, Reçu: <class 'list'>

_______________________ test_group_results_nominal_case _______________________



sample_results_various_sources = [{'id': 1, 'source_name': "Discours d'Hitler - 1933", 'text': 'Texte A'}, {'id': 2, 'source_name': 'Débat Lincoln-Doug...urce_name': 'Autre source Lincoln', 'text': 'Texte E'}, {'id': 6, 'source_name': 'Document Y', 'text': 'Texte F'}, ...]



    def test_group_results_nominal_case(sample_results_various_sources):

        """Teste le regroupement nominal des résultats."""

        grouped = group_results_by_corpus(sample_results_various_sources)

    

        assert "Discours d'Hitler" in grouped

        assert len(grouped["Discours d'Hitler"]) == 2

        assert any(r["id"] == 1 for r in grouped["Discours d'Hitler"])

        assert any(r["id"] == 4 for r in grouped["Discours d'Hitler"])

    

        assert "Débats Lincoln-Douglas" in grouped

        assert len(grouped["Débats Lincoln-Douglas"]) == 3

        assert any(r["id"] == 2 for r in grouped["Débats Lincoln-Douglas"])

        assert any(r["id"] == 5 for r in grouped["Débats Lincoln-Douglas"])

        assert any(r["id"] == 7 for r in grouped["Débats Lincoln-Douglas"])

    

>       assert "Autres corpus" in grouped

E       assert 'Autres corpus' in defaultdict(<class 'list'>, {"Discours d'Hitler": [{'id': 1, 'text': 'Texte A', 'source_name': "Discours d'Hitler - 19...e C', 'source_name': 'Article de Blog X'}], 'Document Y': [{'id': 6, 'text': 'Texte F', 'source_name': 'Document Y'}]})



tests\unit\argumentation_analysis\utils\test_data_processing_utils.py:61: AssertionError

------------------------------ Captured log call ------------------------------

INFO     argumentation_analysis.utils.data_processing_utils:data_processing_utils.py:34 Regroupement de 7 résultats par corpus...

INFO     argumentation_analysis.utils.data_processing_utils:data_processing_utils.py:60 Résultats regroupés en 4 corpus.

___________________ test_group_results_missing_source_name ____________________



results_with_missing_source_name = [{'id': 1, 'source_name': "Discours d'Hitler - 1933", 'text': 'Texte A'}, {'id': 2, 'text': 'Texte B'}, {'id': 3, 'source_name': 'Article de Blog X', 'text': 'Texte C'}]



    def test_group_results_missing_source_name(results_with_missing_source_name):

        """Teste le regroupement quand 'source_name' est manquant."""

        grouped = group_results_by_corpus(results_with_missing_source_name)

    

        assert "Discours d'Hitler" in grouped

        assert len(grouped["Discours d'Hitler"]) == 1

        assert grouped["Discours d'Hitler"][0]["id"] == 1

    

>       assert "Autres corpus" in grouped  # Le résultat sans source_name va ici par défaut

E       assert 'Autres corpus' in defaultdict(<class 'list'>, {"Discours d'Hitler": [{'id': 1, 'text': 'Texte A', 'source_name': "Discours d'Hitler - 19...'id': 2, 'text': 'Texte B'}], 'Article de Blog X': [{'id': 3, 'text': 'Texte C', 'source_name': 'Article de Blog X'}]})



tests\unit\argumentation_analysis\utils\test_data_processing_utils.py:79: AssertionError

------------------------------ Captured log call ------------------------------

INFO     argumentation_analysis.utils.data_processing_utils:data_processing_utils.py:34 Regroupement de 3 résultats par corpus...

INFO     argumentation_analysis.utils.data_processing_utils:data_processing_utils.py:60 Résultats regroupés en 3 corpus.

__________________ test_group_results_with_non_dict_elements __________________



results_with_non_dict_elements = [{'id': 1, 'source_name': "Discours d'Hitler - 1933", 'text': 'Texte A'}, "ceci n'est pas un dict", {'id': 3, 'source_name': 'Article de Blog X', 'text': 'Texte C'}]



    def test_group_results_with_non_dict_elements(results_with_non_dict_elements):

        """Teste le regroupement avec des éléments non-dictionnaires dans la liste."""

        grouped = group_results_by_corpus(results_with_non_dict_elements)

    

        assert "Discours d'Hitler" in grouped

        assert len(grouped["Discours d'Hitler"]) == 1

        assert grouped["Discours d'Hitler"][0]["id"] == 1

    

>       assert "Autres corpus" in grouped

E       assert 'Autres corpus' in defaultdict(<class 'list'>, {"Discours d'Hitler": [{'id': 1, 'text': 'Texte A', 'source_name': "Discours d'Hitler - 1933"}], 'Article de Blog X': [{'id': 3, 'text': 'Texte C', 'source_name': 'Article de Blog X'}]})



tests\unit\argumentation_analysis\utils\test_data_processing_utils.py:93: AssertionError

------------------------------ Captured log call ------------------------------

INFO     argumentation_analysis.utils.data_processing_utils:data_processing_utils.py:34 Regroupement de 3 résultats par corpus...

WARNING  argumentation_analysis.utils.data_processing_utils:data_processing_utils.py:40 Élément à l'index 1 n'est pas un dictionnaire et sera ignoré: ceci n'est pas un dict

INFO     argumentation_analysis.utils.data_processing_utils:data_processing_utils.py:60 Résultats regroupés en 2 corpus.

_____________________ test_group_results_all_other_corpus _____________________



    def test_group_results_all_other_corpus():

        """Teste le cas où tous les résultats vont dans 'Autres corpus'."""

        results = [

            {"id": 1, "text": "Texte A", "source_name": "Source Inconnue 1"},

            {"id": 2, "text": "Texte B", "source_name": "Source Inconnue 2"},

        ]

        grouped = group_results_by_corpus(results)

>       assert "Autres corpus" in grouped

E       AssertionError: assert 'Autres corpus' in defaultdict(<class 'list'>, {'Source Inconnue 1': [{'id': 1, 'text': 'Texte A', 'source_name': 'Source Inconnue 1'}], 'Source Inconnue 2': [{'id': 2, 'text': 'Texte B', 'source_name': 'Source Inconnue 2'}]})



tests\unit\argumentation_analysis\utils\test_data_processing_utils.py:119: AssertionError

------------------------------ Captured log call ------------------------------

INFO     argumentation_analysis.utils.data_processing_utils:data_processing_utils.py:34 Regroupement de 2 résultats par corpus...

INFO     argumentation_analysis.utils.data_processing_utils:data_processing_utils.py:60 Résultats regroupés en 2 corpus.

____________ test_estimate_false_positives_negatives_rates_success ____________



sample_base_results_for_error_rates = [{'analyses': {'contextual_fallacies': {'argument_results': [{'detected_fallacies': [...]}, {'detected_fallacies': [..... {'argument_results': [{'detected_fallacies': [...]}]}}, 'extract_name': 'Extract_OnlyBase', 'source_name': 'SourceB'}]

sample_advanced_results_for_error_rates = [{'analyses': {'complex_fallacies': {'composite_severity': {'severity_level': 'Modéré'}}, 'contextual_fallacies': {'co...onfidence': 0.9, 'fallacy_type': 'FalseDilemma'}]}}, 'extract_name': 'Extract_OnlyAdvanced', 'source_name': 'SourceC'}]



    @pytest.mark.use_real_numpy

    def test_estimate_false_positives_negatives_rates_success(

        sample_base_results_for_error_rates: List[Dict[str, Any]],

        sample_advanced_results_for_error_rates: List[Dict[str, Any]]

    ):

        """Teste l'estimation réussie des taux de FP/FN."""

        error_rates = estimate_false_positives_negatives_rates(

            sample_base_results_for_error_rates,

            sample_advanced_results_for_error_rates

        )

        assert "base_contextual" in error_rates

>       assert pytest.approx(error_rates["base_contextual"]["false_positive_rate"], 0.01) == (1/3 + 0)/2



tests\unit\argumentation_analysis\utils\test_error_estimation.py:49: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 0.16666666666666666



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

---------------------------- Captured stderr call -----------------------------

04:23:35 [INFO] [argumentation_analysis.utils.error_estimation] Estimation des taux d'erreur terminée. Extraits communs traités: 2

------------------------------ Captured log call ------------------------------

INFO     argumentation_analysis.utils.error_estimation:error_estimation.py:158 Estimation des taux d'erreur terminée. Extraits communs traités: 2

__________ test_generate_performance_metrics_for_agents_calculations __________



sample_base_results_for_metrics = [{'analyses': {'argument_coherence': {'analysis_timestamp': '2023-01-01T10:00:05', 'coherence_evaluations': {'e1': {}}...is_timestamp': '2023-01-01T10:00:03'}}, 'extract_name': 'E1', 'source_name': 'S1', 'timestamp': '2023-01-01T10:00:00'}]

sample_advanced_results_for_metrics = [{'analyses': {'complex_fallacies': {'analysis_timestamp': '2023-01-01T10:00:30', 'basic_combinations': [{}], 'composi...ce_recommendations': ['ar1', 'ar2']}}}, 'extract_name': 'E1', 'source_name': 'S1', 'timestamp': '2023-01-01T10:00:00'}]



    @pytest.mark.use_real_numpy

    def test_generate_performance_metrics_for_agents_calculations(

        sample_base_results_for_metrics: List[Dict[str, Any]],

        sample_advanced_results_for_metrics: List[Dict[str, Any]]

    ):

        """Teste les valeurs calculées pour certaines métriques."""

        metrics = generate_performance_metrics_for_agents(

            sample_base_results_for_metrics,

            sample_advanced_results_for_metrics

        )

    

        # Base Contextual

        assert metrics["base_contextual"]["fallacy_count"] == 2.0

        assert metrics["base_contextual"]["execution_time"] == 10.0

        assert metrics["base_contextual"]["contextual_richness"] == 2.0

>       assert pytest.approx(metrics["base_contextual"]["false_positive_rate"]) == 1.0



tests\unit\argumentation_analysis\utils\test_metrics_aggregation.py:88: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 1.0



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

---------------------------- Captured stderr call -----------------------------

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] Comptage des sophismes terminé pour 1 extraits.

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] Comptage des sophismes terminé pour 1 extraits.

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] Extraction des scores de confiance terminée pour 1 extraits.

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] Extraction des scores de confiance terminée pour 1 extraits.

04:23:35 [INFO] [argumentation_analysis.utils.error_estimation] Estimation des taux d'erreur terminée. Extraits communs traités: 1

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] 1 extraits avec des temps d'exécution extraits.

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] 1 extraits avec des temps d'exécution extraits.

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] Analyse de la richesse contextuelle terminée pour 1 extraits.

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] Analyse de la richesse contextuelle terminée pour 1 extraits.

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] Évaluation de la pertinence de cohérence terminée pour 1 extraits.

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] Évaluation de la pertinence de cohérence terminée pour 1 extraits.

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] Analyse de la complexité des résultats terminée pour 1 extraits.

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] Analyse de la complexité des résultats terminée pour 1 extraits.

04:23:35 [INFO] [argumentation_analysis.utils.metrics_aggregation] Génération des métriques de performance agrégées terminée.

------------------------------ Captured log call ------------------------------

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:144 Comptage des sophismes terminé pour 1 extraits.

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:144 Comptage des sophismes terminé pour 1 extraits.

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:211 Extraction des scores de confiance terminée pour 1 extraits.

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:211 Extraction des scores de confiance terminée pour 1 extraits.

INFO     argumentation_analysis.utils.error_estimation:error_estimation.py:158 Estimation des taux d'erreur terminée. Extraits communs traités: 1

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:77 1 extraits avec des temps d'exécution extraits.

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:77 1 extraits avec des temps d'exécution extraits.

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:273 Analyse de la richesse contextuelle terminée pour 1 extraits.

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:273 Analyse de la richesse contextuelle terminée pour 1 extraits.

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:334 Évaluation de la pertinence de cohérence terminée pour 1 extraits.

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:334 Évaluation de la pertinence de cohérence terminée pour 1 extraits.

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:385 Analyse de la complexité des résultats terminée pour 1 extraits.

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:385 Analyse de la complexité des résultats terminée pour 1 extraits.

INFO     argumentation_analysis.utils.metrics_aggregation:metrics_aggregation.py:151 Génération des métriques de performance agrégées terminée.

________________________ test_calculate_obj_complexity ________________________



    @pytest.mark.use_real_numpy

    def test_calculate_obj_complexity():

        assert _calculate_obj_complexity("simple_string") == 0.0

        assert _calculate_obj_complexity([]) == 0.0

        assert _calculate_obj_complexity({}) == 0.0

        assert _calculate_obj_complexity(["a", "b"]) == 1.0

        assert _calculate_obj_complexity({"k1": "v1", "k2": "v2"}) == 1.0

>       assert pytest.approx(_calculate_obj_complexity(["a", ["b", "c"]])) == 2.0



tests\unit\argumentation_analysis\utils\test_metrics_extraction.py:247: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 2.0



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

___________________ test_analyze_result_complexity_success ____________________



sample_results_for_complexity = [{'analyses': {'dict_analysis': {'k1': 'v1', 'k2': 'v2'}, 'list_analysis': [1, 2, 3], 'simple_analysis': 'just a strin...me': 'Extract_Comp_NoAnalysesKey'}, {'analyses': 'not_a_dict_or_list', 'extract_name': 'Extract_Comp_AnalysesNotDict'}]



    @pytest.mark.use_real_numpy

    def test_analyze_result_complexity_success(sample_results_for_complexity: List[Dict[str, Any]]):

        results_subset = sample_results_for_complexity[:2]

        complexity_scores = analyze_result_complexity_from_results(results_subset)

        scores1 = complexity_scores["Extract_Comp1"]

>       assert scores1["simple_analysis"] == 0.0 and pytest.approx(scores1["list_analysis"]) == 1.0 and pytest.approx(scores1["dict_analysis"]) == 1.0



tests\unit\argumentation_analysis\utils\test_metrics_extraction.py:267: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:776: in _is_numpy_array

    return _as_numpy_array(obj) is not None

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



obj = 1.0



    def _as_numpy_array(obj: object) -> ndarray | None:

        """

        Return an ndarray if the given object is implicitly convertible to ndarray,

        and numpy is already imported, otherwise None.

        """

        np: Any = sys.modules.get("numpy")

        if np is not None:

            # avoid infinite recursion on numpy scalars, which have __array__

>           if np.isscalar(obj):

E           AttributeError: partially initialized module 'legacy_numpy_array_mock' has no attribute 'isscalar' (most likely due to a circular import)



C:\Users\MYIA\.conda\envs\projet-is\lib\site-packages\_pytest\python_api.py:787: AttributeError

---------------------------- Captured stderr call -----------------------------

04:23:35 [INFO] [argumentation_analysis.utils.metrics_extraction] Analyse de la complexité des résultats terminée pour 2 extraits.

------------------------------ Captured log call ------------------------------

INFO     argumentation_analysis.utils.metrics_extraction:metrics_extraction.py:385 Analyse de la complexité des résultats terminée pour 2 extraits.

______________________ test_split_delimiters_no_content _______________________



    def test_split_delimiters_no_content():

        """Teste des délimiteurs sans contenu suffisant entre eux."""

        text = ". . . ! ? "

        expected = []

>       assert split_text_into_arguments(text, min_arg_length=5) == expected

E       AssertionError: assert ['. . . ! ?'] == []

E         

E         Left contains one more item: '. . . ! ?'

E         Use -v to get more diff



tests\unit\argumentation_analysis\utils\test_text_processing.py:69: AssertionError

============================== warnings summary ===============================

argumentation_analysis\utils\dev_tools\env_checks.py:20

  D:\2025-Epita-Intelligence-Symbolique-4\argumentation_analysis\utils\dev_tools\env_checks.py:20: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.

    import pkg_resources # Pour parser les requirements



tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_argument

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_argument' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_context

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_context' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_context_without_analyzer

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_context_without_analyzer' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_fallacies

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_fallacies' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_rhetoric

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_rhetoric' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_rhetoric_without_analyzer

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_rhetoric_without_analyzer' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_text_with_semantic_kernel

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_text_with_semantic_kernel' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_text_without_semantic_kernel

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_text_without_semantic_kernel' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_identify_arguments_without_kernel

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAgent.test_identify_arguments_without_kernel' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_analysis_methods.py::TestInformalAnalysisMethods::test_analyze_text

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAnalysisMethods.test_analyze_text' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_analysis_methods.py::TestInformalAnalysisMethods::test_analyze_text_with_confidence_threshold

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAnalysisMethods.test_analyze_text_with_confidence_threshold' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_analysis_methods.py::TestInformalAnalysisMethods::test_analyze_text_with_context

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalAnalysisMethods.test_analyze_text_with_context' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_error_handling.py::TestInformalErrorHandling::test_handle_empty_text

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalErrorHandling.test_handle_empty_text' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_error_handling.py::TestInformalErrorHandling::test_handle_fallacy_detector_exception

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalErrorHandling.test_handle_fallacy_detector_exception' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



tests/agents/core/informal/test_informal_error_handling.py::TestInformalErrorHandling::test_handle_none_text

  C:\Users\MYIA\.conda\envs\projet-is\lib\unittest\case.py:549: RuntimeWarning: coroutine 'TestInformalErrorHandling.test_handle_none_text' was never awaited

    method()

  Enable tracemalloc to get traceback where the object was allocated.

  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.



-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

=========================== short test summary info ===========================

FAILED tests/agents/core/informal/test_informal_definitions.py::TestInformalDefinitions::test_initialization

FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_execute_query_accepted

FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_execute_query_error_tweety

FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_execute_query_invalid_formula

FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_execute_query_rejected

FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_generate_queries

FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_text_to_belief_set_empty_result

FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_text_to_belief_set_invalid_belief_set

FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_validate_formula_invalid

FAILED tests/agents/core/logic/test_watson_logic_assistant.py::test_watson_logic_assistant_instanciation

FAILED tests/agents/core/logic/test_watson_logic_assistant.py::test_watson_logic_assistant_instanciation_with_custom_prompt

FAILED tests/agents/core/logic/test_watson_logic_assistant.py::test_watson_logic_assistant_default_name_and_prompt

FAILED tests/agents/core/logic/test_watson_logic_assistant.py::test_get_agent_belief_set_content

FAILED tests/agents/core/logic/test_watson_logic_assistant.py::test_add_new_deduction_result

FAILED tests/agents/core/pm/test_sherlock_enquete_agent.py::test_sherlock_enquete_agent_instanciation

FAILED tests/agents/core/pm/test_sherlock_enquete_agent.py::test_get_current_case_description

FAILED tests/agents/core/pm/test_sherlock_enquete_agent.py::test_add_new_hypothesis

FAILED tests/argumentation_analysis/utils/dev_tools/test_repair_utils.py::test_setup_agents_successful

FAILED tests/environment_checks/test_core_dependencies.py::test_parametrized_dependency_import[sklearn]

FAILED tests/environment_checks/test_core_dependencies.py::test_parametrized_dependency_import[spacy]

FAILED tests/orchestration/test_cluedo_orchestrator.py::test_cluedo_orchestration_flow

FAILED tests/test_numpy_rec_mock.py::TestNumpyRecMock::test_numpy_rec_recarray_instantiation

FAILED tests/test_numpy_rec_mock.py::TestNumpyRecMock::test_numpy_rec_recarray_properties

FAILED tests/ui/test_extract_definition_persistence.py::test_load_definitions_encrypted_wrong_key

FAILED tests/ui/test_extract_definition_persistence.py::test_load_malformed_json

FAILED tests/unit/argumentation_analysis/analytics/test_stats_calculator.py::test_calculate_average_scores_nominal_case

FAILED tests/unit/argumentation_analysis/analytics/test_stats_calculator.py::test_calculate_average_scores_single_result_corpus

FAILED tests/unit/argumentation_analysis/analytics/test_stats_calculator.py::test_calculate_average_scores_mixed_numeric_types

FAILED tests/unit/argumentation_analysis/analytics/test_stats_calculator.py::test_calculate_average_scores_results_not_dicts

FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_mine_arguments_explicit_premise_conclusion_too_short

FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_mine_arguments_implicit_par_consequent

FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_mine_arguments_implicit_ainsi

FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_mine_arguments_explicit_over_implicit

FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_mine_arguments_complex_scenario_mixed

FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_conclusion_delimitation_explicit

FAILED tests/unit/argumentation_analysis/mocks/test_bias_detection.py::test_detect_bias_confirmation

FAILED tests/unit/argumentation_analysis/mocks/test_claim_mining.py::test_extract_claims_global_claim_success

FAILED tests/unit/argumentation_analysis/mocks/test_claim_mining.py::test_extract_claims_global_claim_too_short

FAILED tests/unit/argumentation_analysis/mocks/test_claim_mining.py::test_extract_claims_by_keyword_text_too_short_after_keyword

FAILED tests/unit/argumentation_analysis/mocks/test_clarity_scoring.py::test_score_clarity_long_sentences

FAILED tests/unit/argumentation_analysis/mocks/test_clarity_scoring.py::test_score_clarity_complex_words

FAILED tests/unit/argumentation_analysis/mocks/test_clarity_scoring.py::test_score_clarity_passive_voice_simulation

FAILED tests/unit/argumentation_analysis/mocks/test_clarity_scoring.py::test_score_clarity_jargon

FAILED tests/unit/argumentation_analysis/mocks/test_clarity_scoring.py::test_score_clarity_ambiguity

FAILED tests/unit/argumentation_analysis/mocks/test_clarity_scoring.py::test_score_clarity_custom_jargon

FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_ideal_text

FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_transition_words

FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_repeated_keywords

FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_contradictions

FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_abrupt_topic_change

FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_multiple_factors_and_clamping

FAILED tests/unit/argumentation_analysis/mocks/test_emotional_tone_analysis.py::test_analyze_tone_single_emotion_joy

FAILED tests/unit/argumentation_analysis/mocks/test_emotional_tone_analysis.py::test_analyze_tone_single_emotion_sadness_strong_keyword

FAILED tests/unit/argumentation_analysis/mocks/test_emotional_tone_analysis.py::test_analyze_tone_mixed_emotions_one_dominant_above_threshold

FAILED tests/unit/argumentation_analysis/mocks/test_engagement_analysis.py::test_analyze_engagement_appel_action

FAILED tests/unit/argumentation_analysis/mocks/test_engagement_analysis.py::test_analyze_engagement_pronoms_inclusifs

FAILED tests/unit/argumentation_analysis/mocks/test_engagement_analysis.py::test_analyze_engagement_combination_and_clamping

FAILED tests/unit/argumentation_analysis/mocks/test_evidence_detection.py::test_detect_evidence_by_keyword_text_too_short_after_keyword

FAILED tests/unit/argumentation_analysis/mocks/test_evidence_detection.py::test_detect_evidence_priority_order

FAILED tests/unit/argumentation_analysis/mocks/test_evidence_detection.py::test_detect_evidence_multiple_keywords

FAILED tests/unit/argumentation_analysis/mocks/test_rhetorical_analysis.py::test_analyze_ironic_tone

FAILED tests/unit/argumentation_analysis/mocks/test_rhetorical_analysis.py::test_analyze_combination_all_keywords

FAILED tests/unit/argumentation_analysis/nlp/test_embedding_utils.py::test_get_embeddings_openai_api_error

FAILED tests/unit/argumentation_analysis/nlp/test_embedding_utils.py::test_save_embeddings_data_mkdir_fails

FAILED tests/unit/argumentation_analysis/orchestration/test_advanced_analyzer.py::test_analyze_extract_advanced_successful_run

FAILED tests/unit/argumentation_analysis/pipelines/test_advanced_rhetoric.py::test_run_advanced_rhetoric_pipeline_success

FAILED tests/unit/argumentation_analysis/utils/dev_tools/test_env_checks.py::test_check_java_environment_java_version_fails_filenotfound

FAILED tests/unit/argumentation_analysis/utils/test_analysis_comparison.py::test_compare_rhetorical_analyses_empty_inputs

FAILED tests/unit/argumentation_analysis/utils/test_analysis_comparison.py::test_compare_rhetorical_analyses_invalid_inputs

FAILED tests/unit/argumentation_analysis/utils/test_data_processing_utils.py::test_group_results_nominal_case

FAILED tests/unit/argumentation_analysis/utils/test_data_processing_utils.py::test_group_results_missing_source_name

FAILED tests/unit/argumentation_analysis/utils/test_data_processing_utils.py::test_group_results_with_non_dict_elements

FAILED tests/unit/argumentation_analysis/utils/test_data_processing_utils.py::test_group_results_all_other_corpus

FAILED tests/unit/argumentation_analysis/utils/test_error_estimation.py::test_estimate_false_positives_negatives_rates_success

FAILED tests/unit/argumentation_analysis/utils/test_metrics_aggregation.py::test_generate_performance_metrics_for_agents_calculations

FAILED tests/unit/argumentation_analysis/utils/test_metrics_extraction.py::test_calculate_obj_complexity

FAILED tests/unit/argumentation_analysis/utils/test_metrics_extraction.py::test_analyze_result_complexity_success

FAILED tests/unit/argumentation_analysis/utils/test_text_processing.py::test_split_delimiters_no_content

ERROR tests/argumentation_analysis/utils/dev_tools/test_repair_utils.py::test_run_extract_repair_pipeline_successful_run_no_save

ERROR tests/argumentation_analysis/utils/dev_tools/test_repair_utils.py::test_run_extract_repair_pipeline_with_save_and_json_export

ERROR tests/argumentation_analysis/utils/dev_tools/test_repair_utils.py::test_run_extract_repair_pipeline_hitler_only_filter

===== 78 failed, 843 passed, 15 skipped, 16 warnings, 3 errors in 11.81s ======

[5.019s][debug][jni,resolve] Checked JNI functions are being used to validate JNI usage
[5.029s][debug][jni,resolve] [Dynamic-linking native method org.jpype.JPypeContext.onShutdown ... JNI]
[5.030s][debug][jni,resolve] [Dynamic-linking native method org.jpype.manager.TypeFactoryNative.destroy ... JNI]

