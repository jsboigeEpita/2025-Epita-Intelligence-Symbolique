#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Script de test pour valider les corrections critiques apportÃ©es.
"""

import sys
import os
from pathlib import Path

# Ajouter le projet au chemin Python
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_unified_config():
    """Test de la configuration unifiÃ©e."""
    print("ğŸ”§ Test de config/unified_config.py...")
    try:
        from config.unified_config import get_config, get_test_config
        
        # Test configuration par dÃ©faut
        config = get_config()
        print(f"  âœ… Configuration chargÃ©e")
        print(f"  ğŸ“Š JVM activÃ©e: {config.is_jvm_enabled()}")
        print(f"  ğŸ“Š Mode test: {config.is_testing_mode()}")
        print(f"  ğŸ“Š Agent FOL activÃ©: {config.is_enabled('agents', 'fol_logic')}")
        
        # Test configuration de test
        test_config = get_test_config()
        print(f"  âœ… Configuration de test chargÃ©e")
        print(f"  ğŸ“Š Mode mock: {test_config.is_testing_mode()}")
        print(f"  ğŸ“Š JVM dÃ©sactivÃ©e en test: {not test_config.is_jvm_enabled()}")
        
        return True
    except Exception as e:
        print(f"  âŒ Erreur: {e}")
        return False

def test_analyze_text_script():
    """Test du script CLI analyze_text."""
    print("\nğŸ”§ Test de scripts/main/analyze_text.py...")
    try:
        from scripts.main.analyze_text import setup_argument_parser, create_mock_analysis
        
        # Test parser d'arguments
        parser = setup_argument_parser()
        print(f"  âœ… Parser d'arguments crÃ©Ã©")
        
        # Test analyse mock
        mock_result = create_mock_analysis("Tout le monde sait que cette thÃ©orie est correcte", "scientifique")
        print(f"  âœ… Analyse mock effectuÃ©e")
        print(f"  ğŸ“Š Sophismes dÃ©tectÃ©s: {mock_result['fallacies_detected']}")
        print(f"  ğŸ“Š Mode: {mock_result['mode']}")
        
        return True
    except Exception as e:
        print(f"  âŒ Erreur: {e}")
        return False

def test_existing_modules():
    """Test des modules existants mentionnÃ©s."""
    print("\nğŸ”§ Test des modules existants...")
    try:
        # Test text_utils
        from argumentation_analysis.core.utils.text_utils import clean_text, truncate_text
        test_text = clean_text("  Test avec espaces  ")
        print(f"  âœ… text_utils.py fonctionne: '{test_text}'")
        
        # Test contextual_fallacy_analyzer
        from argumentation_analysis.agents.tools.analysis.contextual_fallacy_analyzer import ContextualFallacyAnalyzer
        analyzer = ContextualFallacyAnalyzer()
        print(f"  âœ… contextual_fallacy_analyzer.py fonctionne")
        
        return True
    except Exception as e:
        print(f"  âŒ Erreur: {e}")
        return False

def test_imports_coherence():
    """Test de cohÃ©rence des imports."""
    print("\nğŸ”§ Test de cohÃ©rence des imports...")
    try:
        # Test import configuration depuis le script analyze_text
        sys.path.append(str(project_root / "scripts" / "main"))
        import importlib.util
        
        # Charger le module analyze_text
        spec = importlib.util.spec_from_file_location("analyze_text", project_root / "scripts" / "main" / "analyze_text.py")
        analyze_text_module = importlib.util.module_from_spec(spec)
        
        print(f"  âœ… Module analyze_text chargeable")
        
        return True
    except Exception as e:
        print(f"  âŒ Erreur: {e}")
        return False

def main():
    """Fonction principale de test."""
    print("=" * 60)
    print("ğŸ§ª VALIDATION DES CORRECTIONS CRITIQUES")
    print("=" * 60)
    
    tests = [
        test_unified_config,
        test_analyze_text_script,
        test_existing_modules,
        test_imports_coherence
    ]
    
    results = []
    for test in tests:
        results.append(test())
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ RÃ‰SUMÃ‰ DES TESTS")
    print("=" * 60)
    
    success_count = sum(results)
    total_count = len(results)
    success_rate = (success_count / total_count) * 100
    
    print(f"Tests rÃ©ussis: {success_count}/{total_count} ({success_rate:.1f}%)")
    
    if success_count == total_count:
        print("ğŸ‰ TOUTES LES CORRECTIONS SONT VALIDÃ‰ES!")
        return 0
    else:
        print(f"âš ï¸  {total_count - success_count} correction(s) nÃ©cessitent une attention")
        return 1

if __name__ == "__main__":
    sys.exit(main())