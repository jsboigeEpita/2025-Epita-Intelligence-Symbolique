{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-md-intro",
   "metadata": {},
   "source": [
    "# üöÄ Analyse Rh√©torique Collaborative par Agents IA - Orchestrateur Principal (v_py)\n",
    "\n",
    "**Objectif:** Ce notebook orchestre et ex√©cute une analyse rh√©torique multi-agents sur un texte donn√©, en utilisant une structure de projet Python modulaire.\n",
    "\n",
    "**Structure Modulaire Utilis√©e:**\n",
    "* `config/`: Fichiers de configuration (`.env`).\n",
    "* `core/`: Composants partag√©s (√âtat, StateManager, Strat√©gies, Setup JVM & LLM).\n",
    "* `utils/`: Fonctions utilitaires.\n",
    "* `ui/`: Logique de l'interface utilisateur (`text_input_ui.py`) et lanceur (`run_ui_configuration.ipynb` - optionnel).\n",
    "* `agents/`: D√©finitions des agents sp√©cialis√©s (PM, Informal, PL).\n",
    "* `orchestration/`: Logique d'ex√©cution de la conversation (`analysis_runner.py`).\n",
    "\n",
    "**Flux d'Ex√©cution:**\n",
    "1.  Chargement de l'environnement (`.env`).\n",
    "2.  Configuration du Logging.\n",
    "3.  Initialisation de la JVM (via `core.jvm_setup`).\n",
    "4.  Cr√©ation du service LLM (via `core.llm_service`).\n",
    "5.  Affichage de l'UI de configuration (via `ui.text_input_ui`) pour obtenir le texte.\n",
    "6.  Ex√©cution de l'analyse collaborative (via `orchestration.analysis_runner`) si un texte est fourni.\n",
    "7.  Affichage des r√©sultats (logs, √©tat final).\n",
    "\n",
    "**Pr√©requis:**\n",
    "* Un fichier `.env` √† la racine contenant les cl√©s API, configurations LLM, et la phrase secr√®te `TEXT_CONFIG_PASSPHRASE`.\n",
    "* Un environnement Java Development Kit (JDK >= 11) correctement install√© et configur√© (`JAVA_HOME`).\n",
    "* Les d√©pendances Python install√©es (voir `requirements.txt` ou `pyproject.toml` du projet).\n",
    "* Les JARs Tweety plac√©s dans le dossier `libs/`.\n",
    "* Le fichier `extract_sources.json.gz.enc` (s'il existe d√©j√†) dans `data/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-md-env",
   "metadata": {},
   "source": [
    "## 1. Chargement de l'Environnement (.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "main-code-env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env charg√©: True\n",
      "LLM Model ID pr√©sent: True\n",
      "LLM API Key pr√©sent: True\n",
      "UI Passphrase pr√©sente: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "loaded = load_dotenv(find_dotenv(), override=True)\n",
    "print(f\".env charg√©: {loaded}\")\n",
    "\n",
    "# V√©rification rapide de quelques variables cl√©s (optionnel)\n",
    "print(f\"LLM Model ID pr√©sent: {'OPENAI_CHAT_MODEL_ID' in os.environ}\")\n",
    "print(f\"LLM API Key pr√©sent: {'OPENAI_API_KEY' in os.environ}\")\n",
    "print(f\"UI Passphrase pr√©sente: {'TEXT_CONFIG_PASSPHRASE' in os.environ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-md-log",
   "metadata": {},
   "source": [
    "## 2. Configuration du Logging Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "main-code-log",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:24 [INFO] [root] Logging configur√©.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Configuration de base - Les modules peuvent d√©finir des loggers plus sp√©cifiques\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, # Mettre DEBUG pour voir plus de d√©tails\n",
    "    format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "\n",
    "# R√©duire la verbosit√© de certaines biblioth√®ques\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"semantic_kernel.connectors.ai\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"semantic_kernel.kernel\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"semantic_kernel.functions\").setLevel(logging.WARNING)\n",
    "# Garder INFO pour l'orchestration et les agents\n",
    "logging.getLogger(\"Orchestration\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"semantic_kernel.agents\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"App.UI\").setLevel(logging.INFO) # Logger pour l'UI\n",
    "\n",
    "logging.info(\"Logging configur√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-md-jvm",
   "metadata": {},
   "source": [
    "## 3. Initialisation de la JVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "main-code-jvm",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:24 [INFO] [root] Tentative d'initialisation de la JVM...\n",
      "15:47:24 [INFO] [Orchestration.JPype] \n",
      "--- Pr√©paration et Initialisation de la JVM via JPype ---\n",
      "15:47:24 [INFO] [Orchestration.JPype] \n",
      "--- V√©rification/T√©l√©chargement des JARs Tweety v1.28 ---\n",
      "15:47:24 [INFO] [Orchestration.JPype] V√©rification de l'acc√®s √† https://tweetyproject.org/builds/1.28/...\n",
      "15:47:25 [INFO] [Orchestration.JPype] ‚úîÔ∏è URL de base Tweety v1.28 accessible.\n",
      "15:47:25 [INFO] [Orchestration.JPype] \n",
      "--- V√©rification/T√©l√©chargement JAR Core ---\n",
      "15:47:25 [INFO] [Orchestration.JPype] ‚úîÔ∏è JAR Core 'org.tweetyproject.tweety-full-1.28-with-dependencies.jar': d√©j√† pr√©sent.\n",
      "15:47:25 [INFO] [Orchestration.JPype] \n",
      "--- V√©rification/T√©l√©chargement des 32 JARs de modules ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d0eac2533c4787926031914f7f589f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Modules JARs:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:25 [INFO] [Orchestration.JPype] -> Modules: 0 t√©l√©charg√©s, 32/32 pr√©sents.\n",
      "15:47:25 [INFO] [Orchestration.JPype] \n",
      "--- V√©rification/T√©l√©chargement des 3 binaires natifs (Windows) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce47a41d76a8487dafae6d697d414098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binaires Natifs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:25 [INFO] [Orchestration.JPype] -> Binaires natifs: 0 t√©l√©charg√©s, 3/3 pr√©sents.\n",
      "15:47:25 [INFO] [Orchestration.JPype]    Note: S'assurer que le chemin 'C:\\dev\\2025-Epita-Intelligence-Symbolique\\argumentation_analysis\\libs\\native' est inclus dans java.library.path lors du d√©marrage JVM.\n",
      "15:47:25 [INFO] [Orchestration.JPype] --- Fin V√©rification/T√©l√©chargement Tweety ---\n",
      "15:47:25 [INFO] [Orchestration.JPype] ‚ÑπÔ∏è Variable d'environnement JAVA_HOME non d√©finie.\n",
      "15:47:25 [INFO] [Orchestration.JPype] ‚ÑπÔ∏è Tentative de d√©tection via heuristiques sp√©cifiques √† l'OS...\n",
      "15:47:25 [INFO] [Orchestration.JPype]   1 installations Java potentielles trouv√©es par heuristique.\n",
      "15:47:25 [INFO] [Orchestration.JPype] ‚úîÔ∏è R√©pertoire Java Home valide trouv√© via heuristique: C:\\Program Files\\Java\\jdk-21\n",
      "15:47:25 [INFO] [Orchestration.JPype] ‚úÖ JAVA_HOME d√©fini dynamiquement √† 'C:\\Program Files\\Java\\jdk-21' pour cette session.\n",
      "15:47:25 [INFO] [Orchestration.JPype] ‚è≥ Tentative de d√©marrage JVM...\n",
      "15:47:25 [INFO] [Orchestration.JPype]    (Chemin JVM par d√©faut d√©tect√© par JPype: C:\\Program Files\\Java\\jdk-21\\bin\\server\\jvm.dll)\n",
      "15:47:25 [INFO] [Orchestration.JPype]    Classpath construit (33 JARs depuis 'libs').\n",
      "15:47:25 [INFO] [Orchestration.JPype]    Argument JVM natif ajout√©: -Djava.library.path=C:\\dev\\2025-Epita-Intelligence-Symbolique\\argumentation_analysis\\libs\\native\n",
      "15:47:26 [INFO] [Orchestration.JPype] ‚úÖ JVM d√©marr√©e avec succ√®s et domaines enregistr√©s.\n",
      "15:47:26 [INFO] [Orchestration.JPype] \n",
      "‚úÖ JVM pr√™te pour utilisation.\n",
      "15:47:26 [INFO] [Orchestration.JPype] --- Fin Initialisation JVM ---\n",
      "15:47:26 [INFO] [root] ‚úÖ JVM initialis√©e avec succ√®s ou d√©j√† active.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from core.jvm_setup import initialize_jvm\n",
    "\n",
    "logging.info(\"Tentative d'initialisation de la JVM...\")\n",
    "# La fonction initialize_jvm g√®re maintenant aussi le t√©l√©chargement des JARs\n",
    "jvm_ready_status = initialize_jvm(lib_dir_path=\"libs\")\n",
    "\n",
    "if jvm_ready_status:\n",
    "    logging.info(\"‚úÖ JVM initialis√©e avec succ√®s ou d√©j√† active.\")\n",
    "else:\n",
    "    logging.warning(\"‚ö†Ô∏è JVM n'a pas pu √™tre initialis√©e. L'agent PropositionalLogicAgent ne fonctionnera pas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-md-llm",
   "metadata": {},
   "source": [
    "## 4. Cr√©ation du Service LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "main-code-llm",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:34 [INFO] [root] Cr√©ation du service LLM...\n",
      "15:47:34 [INFO] [Orchestration.LLM] --- Configuration du Service LLM (global_llm_service) ---\n",
      "15:47:34 [INFO] [Orchestration.LLM] Configuration Service: OpenAIChatCompletion...\n",
      "15:47:35 [INFO] [Orchestration.LLM] Service LLM OpenAI (gpt-4o-mini) cr√©√© avec ID 'global_llm_service'.\n",
      "15:47:35 [INFO] [root] ‚úÖ Service LLM cr√©√© avec succ√®s (ID: global_llm_service).\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from core.llm_service import create_llm_service\n",
    "\n",
    "llm_service = None\n",
    "try:\n",
    "    logging.info(\"Cr√©ation du service LLM...\")\n",
    "    llm_service = create_llm_service() # Utilise l'ID par d√©faut\n",
    "    logging.info(f\"‚úÖ Service LLM cr√©√© avec succ√®s (ID: {llm_service.service_id}).\")\n",
    "except Exception as e:\n",
    "    logging.critical(f\"‚ùå √âchec critique de la cr√©ation du service LLM: {e}\", exc_info=True)\n",
    "    print(f\"‚ùå ERREUR: Impossible de cr√©er le service LLM. V√©rifiez la configuration .env et les logs.\")\n",
    "    # raise # D√©commenter pour arr√™ter si LLM indispensable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-md-ui",
   "metadata": {},
   "source": [
    "## 5. Configuration de la T√¢che via l'Interface Utilisateur\n",
    "\n",
    "La cellule suivante importe et ex√©cute la fonction `configure_analysis_task` du module `ui.app`.\n",
    "\n",
    "Cela affichera l'interface utilisateur. Interagissez avec l'UI (s√©lectionnez une source, pr√©parez/extrayez le texte si n√©cessaire) puis cliquez sur **\"Lancer l'Analyse\"**. Le texte pr√©par√© sera retourn√© et stock√© pour l'√©tape suivante.\n",
    "\n",
    "**La cellule attendra la fin de votre interaction avec l'UI.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "main-code-ui",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:35 [INFO] [App.UI.Config] V√©rification de la phrase secr√®te 'TEXT_CONFIG_PASSPHRASE' dans .env...\n",
      "15:47:35 [INFO] [App.UI.Config] ‚úÖ Phrase secr√®te trouv√©e. D√©rivation de la cl√©...\n",
      "15:47:35 [INFO] [App.UI.Config] ‚úÖ Cl√© de chiffrement d√©riv√©e et encod√©e.\n",
      "15:47:35 [INFO] [App.UI.Config] Cache r√©pertoire assur√© : C:\\dev\\2025-Epita-Intelligence-Symbolique\\argumentation_analysis\\text_cache\n",
      "15:47:35 [INFO] [App.UI.Config] R√©pertoire config UI assur√© : C:\\dev\\2025-Epita-Intelligence-Symbolique\\argumentation_analysis\\data\n",
      "15:47:35 [INFO] [App.UI.Config] R√©pertoire temporaire assur√© : C:\\dev\\2025-Epita-Intelligence-Symbolique\\argumentation_analysis\\temp_downloads\n",
      "15:47:35 [INFO] [App.UI.Config] ‚úÖ Configuration charg√©e depuis extract_sources.json\n",
      "15:47:35 [INFO] [App.UI.Config] Config UI initialis√©e. 4 sources charg√©es.\n",
      "15:47:35 [INFO] [App.UI.Utils] Fonctions utilitaires UI d√©finies.\n",
      "15:47:35 [CRITICAL] [root] ‚ùå ERREUR: Impossible d'importer l'UI: cannot import name 'CONFIG_FILE' from 'ui.config' (c:\\dev\\2025-Epita-Intelligence-Symbolique\\argumentation_analysis\\ui\\config.py)\n",
      "15:47:35 [WARNING] [root] \n",
      "‚ùå Aucun texte pr√©par√© via l'UI. L'analyse ne peut pas continuer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ERREUR d'importation de l'interface utilisateur: cannot import name 'CONFIG_FILE' from 'ui.config' (c:\\dev\\2025-Epita-Intelligence-Symbolique\\argumentation_analysis\\ui\\config.py). V√©rifiez la structure du projet et les __init__.py.\n",
      "\n",
      "‚ùå Aucun texte pr√©par√© via l'UI. L'analyse ne peut pas continuer.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import traceback\n",
    "\n",
    "texte_pour_analyse = None # Initialiser\n",
    "\n",
    "try:\n",
    "    # Importer la fonction UI depuis le module .py\n",
    "    # Assurez-vous que ce notebook est ex√©cut√© depuis la racine du projet\n",
    "    from ui.app import configure_analysis_task\n",
    "    logging.info(\"Fonction 'configure_analysis_task' import√©e depuis ui.app.\")\n",
    "\n",
    "    # Appeler la fonction pour afficher l'UI et obtenir le texte\n",
    "    logging.info(\"Lancement de l'interface de configuration...\")\n",
    "    texte_pour_analyse = configure_analysis_task() # Bloque jusqu'au clic sur \"Lancer\"\n",
    "\n",
    "except ImportError as e_import:\n",
    "    logging.critical(f\"‚ùå ERREUR: Impossible d'importer l'UI: {e_import}\")\n",
    "    print(f\"‚ùå ERREUR d'importation de l'interface utilisateur: {e_import}. V√©rifiez la structure du projet et les __init__.py.\")\n",
    "except Exception as e_ui:\n",
    "    logging.error(f\"‚ùå Une erreur est survenue lors de l'ex√©cution de l'interface utilisateur : {e_ui}\", exc_info=True)\n",
    "    print(f\"‚ùå Une erreur est survenue pendant l'ex√©cution de l'UI : {e_ui}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# V√©rifier si on a bien re√ßu du texte apr√®s l'interaction UI\n",
    "if not texte_pour_analyse:\n",
    "    logging.warning(\"\\n‚ùå Aucun texte pr√©par√© via l'UI. L'analyse ne peut pas continuer.\")\n",
    "    print(\"\\n‚ùå Aucun texte pr√©par√© via l'UI. L'analyse ne peut pas continuer.\")\n",
    "else:\n",
    "    logging.info(f\"\\n‚úÖ Texte pr√™t pour l'analyse (longueur: {len(texte_pour_analyse)}) r√©cup√©r√© via l'UI.\")\n",
    "    print(f\"\\n‚úÖ Texte pr√™t pour l'analyse (longueur: {len(texte_pour_analyse)}). Passage √† l'ex√©cution.\")\n",
    "    # print(\"--- Extrait Texte --- \\n\", texte_pour_analyse[:500] + \"...\") # D√©commenter pour voir extrait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-md-run",
   "metadata": {},
   "source": [
    "## 6. Ex√©cution de l'Analyse Collaborative\n",
    "\n",
    "Si un texte a √©t√© pr√©par√© avec succ√®s via l'UI et que le service LLM est disponible, cette cellule lance l'analyse collaborative en appelant la fonction `run_analysis_conversation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "main-code-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:35 [WARNING] [root] Analyse non lanc√©e : aucun texte n'a √©t√© pr√©par√© via l'UI.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Analyse non lanc√©e : aucun texte n'a √©t√© pr√©par√© via l'UI.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "# Appliquer nest_asyncio pour compatibilit√© Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Lancer seulement si on a un texte ET un service LLM valide\n",
    "if texte_pour_analyse and llm_service:\n",
    "    logging.info(\"\\nüöÄ Tentative de lancement de l'ex√©cution asynchrone de l'analyse...\")\n",
    "    print(\"\\nüöÄ Lancement de l'analyse collaborative (peut prendre du temps)... \")\n",
    "    try:\n",
    "        # Importer la fonction d'orchestration\n",
    "        from orchestration.analysis_runner import run_analysis_conversation\n",
    "\n",
    "        # Ex√©cuter la fonction d'analyse en passant le texte et le service LLM\n",
    "        # asyncio.run() g√®re la boucle d'√©v√©nements\n",
    "        asyncio.run(run_analysis_conversation(\n",
    "            texte_a_analyser=texte_pour_analyse,\n",
    "            llm_service=llm_service\n",
    "        ))\n",
    "\n",
    "        logging.info(\"\\nüèÅ Ex√©cution asyncio.run termin√©e.\")\n",
    "        print(\"\\nüèÅ Analyse termin√©e.\")\n",
    "\n",
    "    except ImportError as e_import_run:\n",
    "         logging.critical(f\"‚ùå ERREUR: Impossible d'importer 'run_analysis_conversation': {e_import_run}\")\n",
    "         print(f\"‚ùå ERREUR d'importation de la fonction d'orchestration: {e_import_run}\")\n",
    "    except Exception as e_analysis:\n",
    "        logging.error(f\"\\n‚ùå Une erreur est survenue pendant l'ex√©cution de l'analyse : {e_analysis}\", exc_info=True)\n",
    "        print(f\"\\n‚ùå Une erreur est survenue pendant l'ex√©cution de l'analyse : {e_analysis}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "elif not texte_pour_analyse:\n",
    "    logging.warning(\"Analyse non lanc√©e : aucun texte n'a √©t√© pr√©par√© via l'UI.\")\n",
    "    print(\"\\n Analyse non lanc√©e : aucun texte n'a √©t√© pr√©par√© via l'UI.\")\n",
    "else: # Implique que llm_service est None ou invalide\n",
    "     logging.error(\"Analyse non lanc√©e : le service LLM n'a pas pu √™tre configur√© ou est invalide.\")\n",
    "     print(\"\\n Analyse non lanc√©e : le service LLM n'a pas pu √™tre configur√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-md-results",
   "metadata": {},
   "source": [
    "## 7. R√©sultats et Conclusion\n",
    "\n",
    "V√©rifiez les logs et l'√©tat final JSON affich√©s par l'ex√©cution pr√©c√©dente pour voir le r√©sultat de l'analyse collaborative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-md-next",
   "metadata": {},
   "source": [
    "## 8. üèÅ Pistes d'Am√©lioration Futures\n",
    "\n",
    "*(Repris des notebooks pr√©c√©dents)*\n",
    "\n",
    "**Prochaines √©tapes possibles :**\n",
    "* **Activer & Finaliser PL:** Impl√©menter r√©ellement les appels JPype/Tweety dans `PropositionalLogicPlugin._internal_execute_query` et tester de bout en bout l'ex√©cution des requ√™tes logiques (parsing, query, interpr√©tation).\n",
    "* **Affiner Analyse Sophismes:** Am√©liorer les instructions de `InformalAnalysisAgent` pour une exploration plus fine de la taxonomie (gestion de la profondeur, choix des branches) ou l'attribution de sophismes sp√©cifiques bas√©e sur les d√©tails r√©cup√©r√©s (`get_fallacy_details`).\n",
    "* **Externaliser Prompts & Config:** D√©placer les prompts et configurations (ex: noms agents, constantes) hors du code Python vers des fichiers d√©di√©s (YAML, JSON, .env) pour une meilleure maintenabilit√©. Utiliser `kernel.import_plugin_from_directory`.\n",
    "* **Gestion Erreurs Agents:** Renforcer la capacit√© des agents √† g√©rer les erreurs retourn√©es par les outils (`FUNC_ERROR:`) et √† adapter leur plan (ex: demander une clarification, r√©essayer, passer √† autre chose).\n",
    "* **Nouveaux Agents/Capacit√©s:** Impl√©menter des agents pour d'autres logiques (FOL, Modale), d'autres t√¢ches (r√©sum√©, extraction d'entit√©s) ou d'autres outils (recherche web, base de donn√©es).\n",
    "* **√âtat RDF/KG:** Explorer le passage √† une structure d'√©tat plus riche et s√©mantiquement structur√©e en utilisant RDF/KG (avec `rdflib` ou une base de graphe) pour repr√©senter les arguments, relations, et m√©tadonn√©es de mani√®re plus formelle.\n",
    "* **Interface Utilisateur:** Cr√©er une interface (ex: avec Gradio, Streamlit) pour faciliter l'interaction et la visualisation de l'analyse (au-del√† de l'UI de configuration initiale)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (arg-analysis-venv)",
   "language": "python",
   "name": "arg-analysis-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
