#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
üöÄ Analyse Rh√©torique Collaborative par Agents IA - Orchestrateur Principal (v_py)

Ce script orchestre et ex√©cute une analyse rh√©torique multi-agents sur un texte donn√©,
en utilisant une structure de projet Python modulaire.

Structure Modulaire Utilis√©e:
* `config/`: Fichiers de configuration (`.env`).
* `core/`: Composants partag√©s (√âtat, StateManager, Strat√©gies, Setup JVM & LLM).
* `utils/`: Fonctions utilitaires.
* `ui/`: Logique de l'interface utilisateur et lanceur.
* `agents/`: D√©finitions des agents sp√©cialis√©s (PM, Informal, PL).
* `orchestration/`: Logique d'ex√©cution de la conversation.

Flux d'Ex√©cution:
1. Chargement de l'environnement (`.env`).
2. Configuration du Logging.
3. Initialisation de la JVM (via `core.jvm_setup`).
4. Cr√©ation du service LLM (via `core.llm_service`).
5. Affichage de l'UI de configuration (via `ui.app`) pour obtenir le texte.
6. Ex√©cution de l'analyse collaborative (via `orchestration.analysis_runner`) si un texte est fourni.
7. Affichage des r√©sultats (logs, √©tat final).

Pr√©requis:
* Un fichier `.env` √† la racine contenant les cl√©s API, configurations LLM, et la phrase secr√®te `TEXT_CONFIG_PASSPHRASE`.
* Un environnement Java Development Kit (JDK >= 11) correctement install√© et configur√© (`JAVA_HOME`).
* Les d√©pendances Python install√©es (voir `requirements.txt` ou `pyproject.toml` du projet).
* Les JARs Tweety plac√©s dans le dossier `libs/`.
* Le fichier `extract_sources.json.gz.enc` (s'il existe d√©j√†) dans `data/`.
"""

import os
import sys
import logging
import traceback
import asyncio
import argparse
from pathlib import Path

# Ajouter le r√©pertoire parent au chemin de recherche des modules
current_dir = Path(__file__).parent
if str(current_dir) not in sys.path:
    sys.path.append(str(current_dir))

# Activation automatique de l'environnement
from scripts.core.auto_env import ensure_env
ensure_env()

def setup_logging():
    """Configuration du logging global"""
    # Configuration de base - Les modules peuvent d√©finir des loggers plus sp√©cifiques
    logging.basicConfig(
        level=logging.INFO,  # Mettre DEBUG pour voir plus de d√©tails
        format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',
        datefmt='%H:%M:%S'
    )

    # R√©duire la verbosit√© de certaines biblioth√®ques
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("openai").setLevel(logging.WARNING)
    logging.getLogger("requests").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("semantic_kernel.connectors.ai").setLevel(logging.WARNING)
    logging.getLogger("semantic_kernel.kernel").setLevel(logging.WARNING)
    logging.getLogger("semantic_kernel.functions").setLevel(logging.WARNING)
    
    # Garder INFO pour l'orchestration et les agents
    logging.getLogger("Orchestration").setLevel(logging.INFO)
    logging.getLogger("semantic_kernel.agents").setLevel(logging.INFO)
    logging.getLogger("App.UI").setLevel(logging.INFO)  # Logger pour l'UI

    logging.info("Logging configur√©.")

async def main():
    """Fonction principale d'ex√©cution de l'analyse rh√©torique"""
    parser = argparse.ArgumentParser(description="Analyse Rh√©torique Collaborative par Agents IA")
    parser.add_argument("--skip-ui", action="store_true", help="Sauter l'interface utilisateur et utiliser un texte pr√©d√©fini")
    parser.add_argument("--text-file", type=str, help="Chemin vers un fichier texte √† analyser (utilis√© avec --skip-ui)")
    args = parser.parse_args()

    # 1. Environnement d√©j√† configur√© par auto_env
    print("‚úÖ Environnement configur√© via auto_env")

    # V√©rification rapide de quelques variables cl√©s (optionnel)
    print(f"LLM Model ID pr√©sent: {'OPENAI_CHAT_MODEL_ID' in os.environ}")
    print(f"LLM API Key pr√©sent: {'OPENAI_API_KEY' in os.environ}")
    print(f"UI Passphrase pr√©sente: {'TEXT_CONFIG_PASSPHRASE' in os.environ}")

    # 2. Configuration du Logging Global
    setup_logging()

    # 3. Initialisation de la JVM
    from argumentation_analysis.core.jvm_setup import initialize_jvm
    from argumentation_analysis.paths import LIBS_DIR
    logging.info("Tentative d'initialisation de la JVM...")
    # La fonction initialize_jvm g√®re maintenant aussi le t√©l√©chargement des JARs
    jvm_ready_status = initialize_jvm(lib_dir_path=LIBS_DIR)

    if jvm_ready_status:
        logging.info("[OK] JVM initialis√©e avec succ√®s ou d√©j√† active.")
    else:
        logging.warning("‚ö†Ô∏è JVM n'a pas pu √™tre initialis√©e. L'agent PropositionalLogicAgent ne fonctionnera pas.")

    # 4. Cr√©ation du Service LLM
    from argumentation_analysis.core.llm_service import create_llm_service
    llm_service = None
    try:
        logging.info("Cr√©ation du service LLM...")
        llm_service = create_llm_service()  # Utilise l'ID par d√©faut
        logging.info(f"[OK] Service LLM cr√©√© avec succ√®s (ID: {llm_service.service_id}).")
    except Exception as e:
        logging.critical(f"‚ùå √âchec critique de la cr√©ation du service LLM: {e}", exc_info=True)
        print(f"‚ùå ERREUR: Impossible de cr√©er le service LLM. V√©rifiez la configuration .env et les logs.")
        # raise  # D√©commenter pour arr√™ter si LLM indispensable

    # 5. Configuration de la T√¢che via l'Interface Utilisateur
    texte_pour_analyse = None  # Initialiser

    if args.skip_ui:
        # Mode sans UI: utiliser un texte pr√©d√©fini ou depuis un fichier
        if args.text_file:
            try:
                with open(args.text_file, 'r', encoding='utf-8') as f:
                    texte_pour_analyse = f.read()
                logging.info(f"[OK] Texte charg√© depuis le fichier: {args.text_file}")
            except Exception as e:
                logging.error(f"‚ùå Erreur lors de la lecture du fichier texte: {e}")
                return
        else:
            # Texte d'exemple si aucun fichier n'est sp√©cifi√©
            texte_pour_analyse = "Ceci est un texte d'exemple pour l'analyse rh√©torique."
            logging.info("[OK] Utilisation du texte d'exemple pr√©d√©fini.")
    else:
        # Mode normal avec UI
        try:
            # Importer la fonction UI depuis le module .py
            from argumentation_analysis.ui.app import configure_analysis_task
            logging.info("Fonction 'configure_analysis_task' import√©e depuis ui.app.")

            # Appeler la fonction pour afficher l'UI et obtenir le texte
            logging.info("Lancement de l'interface de configuration...")
            texte_pour_analyse = configure_analysis_task()  # Bloque jusqu'au clic sur "Lancer"

        except ImportError as e_import:
            logging.critical(f"‚ùå ERREUR: Impossible d'importer l'UI: {e_import}")
            print(f"‚ùå ERREUR d'importation de l'interface utilisateur: {e_import}. V√©rifiez la structure du projet et les __init__.py.")
            return
        except Exception as e_ui:
            logging.error(f"‚ùå Une erreur est survenue lors de l'ex√©cution de l'interface utilisateur : {e_ui}", exc_info=True)
            print(f"‚ùå Une erreur est survenue pendant l'ex√©cution de l'UI : {e_ui}")
            traceback.print_exc()
            return

    # V√©rifier si on a bien re√ßu du texte apr√®s l'interaction UI
    if not texte_pour_analyse:
        logging.warning("\n‚ùå Aucun texte pr√©par√©. L'analyse ne peut pas continuer.")
        print("\n‚ùå Aucun texte pr√©par√©. L'analyse ne peut pas continuer.")
        return
    else:
        logging.info(f"\n[OK] Texte pr√™t pour l'analyse (longueur: {len(texte_pour_analyse)}).")
        print(f"\n[OK] Texte pr√™t pour l'analyse (longueur: {len(texte_pour_analyse)}). Passage √† l'ex√©cution.")
        # print("--- Extrait Texte --- \n", texte_pour_analyse[:500] + "...") # D√©commenter pour voir extrait

    # 6. Ex√©cution de l'Analyse Collaborative
    # Lancer seulement si on a un texte ET un service LLM valide
    if texte_pour_analyse and llm_service:
        logging.info("\n[LAUNCH] Tentative de lancement de l'execution asynchrone de l'analyse...")
        print("\n[LAUNCH] Lancement de l'analyse collaborative (peut prendre du temps)... ")
        # Importer les d√©pendances n√©cessaires
        from argumentation_analysis.orchestration.analysis_runner import run_analysis_conversation
        
        try:
            # Ex√©cuter la fonction d'analyse en passant le texte et le service LLM
            await run_analysis_conversation(
                texte_a_analyser=texte_pour_analyse,
                llm_service=llm_service
            )

            logging.info("\n[FINISH] Execution terminee.")
            print("\n[FINISH] Analyse terminee.")

        except ImportError as e_import_run:
            logging.critical(f"‚ùå ERREUR: Impossible d'importer 'run_analysis_conversation': {e_import_run}")
            print(f"‚ùå ERREUR d'importation de la fonction d'orchestration: {e_import_run}")
        except Exception as e_analysis:
            logging.error(f"\n‚ùå Une erreur est survenue pendant l'ex√©cution de l'analyse : {e_analysis}", exc_info=True)
            print(f"\n‚ùå Une erreur est survenue pendant l'ex√©cution de l'analyse : {e_analysis}")
            traceback.print_exc()

    elif not texte_pour_analyse:
        logging.warning("Analyse non lanc√©e : aucun texte n'a √©t√© pr√©par√©.")
        print("\n Analyse non lanc√©e : aucun texte n'a √©t√© pr√©par√©.")
    else:  # Implique que llm_service est None ou invalide
        logging.error("Analyse non lanc√©e : le service LLM n'a pas pu √™tre configur√© ou est invalide.")
        print("\n Analyse non lanc√©e : le service LLM n'a pas pu √™tre configur√©.")

if __name__ == "__main__":
    # Utiliser asyncio.run() pour ex√©cuter la fonction principale asynchrone
    asyncio.run(main())