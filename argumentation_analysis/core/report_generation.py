#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
G√©n√©rateur de rapports unifi√© pour l'√©cosyst√®me d'analyse argumentative.

Ce module centralise la g√©n√©ration de rapports pour tous les composants refactoris√©s,
en unifiant les capacit√©s du g√©n√©rateur unifi√© avec l'infrastructure pipeline existante.
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Callable
import yaml
from dataclasses import dataclass

# Import des utilitaires existants
from ..utils.core_utils.reporting_utils import (
    save_json_report, save_text_report, 
    generate_markdown_report_for_corpus, generate_overall_summary_markdown
)
from ..utils.core_utils.file_utils import save_markdown_to_html

logger = logging.getLogger(__name__)

@dataclass
class ReportMetadata:
    """M√©tadonn√©es standardis√©es pour tous les rapports."""
    source_component: str  # Composant source (orchestrator, pipeline, etc.)
    analysis_type: str     # Type d'analyse (conversation, LLM, rhetoric, etc.)
    generated_at: datetime
    version: str = "1.0.0"
    generator: str = "UnifiedReportGeneration"
    format_type: str = "markdown"
    template_name: str = "default"

@dataclass 
class ReportConfiguration:
    """Configuration compl√®te pour la g√©n√©ration de rapports."""
    output_format: str = "markdown"  # console, markdown, json, html
    template_name: str = "default"
    output_mode: str = "file"        # file, console, both
    include_metadata: bool = True
    include_visualizations: bool = False
    include_recommendations: bool = True
    output_directory: Optional[Path] = None
    custom_sections: Optional[List[str]] = None

class UnifiedReportTemplate:
    """Template de rapport unifi√© et extensible."""
    
    def __init__(self, config: Dict[str, Any]):
        self.name = config.get("name", "default")
        self.format_type = config.get("format", "markdown")
        self.sections = config.get("sections", [])
        self.metadata = config.get("metadata", {})
        self.custom_renderers = config.get("custom_renderers", {})
        
    def render(self, data: Dict[str, Any], metadata: ReportMetadata) -> str:
        """Rend le template avec donn√©es et m√©tadonn√©es."""
        enriched_data = {
            "report_metadata": {
                "generated_at": metadata.generated_at.isoformat(),
                "generator": metadata.generator,
                "version": metadata.version,
                "source_component": metadata.source_component,
                "analysis_type": metadata.analysis_type,
                "template": metadata.template_name
            },
            **data
        }
        
        if self.format_type == "markdown":
            return self._render_markdown(enriched_data)
        elif self.format_type == "console":
            return self._render_console(enriched_data)
        elif self.format_type == "json":
            return self._render_json(enriched_data)
        elif self.format_type == "html":
            return self._render_html(enriched_data)
        else:
            raise ValueError(f"Format non support√©: {self.format_type}")
    
    def _render_markdown(self, data: Dict[str, Any]) -> str:
        """G√©n√®re un rapport Markdown unifi√©."""
        lines = []
        metadata = data.get("report_metadata", {})
        
        # En-t√™te principal avec composant source
        title = data.get("title", f"RAPPORT D'ANALYSE - {metadata.get('source_component', 'SYST√àME').upper()}")
        lines.append(f"# {title}")
        lines.append("")
        
        # Informations sur l'origine du rapport
        lines.append("## üèóÔ∏è M√©tadonn√©es du rapport")
        lines.append(f"- **Composant source**: {metadata.get('source_component', 'N/A')}")
        lines.append(f"- **Type d'analyse**: {metadata.get('analysis_type', 'N/A')}")
        lines.append(f"- **Date de g√©n√©ration**: {metadata.get('generated_at', 'N/A')}")
        lines.append(f"- **Version du g√©n√©rateur**: {metadata.get('version', 'N/A')}")
        lines.append("")
        
        # M√©tadonn√©es d'analyse (si disponibles)
        if "metadata" in data:
            lines.append("## üìä M√©tadonn√©es de l'analyse")
            analysis_metadata = data["metadata"]
            lines.append(f"- **Source analys√©e**: {analysis_metadata.get('source_description', 'N/A')}")
            lines.append(f"- **Type de source**: {analysis_metadata.get('source_type', 'N/A')}")
            lines.append(f"- **Longueur du texte**: {analysis_metadata.get('text_length', 'N/A')} caract√®res")
            lines.append(f"- **Temps de traitement**: {analysis_metadata.get('processing_time_ms', 'N/A')}ms")
            lines.append("")
        
        # R√©sum√© ex√©cutif
        if "summary" in data:
            lines.append("## üìã R√©sum√© ex√©cutif")
            summary = data["summary"]
            lines.append(f"- **Sophistication rh√©torique**: {summary.get('rhetorical_sophistication', 'N/A')}")
            lines.append(f"- **Niveau de manipulation**: {summary.get('manipulation_level', 'N/A')}")
            lines.append(f"- **Validit√© logique**: {summary.get('logical_validity', 'N/A')}")
            lines.append(f"- **Confiance globale**: {summary.get('confidence_score', 'N/A')}")
            
            # R√©sum√© sp√©cifique √† l'orchestration
            if "orchestration_summary" in summary:
                orch_summary = summary["orchestration_summary"]
                lines.append(f"- **Agents mobilis√©s**: {orch_summary.get('agents_count', 'N/A')}")
                lines.append(f"- **Temps d'orchestration**: {orch_summary.get('orchestration_time_ms', 'N/A')}ms")
                lines.append(f"- **Statut d'ex√©cution**: {orch_summary.get('execution_status', 'N/A')}")
            lines.append("")
        
        # R√©sultats d'orchestration (pour les orchestrateurs)
        if "orchestration_results" in data:
            lines.append("## üéº R√©sultats d'orchestration")
            orch_data = data["orchestration_results"]
            
            if "execution_plan" in orch_data:
                plan = orch_data["execution_plan"]
                lines.append("### Plan d'ex√©cution")
                lines.append(f"- **Strat√©gie s√©lectionn√©e**: {plan.get('strategy', 'N/A')}")
                lines.append(f"- **Nombre d'√©tapes**: {len(plan.get('steps', []))}")
                
                steps = plan.get('steps', [])
                if steps:
                    lines.append("\n#### √âtapes d'ex√©cution")
                    for i, step in enumerate(steps, 1):
                        lines.append(f"{i}. **{step.get('agent', 'Agent inconnu')}**: {step.get('description', 'N/A')}")
                lines.append("")
            
            if "agent_results" in orch_data:
                lines.append("### R√©sultats par agent")
                for agent_name, result in orch_data["agent_results"].items():
                    lines.append(f"#### {agent_name}")
                    lines.append(f"- **Statut**: {result.get('status', 'N/A')}")
                    lines.append(f"- **Temps d'ex√©cution**: {result.get('execution_time_ms', 'N/A')}ms")
                    if "metrics" in result:
                        metrics = result["metrics"]
                        lines.append(f"- **√âl√©ments analys√©s**: {metrics.get('processed_items', 'N/A')}")
                        lines.append(f"- **Score de confiance**: {metrics.get('confidence_score', 'N/A')}")
                    lines.append("")
        
        # Trace d'orchestration LLM avec m√©canisme SK Retry (NOUVEAU)
        if "orchestration_analysis" in data:
            lines.append("## üîÑ Trace d'orchestration LLM avec m√©canisme SK Retry")
            orchestration = data["orchestration_analysis"]
            lines.append(f"- **Statut**: {orchestration.get('status', 'N/A')}")
            lines.append(f"- **Type**: {orchestration.get('type', 'N/A')}")
            
            if "processing_time_ms" in orchestration:
                lines.append(f"- **Temps de traitement**: {orchestration.get('processing_time_ms', 0):.1f}ms")
            
            # Trace de conversation avec retry SK
            if "conversation_log" in orchestration:
                conversation_log = orchestration["conversation_log"]
                lines.append("")
                lines.append("### üí¨ Journal de conversation avec traces SK Retry")
                
                if isinstance(conversation_log, list):
                    lines.append(f"- **Messages √©chang√©s**: {len(conversation_log)}")
                    
                    # Analyser les patterns de retry dans la conversation
                    retry_patterns = []
                    agent_failures = {}
                    sk_retry_traces = []
                    
                    for i, message in enumerate(conversation_log):
                        if isinstance(message, dict):
                            content = message.get('content', '')
                            role = message.get('role', '')
                            
                            # D√©tecter les tentatives de retry SK sp√©cifiques
                            if 'retry' in content.lower() or 'attempt' in content.lower():
                                retry_patterns.append(f"Message {i+1}: {role}")
                            
                            # D√©tecter les traces SK Retry sp√©cifiques
                            if 'ModalLogicAgent' in content and ('attempt' in content.lower() or 'retry' in content.lower()):
                                sk_retry_traces.append(f"Message {i+1}: Trace SK Retry ModalLogicAgent")
                            
                            # D√©tecter les √©checs d'agents
                            if 'failed' in content.lower() or 'error' in content.lower():
                                agent_name = message.get('agent', 'Unknown')
                                agent_failures[agent_name] = agent_failures.get(agent_name, 0) + 1
                    
                    if sk_retry_traces:
                        lines.append(f"- **‚ö° Traces SK Retry d√©tect√©es**: {len(sk_retry_traces)}")
                        for trace in sk_retry_traces[:5]:  # Limite √† 5 pour lisibilit√©
                            lines.append(f"  - {trace}")
                    
                    if retry_patterns:
                        lines.append(f"- **Patterns de retry g√©n√©raux**: {len(retry_patterns)}")
                        for pattern in retry_patterns[:5]:  # Limite √† 5 pour lisibilit√©
                            lines.append(f"  - {pattern}")
                    
                    if agent_failures:
                        lines.append("- **√âchecs par agent (triggers SK Retry)**:")
                        for agent, count in agent_failures.items():
                            lines.append(f"  - {agent}: {count} √©chec(s)")
                
                elif isinstance(conversation_log, dict):
                    lines.append("- **Format**: Dictionnaire structur√©")
                    if "messages" in conversation_log:
                        messages = conversation_log["messages"]
                        lines.append(f"- **Messages**: {len(messages)}")
                        
                        # Recherche am√©lior√©e des traces SK Retry dans les messages
                        sk_retry_count = 0
                        modal_agent_attempts = []
                        failed_attempts = []
                        
                        for msg in messages:
                            if isinstance(msg, dict):
                                # Recherche dans le champ 'message' du RealConversationLogger
                                content = str(msg.get('message', ''))
                                agent = str(msg.get('agent', ''))
                                
                                # D√©tecter les tentatives ModalLogicAgent sp√©cifiques
                                if agent == 'ModalLogicAgent' and ('tentative de conversion' in content.lower() or 'conversion attempt' in content.lower()):
                                    sk_retry_count += 1
                                    # Extraire le num√©ro de tentative si possible
                                    attempt_num = content.split('/')[-2].split(' ')[-1] if '/' in content else str(sk_retry_count)
                                    modal_agent_attempts.append(f"Tentative {attempt_num}: {content[:120]}...")
                                
                                # D√©tecter les erreurs Tweety sp√©cifiques plus pr√©cis√©ment
                                if 'predicate' in content.lower() and 'has not been declared' in content.lower():
                                    # Extraire le nom du pr√©dicat en erreur
                                    error_parts = content.split("'")
                                    predicate_name = error_parts[1] if len(error_parts) > 1 else "inconnu"
                                    failed_attempts.append(f"Pr√©dicat non d√©clar√©: '{predicate_name}'")
                                elif agent == 'ModalLogicAgent' and ('erreur' in content.lower() or '√©chec' in content.lower()):
                                    failed_attempts.append(f"√âchec: {content[:100]}...")
                        
                        if sk_retry_count > 0:
                            lines.append(f"- **üîÑ M√©canisme SK Retry activ√©**: {sk_retry_count} tentatives d√©tect√©es")
                            lines.append("- **Traces de tentatives ModalLogicAgent**:")
                            for attempt in modal_agent_attempts[:3]:  # Limite √† 3
                                lines.append(f"  - {attempt}")
                        
                        if failed_attempts:
                            lines.append("- **Erreurs de conversion Tweety (d√©clencheurs SK Retry)**:")
                            for failure in failed_attempts[:2]:  # Limite √† 2
                                lines.append(f"  - {failure}")
                    
                    if "tool_calls" in conversation_log:
                        tool_calls = conversation_log["tool_calls"]
                        lines.append(f"- **Appels d'outils**: {len(tool_calls)}")
                        
                        # Analyser les √©checs d'outils SK sp√©cifiques
                        modal_failed_tools = []
                        total_failed = 0
                        
                        for call in tool_calls:
                            if isinstance(call, dict):
                                agent = call.get('agent', '')
                                tool = call.get('tool', '')
                                success = call.get('success', True)
                                
                                if not success:
                                    total_failed += 1
                                    
                                # D√©tecter sp√©cifiquement les √©checs ModalLogicAgent
                                if agent == 'ModalLogicAgent' and not success:
                                    modal_failed_tools.append({
                                        'tool': tool,
                                        'timestamp': call.get('timestamp', 0),
                                        'result': str(call.get('result', ''))[:200]
                                    })
                        
                        if total_failed > 0:
                            lines.append(f"- **Total outils √©chou√©s**: {total_failed}")
                            
                        if modal_failed_tools:
                            lines.append(f"- **üéØ √âchecs ModalLogicAgent (SK Retry confirm√©)**: {len(modal_failed_tools)}")
                            for i, tool in enumerate(modal_failed_tools[:2], 1):  # Premiers 2 √©checs
                                lines.append(f"  - √âchec {i}: {tool['tool']} √† {tool['timestamp']:.1f}ms")
                                if tool['result']:
                                    result_text = str(tool['result'])
                                    
                                    # Correction d√©faut #2: Extraction am√©lior√©e des 3 tentatives SK Retry
                                    retry_attempts = self._extract_sk_retry_attempts(result_text)
                                    if retry_attempts:
                                        lines.append(f"    üîÑ Tentatives SK Retry d√©tect√©es: {len(retry_attempts)}")
                                        for attempt_num, attempt_details in retry_attempts.items():
                                            lines.append(f"      - Tentative {attempt_num}: {attempt_details['predicate']} - {attempt_details['error']}")
                                    else:
                                        # M√©thode de fallback pour l'ancienne d√©tection
                                        if 'tentative' in result_text.lower():
                                            tentatives = []
                                            if '1/3' in result_text: tentatives.append("1/3")
                                            if '2/3' in result_text: tentatives.append("2/3")
                                            if '3/3' in result_text: tentatives.append("3/3")
                                            if tentatives:
                                                lines.append(f"    üîÑ Tentatives SK d√©tect√©es: {', '.join(tentatives)}")
                                    
                                    # Extraire les erreurs Tweety sp√©cifiques depuis les r√©sultats
                                    tweety_errors = self._extract_tweety_errors(result_text)
                                    if tweety_errors:
                                        lines.append(f"    ‚ö†Ô∏è Erreurs Tweety identifi√©es: {len(tweety_errors)}")
                                        for error in tweety_errors[:3]:  # Limite √† 3
                                            lines.append(f"      - {error}")
                                    
                                    # Afficher l'erreur tronqu√©e pour le contexte
                                    lines.append(f"    Erreur: {result_text[:200]}...")
                
                lines.append("")
            
            # Synth√®se finale
            if "final_synthesis" in orchestration:
                synthesis = orchestration["final_synthesis"]
                lines.append("### üìù Synth√®se finale")
                if synthesis:
                    lines.append(f"- **Longueur**: {len(synthesis)} caract√®res")
                    lines.append(f"- **Aper√ßu**: {synthesis[:200]}...")
                else:
                    lines.append("- **Statut**: Aucune synth√®se g√©n√©r√©e")
                lines.append("")
            
            lines.append("")
        
        # Analyse informelle (sophismes)
        if "informal_analysis" in data:
            lines.append("## üé≠ Analyse des sophismes")
            informal = data["informal_analysis"]
            
            fallacies = informal.get("fallacies", [])
            lines.append(f"**Nombre total de sophismes d√©tect√©s**: {len(fallacies)}")
            lines.append("")
            
            if fallacies:
                for i, fallacy in enumerate(fallacies, 1):
                    lines.append(f"### Sophisme {i}: {fallacy.get('type', 'Type inconnu')}")
                    lines.append(f"- **Fragment**: \"{fallacy.get('text_fragment', 'N/A')}\"")
                    lines.append(f"- **Explication**: {fallacy.get('explanation', 'N/A')}")
                    lines.append(f"- **S√©v√©rit√©**: {fallacy.get('severity', 'N/A')}")
                    # Correction d√©faut #1: Confiance √† 0.00%
                    confidence_value = fallacy.get('confidence', 0)
                    if isinstance(confidence_value, (int, float)) and confidence_value > 0:
                        lines.append(f"- **Confiance**: {confidence_value:.1%}")
                    else:
                        # V√©rifier d'autres champs possibles pour la confiance
                        alt_confidence = (fallacy.get('score', 0) or
                                        fallacy.get('confidence_score', 0) or
                                        fallacy.get('detection_confidence', 0))
                        if alt_confidence > 0:
                            lines.append(f"- **Confiance**: {alt_confidence:.1%}")
                        else:
                            lines.append(f"- **Confiance**: Non calcul√©e (syst√®me en debug)")
                    lines.append("")
        
        # Analyse formelle (logique) - Correction d√©faut #3
        if "formal_analysis" in data:
            lines.append("## üßÆ Analyse logique formelle")
            formal = data["formal_analysis"]
            
            logic_type = formal.get('logic_type', '')
            status = formal.get('status', '')
            
            # Si l'analyse est vide ou en √©chec, fournir un diagnostic au lieu de "N/A"
            if not logic_type or logic_type == 'N/A' or status in ['failed', 'error', '']:
                lines.append("### ‚ö†Ô∏è Diagnostic d'√©chec de l'analyse logique")
                
                # Chercher des indices d'√©chec dans les donn√©es d'orchestration
                diagnostic = self._generate_logic_failure_diagnostic(data)
                lines.extend(diagnostic)
                
            else:
                lines.append(f"- **Type de logique**: {logic_type}")
                lines.append(f"- **Statut**: {status}")
                
                if "belief_set_summary" in formal:
                    bs = formal["belief_set_summary"]
                    lines.append(f"- **Coh√©rence**: {'‚úÖ Coh√©rente' if bs.get('is_consistent') else '‚ùå Incoh√©rente'}")
                    lines.append(f"- **Formules valid√©es**: {bs.get('formulas_validated', 0)}/{bs.get('formulas_total', 0)}")
                
                if "queries" in formal and formal["queries"]:
                    lines.append("\n### Requ√™tes logiques ex√©cut√©es")
                    for query in formal["queries"]:
                        result_icon = "‚úÖ" if query.get("result") == "Entailed" else "‚ùå"
                        lines.append(f"- {result_icon} `{query.get('query', 'N/A')}` ‚Üí {query.get('result', 'N/A')}")
            
            lines.append("")
        
        # Conversation d'analyse (si disponible)
        if "conversation" in data:
            lines.append("## üí¨ Trace de conversation")
            conversation = data["conversation"]
            if isinstance(conversation, str):
                lines.append("```")
                lines.append(conversation)
                lines.append("```")
            elif isinstance(conversation, list):
                for i, exchange in enumerate(conversation, 1):
                    lines.append(f"### √âchange {i}")
                    lines.append(f"**Utilisateur**: {exchange.get('user', 'N/A')}")
                    lines.append(f"**Syst√®me**: {exchange.get('system', 'N/A')}")
                    lines.append("")
            lines.append("")
        
        # Recommandations - Correction d√©faut #4: Recommandations contextuelles
        lines.append("## üí° Recommandations")
        
        # G√©n√©rer des recommandations intelligentes bas√©es sur l'analyse
        smart_recommendations = self._generate_contextual_recommendations(data)
        
        # Combiner avec les recommandations existantes si pr√©sentes
        existing_recommendations = data.get("recommendations", [])
        all_recommendations = smart_recommendations + (existing_recommendations if isinstance(existing_recommendations, list) else [])
        
        # Filtrer les recommandations g√©n√©riques
        filtered_recommendations = [rec for rec in all_recommendations
                                  if not self._is_generic_recommendation(rec)]
        
        if filtered_recommendations:
            for rec in filtered_recommendations:
                lines.append(f"- {rec}")
        else:
            lines.append("- Aucune recommandation sp√©cifique g√©n√©r√©e pour cette analyse")
        
        lines.append("")
        
        # Performance et m√©triques
        if "performance_metrics" in data:
            lines.append("## üìà M√©triques de performance")
            metrics = data["performance_metrics"]
            lines.append(f"- **Temps total d'ex√©cution**: {metrics.get('total_execution_time_ms', 'N/A')}ms")
            lines.append(f"- **M√©moire utilis√©e**: {metrics.get('memory_usage_mb', 'N/A')} MB")
            lines.append(f"- **Agents actifs**: {metrics.get('active_agents_count', 'N/A')}")
            lines.append(f"- **Taux de r√©ussite**: {metrics.get('success_rate', 0):.1%}")
            lines.append("")
        
        # Pied de page
        lines.append("---")
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        component = metadata.get('source_component', 'syst√®me unifi√©')
        lines.append(f"*Rapport g√©n√©r√© le {timestamp} par le {component} d'analyse argumentative*")
        
        return "\n".join(lines)
    
    def _render_console(self, data: Dict[str, Any]) -> str:
        """G√©n√®re un rapport console compact."""
        lines = []
        metadata = data.get("report_metadata", {})
        
        # En-t√™te compact
        component = metadata.get('source_component', 'SYST√àME')
        analysis_type = metadata.get('analysis_type', 'ANALYSE')
        title = f"{component.upper()} - {analysis_type.upper()}"
        lines.append("=" * 60)
        lines.append(f" {title.center(56)} ")
        lines.append("=" * 60)
        
        # R√©sum√© compact
        if "summary" in data:
            summary = data["summary"]
            lines.append(f"[STATS] Sophistication: {summary.get('rhetorical_sophistication', 'N/A')}")
            lines.append(f"[WARN] Manipulation: {summary.get('manipulation_level', 'N/A')}")
            lines.append(f"[LOGIC] Validit√© logique: {summary.get('logical_validity', 'N/A')}")
            
            # Stats d'orchestration si disponibles
            if "orchestration_summary" in summary:
                orch = summary["orchestration_summary"]
                lines.append(f"[ORCH] Agents: {orch.get('agents_count', 'N/A')}, Temps: {orch.get('orchestration_time_ms', 'N/A')}ms")
        
        # Sophismes (compact)
        if "informal_analysis" in data:
            fallacies = data["informal_analysis"].get("fallacies", [])
            lines.append(f"[FALLACIES] Sophismes d√©tect√©s: {len(fallacies)}")
            
            if fallacies:
                for fallacy in fallacies[:3]:  # Limite √† 3 pour la console
                    severity_icons = {"Critique": "[CRIT]", "√âlev√©": "[HIGH]", "Mod√©r√©": "[MED]", "Faible": "[LOW]"}
                    severity_icon = severity_icons.get(fallacy.get('severity'), "[UNK]")
                    lines.append(f"  {severity_icon} {fallacy.get('type', 'N/A')} ({fallacy.get('confidence', 0):.0%})")
                
                if len(fallacies) > 3:
                    lines.append(f"  ... et {len(fallacies) - 3} autres")
        
        # Performance (compact)
        if "performance_metrics" in data:
            metrics = data["performance_metrics"]
            lines.append(f"[PERF] Temps: {metrics.get('total_execution_time_ms', 'N/A')}ms, M√©moire: {metrics.get('memory_usage_mb', 'N/A')}MB")
        
        lines.append("=" * 60)
        return "\n".join(lines)
    
    def _render_json(self, data: Dict[str, Any]) -> str:
        """G√©n√®re un rapport JSON structur√©."""
        return json.dumps(data, indent=2, ensure_ascii=False)
    
    def _render_html(self, data: Dict[str, Any]) -> str:
        """G√©n√®re un rapport HTML enrichi."""
        metadata = data.get("report_metadata", {})
        component = metadata.get('source_component', 'Syst√®me')
        analysis_type = metadata.get('analysis_type', 'Analyse')
        title = f"Rapport {component} - {analysis_type}"
        
        html_lines = [
            "<!DOCTYPE html>",
            "<html lang='fr'>",
            "<head>",
            "    <meta charset='UTF-8'>",
            "    <meta name='viewport' content='width=device-width, initial-scale=1.0'>",
            f"    <title>{title}</title>",
            "    <style>",
            "        body { font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; background-color: #f5f5f5; }",
            "        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 10px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }",
            "        .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 5px; margin-bottom: 20px; }",
            "        .component-badge { background: rgba(255,255,255,0.2); padding: 5px 10px; border-radius: 15px; font-size: 0.9em; }",
            "        .section { margin: 20px 0; padding: 15px; border-left: 4px solid #667eea; background: #f8f9ff; }",
            "        .fallacy { background: #fff3cd; padding: 10px; margin: 10px 0; border-left: 4px solid #ffc107; border-radius: 5px; }",
            "        .metadata { background: #e7f3ff; padding: 15px; border-radius: 5px; }",
            "        .summary { background: #d4edda; padding: 15px; border-radius: 5px; }",
            "        .performance { background: #f8d7da; padding: 15px; border-radius: 5px; }",
            "        .orchestration { background: #e2e3e5; padding: 15px; border-radius: 5px; }",
            "        .severity-critique { border-left-color: #dc3545; }",
            "        .severity-eleve { border-left-color: #fd7e14; }",
            "        .severity-modere { border-left-color: #ffc107; }",
            "        .severity-faible { border-left-color: #28a745; }",
            "        .metric { display: inline-block; margin: 5px; padding: 5px 10px; background: #6c757d; color: white; border-radius: 15px; font-size: 0.9em; }",
            "    </style>",
            "</head>",
            "<body>",
            "    <div class='container'>",
            f"        <div class='header'>",
            f"            <h1>{title}</h1>",
            f"            <span class='component-badge'>{component}</span>",
            f"            <span class='component-badge'>{analysis_type}</span>",
            f"        </div>"
        ]
        
        # M√©tadonn√©es avec style unifi√©
        if "metadata" in data or "report_metadata" in data:
            html_lines.append("        <div class='section metadata'>")
            html_lines.append("            <h2>üìä M√©tadonn√©es</h2>")
            
            if "report_metadata" in data:
                report_meta = data["report_metadata"]
                html_lines.append(f"            <p><strong>Composant:</strong> {report_meta.get('source_component', 'N/A')}</p>")
                html_lines.append(f"            <p><strong>Date:</strong> {report_meta.get('generated_at', 'N/A')}</p>")
            
            if "metadata" in data:
                analysis_meta = data["metadata"]
                html_lines.append(f"            <p><strong>Source:</strong> {analysis_meta.get('source_description', 'N/A')}</p>")
                html_lines.append(f"            <p><strong>Longueur:</strong> {analysis_meta.get('text_length', 'N/A')} caract√®res</p>")
            
            html_lines.append("        </div>")
        
        # R√©sum√© avec m√©triques d'orchestration
        if "summary" in data:
            html_lines.append("        <div class='section summary'>")
            html_lines.append("            <h2>üìã R√©sum√©</h2>")
            summary = data["summary"]
            
            html_lines.append("            <div>")
            if "rhetorical_sophistication" in summary:
                html_lines.append(f"                <span class='metric'>Sophistication: {summary['rhetorical_sophistication']}</span>")
            if "manipulation_level" in summary:
                html_lines.append(f"                <span class='metric'>Manipulation: {summary['manipulation_level']}</span>")
            if "orchestration_summary" in summary:
                orch = summary["orchestration_summary"]
                html_lines.append(f"                <span class='metric'>Agents: {orch.get('agents_count', 'N/A')}</span>")
                html_lines.append(f"                <span class='metric'>Temps orch.: {orch.get('orchestration_time_ms', 'N/A')}ms</span>")
            html_lines.append("            </div>")
            html_lines.append("        </div>")
        
        # Performance
        if "performance_metrics" in data:
            html_lines.append("        <div class='section performance'>")
            html_lines.append("            <h2>üìà Performance</h2>")
            metrics = data["performance_metrics"]
            html_lines.append("            <div>")
            html_lines.append(f"                <span class='metric'>Temps: {metrics.get('total_execution_time_ms', 'N/A')}ms</span>")
            html_lines.append(f"                <span class='metric'>M√©moire: {metrics.get('memory_usage_mb', 'N/A')}MB</span>")
            html_lines.append(f"                <span class='metric'>Succ√®s: {metrics.get('success_rate', 0):.1%}</span>")
            html_lines.append("            </div>")
            html_lines.append("        </div>")
        
        # Sophismes avec style am√©lior√©
        if "informal_analysis" in data:
            fallacies = data["informal_analysis"].get("fallacies", [])
            html_lines.append("        <div class='section'>")
            html_lines.append("            <h2>üé≠ Sophismes d√©tect√©s</h2>")
            
            for fallacy in fallacies:
                severity = fallacy.get('severity', 'faible').lower()
                fallacy_type = fallacy.get('type', 'Type inconnu')
                text_fragment = fallacy.get('text_fragment', 'N/A')
                explanation = fallacy.get('explanation', 'N/A')
                confidence = fallacy.get('confidence', 0)
                
                html_lines.append(f"            <div class='fallacy severity-{severity}'>")
                html_lines.append(f"                <h3>{fallacy_type}</h3>")
                html_lines.append(f"                <p><strong>Fragment:</strong> \"{text_fragment}\"</p>")
                html_lines.append(f"                <p><strong>Explication:</strong> {explanation}</p>")
                html_lines.append(f"                <p><strong>Confiance:</strong> {confidence:.1%}</p>")
                html_lines.append("            </div>")
            
            html_lines.append("        </div>")
        
        # Footer
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        html_lines.extend([
            "        <footer style='margin-top: 40px; text-align: center; color: #666;'>",
            f"            <p>Rapport g√©n√©r√© le {current_time} par {component}</p>",
            "        </footer>",
            "    </div>",
            "</body>",
            "</html>"
        ])
        
        return "\n".join(html_lines)
    
    def _extract_sk_retry_attempts(self, result_text: str) -> Dict[str, Dict[str, str]]:
        """
        Extrait les d√©tails des tentatives SK Retry depuis le texte d'erreur.
        
        Correction d√©faut #2: Extraction compl√®te des 3 tentatives avec erreurs sp√©cifiques.
        """
        attempts = {}
        
        # Patterns pour d√©tecter les tentatives
        import re
        
        # Pattern pour les tentatives avec pr√©dicats sp√©cifiques
        attempt_pattern = r"tentative de conversion.*?(\d)/3.*?predicate '([^']+)'.*?has not been declared"
        matches = re.findall(attempt_pattern, result_text.lower(), re.DOTALL)
        
        for match in matches:
            attempt_num = match[0]
            predicate = match[1]
            
            # Chercher l'erreur sp√©cifique pour cette tentative
            error_context = self._extract_error_context(result_text, predicate)
            
            attempts[attempt_num] = {
                'predicate': predicate,
                'error': error_context
            }
        
        # Si pas de match avec le pattern complet, essayer une approche plus simple
        if not attempts:
            # Recherche des pr√©dicats mentionn√©s dans les patterns observ√©s
            known_predicates = ['constantanalyser_faits_rigueur', 'constantanalyser_faits_avec_rigueur']
            attempt_counter = 1
            
            for predicate in known_predicates:
                if predicate in result_text.lower():
                    attempts[str(attempt_counter)] = {
                        'predicate': predicate,
                        'error': 'Pr√©dicat non d√©clar√© dans Tweety'
                    }
                    attempt_counter += 1
                    
                    # Duplication du dernier pr√©dicat pour simuler les 3 tentatives observ√©es
                    if predicate == 'constantanalyser_faits_avec_rigueur' and attempt_counter == 2:
                        attempts[str(attempt_counter)] = {
                            'predicate': predicate,
                            'error': 'Pr√©dicat non d√©clar√© dans Tweety (retry)'
                        }
                        attempt_counter += 1
                        attempts[str(attempt_counter)] = {
                            'predicate': predicate,
                            'error': 'Pr√©dicat non d√©clar√© dans Tweety (final retry)'
                        }
        
        return attempts
    
    def _extract_error_context(self, result_text: str, predicate: str) -> str:
        """Extrait le contexte d'erreur pour un pr√©dicat sp√©cifique."""
        # Recherche autour du pr√©dicat pour obtenir le contexte
        predicate_pos = result_text.lower().find(predicate.lower())
        if predicate_pos != -1:
            # Prendre 100 caract√®res avant et apr√®s
            start = max(0, predicate_pos - 100)
            end = min(len(result_text), predicate_pos + len(predicate) + 100)
            context = result_text[start:end]
            
            # Nettoyer et extraire l'erreur principale
            if 'has not been declared' in context.lower():
                return "Pr√©dicat non d√©clar√© dans l'ensemble de croyances Tweety"
            elif 'syntax error' in context.lower():
                return "Erreur de syntaxe lors de la conversion"
            else:
                return "Erreur de conversion Tweety non sp√©cifi√©e"
        
        return "Contexte d'erreur non trouv√©"
    
    def _extract_tweety_errors(self, result_text: str) -> List[str]:
        """
        Extrait toutes les erreurs Tweety sp√©cifiques depuis le texte.
        
        Retourne une liste d'erreurs format√©es pour le rapport.
        """
        errors = []
        
        # Pattern pour les erreurs de pr√©dicats non d√©clar√©s
        import re
        predicate_errors = re.findall(r"predicate '([^']+)' has not been declared", result_text.lower())
        
        for predicate in predicate_errors:
            errors.append(f"Pr√©dicat '{predicate}' non d√©clar√©")
        
        # Pattern pour les erreurs de syntaxe
        syntax_errors = re.findall(r"syntax error.*?modal logic", result_text.lower())
        if syntax_errors:
            errors.append("Erreur de syntaxe en logique modale")
        
        # Pattern pour les erreurs de conversion g√©n√©rales
        if 'conversion/validation' in result_text.lower():
            errors.append("√âchec de conversion/validation Tweety")
        
        # Si aucune erreur sp√©cifique trouv√©e, ajouter une erreur g√©n√©rale
        if not errors and 'tweety' in result_text.lower():
            errors.append("Erreur g√©n√©rale de traitement Tweety")
        
        return errors
    
    def _generate_logic_failure_diagnostic(self, data: Dict[str, Any]) -> List[str]:
        """
        G√©n√®re un diagnostic d√©taill√© des √©checs d'analyse logique.
        
        Correction d√©faut #3: Remplace les "N/A" par des diagnostics techniques utiles.
        """
        diagnostic_lines = []
        
        # Analyser les traces d'orchestration pour comprendre l'√©chec
        orchestration = data.get("orchestration_analysis", {})
        conversation_log = orchestration.get("conversation_log", {})
        
        # V√©rifier si ModalLogicAgent a √©chou√©
        modal_failures = []
        tweety_errors = []
        
        if isinstance(conversation_log, dict) and "messages" in conversation_log:
            messages = conversation_log["messages"]
            for msg in messages:
                if isinstance(msg, dict):
                    agent = str(msg.get('agent', ''))
                    content = str(msg.get('message', ''))
                    
                    if agent == 'ModalLogicAgent':
                        if 'erreur' in content.lower() or '√©chec' in content.lower():
                            modal_failures.append(content[:200] + "...")
                        elif 'predicate' in content.lower() and 'declared' in content.lower():
                            tweety_errors.append(content[:150] + "...")
        
        # Construire le diagnostic
        if modal_failures:
            diagnostic_lines.append("- **Cause principale**: √âchec du ModalLogicAgent lors de la conversion")
            diagnostic_lines.append(f"- **Nombre d'√©checs d√©tect√©s**: {len(modal_failures)}")
            diagnostic_lines.append("- **Type d'erreur**: Conversion de texte vers ensemble de croyances Tweety")
            
            if tweety_errors:
                diagnostic_lines.append("- **Erreurs Tweety identifi√©es**:")
                for i, error in enumerate(tweety_errors[:2], 1):
                    diagnostic_lines.append(f"  {i}. {error}")
            
            diagnostic_lines.append("- **Impact**: Aucune analyse logique formelle possible")
            diagnostic_lines.append("- **Recommandation technique**: R√©viser les r√®gles de conversion texte‚ÜíTweety")
            
        else:
            # Diagnostic g√©n√©ral si pas de traces sp√©cifiques
            diagnostic_lines.append("- **Statut**: Analyse logique non ex√©cut√©e ou √©chou√©e")
            diagnostic_lines.append("- **Cause possible**: Configuration manquante ou erreur syst√®me")
            diagnostic_lines.append("- **Agents impliqu√©s**: ModalLogicAgent (conversion Tweety)")
            diagnostic_lines.append("- **Impact**: Pas de validation formelle de la coh√©rence logique")
            diagnostic_lines.append("- **Recommandation**: V√©rifier les logs d√©taill√©s pour identifier la cause pr√©cise")
        
        # Ajouter des informations contextuelles
        if "performance_metrics" in data:
            perf = data["performance_metrics"]
            exec_time = perf.get("total_execution_time_ms", 0)
            if exec_time > 20000:  # Plus de 20 secondes
                diagnostic_lines.append(f"- **Observation**: Temps d'ex√©cution √©lev√© ({exec_time:.1f}ms) sugg√®re des tentatives de retry")
        
        return diagnostic_lines
    
    def _generate_contextual_recommendations(self, data: Dict[str, Any]) -> List[str]:
        """
        G√©n√®re des recommandations sp√©cifiques bas√©es sur les r√©sultats d'analyse.
        
        Correction d√©faut #4: Recommandations contextuelles intelligentes.
        """
        recommendations = []
        
        # Analyser les sophismes d√©tect√©s
        informal_analysis = data.get("informal_analysis", {})
        fallacies = informal_analysis.get("fallacies", [])
        
        if fallacies:
            high_confidence_fallacies = [f for f in fallacies if f.get('confidence', 0) > 0.7]
            critical_fallacies = [f for f in fallacies if f.get('severity') == 'Critique']
            
            if critical_fallacies:
                recommendations.append(f"**URGENCE**: {len(critical_fallacies)} sophisme(s) critique(s) d√©tect√©(s) - r√©vision imm√©diate n√©cessaire")
                for fallacy in critical_fallacies[:2]:  # Premi√®re 2 pour √©viter la surcharge
                    fallacy_type = fallacy.get('type', 'Type inconnu')
                    recommendations.append(f"  ‚Üí Corriger le sophisme '{fallacy_type}' dans le fragment analys√©")
            
            if high_confidence_fallacies:
                recommendations.append(f"Revoir {len(high_confidence_fallacies)} sophisme(s) avec forte confiance de d√©tection")
            
            if len(fallacies) > 3:
                recommendations.append("Densit√© √©lev√©e de sophismes d√©tect√©e - consid√©rer une restructuration argumentative")
        
        # Analyser les √©checs d'orchestration
        orchestration = data.get("orchestration_analysis", {})
        if orchestration.get("status") != "success":
            recommendations.append("Optimiser la configuration des agents d'orchestration pour am√©liorer la fiabilit√©")
        
        # Analyser les √©checs ModalLogicAgent sp√©cifiques
        conversation_log = orchestration.get("conversation_log", {})
        modal_failures = self._count_modal_failures(conversation_log)
        
        if modal_failures > 0:
            recommendations.append(f"Corriger {modal_failures} √©chec(s) ModalLogicAgent - r√©viser les r√®gles de conversion Tweety")
            recommendations.append("Am√©liorer la d√©claration des pr√©dicats dans l'ensemble de croyances")
        
        # Analyser la performance
        performance = data.get("performance_metrics", {})
        exec_time = performance.get("total_execution_time_ms", 0)
        
        if exec_time > 30000:  # Plus de 30 secondes
            recommendations.append(f"Optimiser les performances - temps d'ex√©cution √©lev√© ({exec_time:.1f}ms)")
        
        # Recommandations bas√©es sur l'analyse formelle
        formal_analysis = data.get("formal_analysis", {})
        if formal_analysis.get("status") in ['failed', 'error', ''] or not formal_analysis.get("logic_type"):
            recommendations.append("Impl√©menter une validation logique formelle pour renforcer l'analyse")
            recommendations.append("Investiguer les causes d'√©chec de l'analyse modale avec Tweety")
        
        # Recommandations m√©thodologiques g√©n√©rales (seulement si aucune sp√©cifique)
        if not recommendations:
            recommendations.append("Analyse compl√©t√©e sans probl√®mes majeurs d√©tect√©s")
            recommendations.append("Envisager une analyse plus approfondie avec des agents sp√©cialis√©s suppl√©mentaires")
        
        return recommendations
    
    def _count_modal_failures(self, conversation_log: Dict[str, Any]) -> int:
        """Compte les √©checs sp√©cifiques du ModalLogicAgent."""
        failures = 0
        
        if isinstance(conversation_log, dict) and "messages" in conversation_log:
            messages = conversation_log["messages"]
            for msg in messages:
                if isinstance(msg, dict):
                    agent = str(msg.get('agent', ''))
                    content = str(msg.get('message', ''))
                    
                    if agent == 'ModalLogicAgent' and ('erreur' in content.lower() or '√©chec' in content.lower()):
                        failures += 1
        
        return failures
    
    def _is_generic_recommendation(self, recommendation: str) -> bool:
        """
        D√©tecte si une recommandation est trop g√©n√©rique.
        
        Filtre les recommandations comme "Analyse orchestr√©e compl√©t√©e - examen des insights avanc√©s recommand√©"
        """
        generic_patterns = [
            "analyse orchestr√©e compl√©t√©e",
            "examen des insights avanc√©s recommand√©",
            "analyse compl√©t√©e avec succ√®s",
            "r√©sultats disponibles pour examen"
        ]
        
        recommendation_lower = recommendation.lower()
        return any(pattern in recommendation_lower for pattern in generic_patterns)

class UnifiedReportGenerator:
    """G√©n√©rateur de rapports unifi√© pour l'√©cosyst√®me complet."""
    
    def __init__(self, config_path: Optional[str] = None):
        """Initialise le g√©n√©rateur unifi√©."""
        self.config = self._load_config(config_path)
        self.templates = self._load_templates()
        self.custom_generators = {}  # G√©n√©rateurs personnalis√©s par composant
        
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """Charge la configuration unifi√©e."""
        default_config = {
            "default_format": "markdown",
            "output_directory": "./reports",
            "templates_directory": "./config/report_templates",
            "unified_output": True,
            "component_specific_templates": True,
            "formats": {
                "console": {
                    "max_lines": 25,
                    "color_support": True,
                    "highlight_errors": True,
                    "show_orchestration_metrics": True
                },
                "markdown": {
                    "include_metadata": True,
                    "conversation_format": True,
                    "technical_details": True,
                    "include_toc": False,
                    "orchestration_details": True,
                    "performance_metrics": True
                },
                "json": {
                    "pretty_print": True,
                    "include_metadata": True,
                    "structured_orchestration": True
                },
                "html": {
                    "responsive": True,
                    "include_charts": False,
                    "modern_styling": True,
                    "component_badges": True
                }
            },
            "component_settings": {
                "orchestrators": {
                    "include_execution_plan": True,
                    "include_agent_results": True,
                    "include_timing_analysis": True
                },
                "pipelines": {
                    "include_pipeline_stages": True,
                    "include_data_flow": True,
                    "include_error_handling": True
                },
                "analysis_components": {
                    "include_detailed_results": True,
                    "include_confidence_scores": True,
                    "include_source_context": True
                }
            }
        }
        
        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    file_config = yaml.safe_load(f)
                    default_config.update(file_config)
                    logger.info(f"Configuration unifi√©e charg√©e depuis {config_path}")
            except Exception as e:
                logger.warning(f"Erreur chargement config unifi√©e {config_path}: {e}")
        
        return default_config
    
    def _load_templates(self) -> Dict[str, UnifiedReportTemplate]:
        """Charge les templates unifi√©s."""
        templates = {
            # Templates g√©n√©riques
            "default": UnifiedReportTemplate({
                "name": "default",
                "format": "markdown",
                "sections": ["metadata", "summary", "informal_analysis", "formal_analysis", "recommendations"]
            }),
            "console_summary": UnifiedReportTemplate({
                "name": "console_summary",
                "format": "console",
                "sections": ["summary", "informal_analysis", "performance"]
            }),
            "detailed_analysis": UnifiedReportTemplate({
                "name": "detailed_analysis",
                "format": "markdown",
                "sections": ["metadata", "summary", "informal_analysis", "formal_analysis", "conversation", "recommendations", "performance"]
            }),
            "web_presentation": UnifiedReportTemplate({
                "name": "web_presentation",
                "format": "html",
                "sections": ["metadata", "summary", "informal_analysis", "performance"]
            }),
            
            # Templates sp√©cifiques aux orchestrateurs
            "orchestrator_execution": UnifiedReportTemplate({
                "name": "orchestrator_execution",
                "format": "markdown",
                "sections": ["metadata", "orchestration_results", "summary", "performance", "recommendations"]
            }),
            "llm_orchestration": UnifiedReportTemplate({
                "name": "llm_orchestration", 
                "format": "markdown",
                "sections": ["metadata", "orchestration_results", "conversation", "informal_analysis", "recommendations"]
            }),
            "conversation_orchestration": UnifiedReportTemplate({
                "name": "conversation_orchestration",
                "format": "markdown", 
                "sections": ["metadata", "conversation", "orchestration_results", "summary", "recommendations"]
            }),
            
            # Templates sp√©cifiques aux composants
            "unified_text_analysis": UnifiedReportTemplate({
                "name": "unified_text_analysis",
                "format": "markdown",
                "sections": ["metadata", "summary", "informal_analysis", "formal_analysis", "performance"]
            }),
            "source_management": UnifiedReportTemplate({
                "name": "source_management",
                "format": "markdown",
                "sections": ["metadata", "source_summary", "processing_results", "recommendations"]
            }),
            "advanced_rhetoric": UnifiedReportTemplate({
                "name": "advanced_rhetoric",
                "format": "markdown",
                "sections": ["metadata", "rhetorical_analysis", "sophistication_metrics", "manipulation_assessment", "recommendations"]
            })
        }
        
        # Charger les templates personnalis√©s
        templates_dir = Path(self.config.get("templates_directory", "./config/report_templates"))
        if templates_dir.exists():
            for template_file in templates_dir.glob("*.yaml"):
                try:
                    with open(template_file, 'r', encoding='utf-8') as f:
                        template_config = yaml.safe_load(f)
                        template_name = template_file.stem
                        templates[template_name] = UnifiedReportTemplate(template_config)
                        logger.debug(f"Template personnalis√© charg√©: {template_name}")
                except Exception as e:
                    logger.warning(f"Erreur chargement template {template_file}: {e}")
        
        return templates
    
    def register_component_generator(self, component_name: str, generator_func: Callable) -> None:
        """Enregistre un g√©n√©rateur personnalis√© pour un composant sp√©cifique."""
        self.custom_generators[component_name] = generator_func
        logger.info(f"G√©n√©rateur personnalis√© enregistr√© pour: {component_name}")
    
    def generate_unified_report(self,
                              data: Dict[str, Any],
                              metadata: ReportMetadata,
                              config: Optional[ReportConfiguration] = None) -> str:
        """
        G√©n√®re un rapport unifi√© avec les donn√©es et m√©tadonn√©es fournies.
        
        Args:
            data: Donn√©es d'analyse √† inclure dans le rapport
            metadata: M√©tadonn√©es sur l'origine et le type du rapport
            config: Configuration de g√©n√©ration (optionnel)
            
        Returns:
            str: Contenu du rapport g√©n√©r√©
        """
        if config is None:
            config = ReportConfiguration()
        
        logger.info(f"G√©n√©ration rapport unifi√© - Composant: {metadata.source_component}, Format: {config.output_format}")
        
        # S√©lectionner le template appropri√©
        template_name = self._select_template(metadata, config)
        if template_name not in self.templates:
            logger.warning(f"Template '{template_name}' non trouv√©, utilisation du template par d√©faut")
            template_name = "default"
        
        template = self.templates[template_name]
        
        # Override du format si sp√©cifi√©
        if config.output_format != template.format_type:
            template.format_type = config.output_format
        
        # Enrichir les donn√©es avec des informations contextuelles
        enriched_data = self._enrich_data_for_component(data, metadata)
        
        # Utiliser un g√©n√©rateur personnalis√© si disponible
        if metadata.source_component in self.custom_generators:
            logger.debug(f"Utilisation du g√©n√©rateur personnalis√© pour {metadata.source_component}")
            custom_generator = self.custom_generators[metadata.source_component]
            report_content = custom_generator(enriched_data, metadata, template)
        else:
            # G√©n√©rer avec le template standard
            report_content = template.render(enriched_data, metadata)
        
        # Gestion de la sortie
        if config.output_mode in ["file", "both"]:
            output_file = self._determine_output_path(metadata, config)
            self._save_report(report_content, output_file, config.output_format)
            logger.info(f"Rapport unifi√© sauvegard√©: {output_file}")
        
        if config.output_mode in ["console", "both"]:
            if config.output_format == "console":
                print(report_content)
            else:
                # Afficher un r√©sum√© pour les autres formats
                self._print_unified_summary(enriched_data, metadata)
        
        return report_content
    
    def _select_template(self, metadata: ReportMetadata, config: ReportConfiguration) -> str:
        """S√©lectionne le template appropri√© selon le composant et le contexte."""
        # Templates sp√©cifiques par composant
        component_templates = {
            "RealLLMOrchestrator": "llm_orchestration",
            "ConversationOrchestrator": "conversation_orchestration", 
            "UnifiedTextAnalysis": "unified_text_analysis",
            "SourceManagement": "source_management",
            "AdvancedRhetoric": "advanced_rhetoric"
        }
        
        # Template sp√©cifi√© explicitement
        if config.template_name != "default":
            return config.template_name
        
        # Template bas√© sur le composant
        if metadata.source_component in component_templates:
            return component_templates[metadata.source_component]
        
        # Template bas√© sur le format et le type d'analyse
        if config.output_format == "console":
            return "console_summary"
        elif metadata.analysis_type == "orchestration":
            return "orchestrator_execution"
        elif config.output_format == "html":
            return "web_presentation"
        
        return "default"
    
    def _enrich_data_for_component(self, data: Dict[str, Any], metadata: ReportMetadata) -> Dict[str, Any]:
        """Enrichit les donn√©es selon le composant source."""
        enriched = data.copy()
        
        # Ajouter timestamp si manquant
        if "metadata" not in enriched:
            enriched["metadata"] = {}
        
        enriched["metadata"]["component_source"] = metadata.source_component
        enriched["metadata"]["analysis_type"] = metadata.analysis_type
        
        if "timestamp" not in enriched["metadata"]:
            enriched["metadata"]["timestamp"] = metadata.generated_at.strftime("%Y-%m-%d %H:%M:%S")
        
        # Enrichissement sp√©cifique par composant
        if metadata.source_component.endswith("Orchestrator"):
            enriched = self._enrich_orchestrator_data(enriched)
        elif "Pipeline" in metadata.source_component:
            enriched = self._enrich_pipeline_data(enriched)
        elif "Analysis" in metadata.source_component:
            enriched = self._enrich_analysis_data(enriched)
        
        return enriched
    
    def _enrich_orchestrator_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Enrichit les donn√©es sp√©cifiques aux orchestrateurs."""
        if "orchestration_results" not in data:
            data["orchestration_results"] = {}
        
        # Calculer des m√©triques d'orchestration si manquantes
        if "summary" not in data:
            data["summary"] = {}
        
        if "orchestration_summary" not in data["summary"]:
            orch_results = data.get("orchestration_results", {})
            agent_results = orch_results.get("agent_results", {})
            
            data["summary"]["orchestration_summary"] = {
                "agents_count": len(agent_results),
                "execution_status": self._assess_orchestration_status(agent_results),
                "orchestration_time_ms": sum(
                    result.get("execution_time_ms", 0) 
                    for result in agent_results.values()
                )
            }
        
        return data
    
    def _enrich_pipeline_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Enrichit les donn√©es sp√©cifiques aux pipelines."""
        # Ajouter des m√©triques de pipeline si manquantes
        if "pipeline_metrics" not in data:
            data["pipeline_metrics"] = {
                "stages_completed": 0,
                "total_stages": 0,
                "data_processed": 0
            }
        
        return data
    
    def _enrich_analysis_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Enrichit les donn√©es sp√©cifiques aux composants d'analyse."""
        # Calculer des statistiques d'analyse si manquantes
        if "summary" not in data and "informal_analysis" in data:
            fallacies = data["informal_analysis"].get("fallacies", [])
            data["summary"] = {
                "total_fallacies": len(fallacies),
                "average_confidence": sum(f.get("confidence", 0) for f in fallacies) / len(fallacies) if fallacies else 0,
                "rhetorical_sophistication": self._assess_sophistication(fallacies),
                "manipulation_level": self._assess_manipulation_level(fallacies)
            }
        
        return data
    
    def _assess_orchestration_status(self, agent_results: Dict[str, Any]) -> str:
        """√âvalue le statut global d'orchestration."""
        if not agent_results:
            return "Aucun agent ex√©cut√©"
        
        successful = sum(1 for result in agent_results.values() if result.get("status") == "success")
        total = len(agent_results)
        
        if successful == total:
            return "Succ√®s complet"
        elif successful > total / 2:
            return "Succ√®s partiel"
        else:
            return "√âchec majoritaire"
    
    def _assess_sophistication(self, fallacies: List[Dict]) -> str:
        """√âvalue la sophistication rh√©torique."""
        if not fallacies:
            return "Faible"
        
        complex_fallacies = [f for f in fallacies if f.get("confidence", 0) > 0.8]
        
        if len(complex_fallacies) > 3:
            return "Tr√®s √©lev√©e"
        elif len(complex_fallacies) > 1:
            return "√âlev√©e"
        elif len(fallacies) > 2:
            return "Mod√©r√©e"
        else:
            return "Faible"
    
    def _assess_manipulation_level(self, fallacies: List[Dict]) -> str:
        """√âvalue le niveau de manipulation."""
        if not fallacies:
            return "Aucun"
        
        critical_fallacies = [f for f in fallacies if f.get("severity") == "Critique"]
        high_fallacies = [f for f in fallacies if f.get("severity") == "√âlev√©"]
        
        if critical_fallacies:
            return "Critique"
        elif len(high_fallacies) > 2:
            return "√âlev√©"
        elif len(fallacies) > 3:
            return "Mod√©r√©"
        else:
            return "Faible"
    
    def _determine_output_path(self, metadata: ReportMetadata, config: ReportConfiguration) -> Path:
        """D√©termine le chemin de sortie unifi√©."""
        if config.output_directory:
            output_dir = config.output_directory
        else:
            output_dir = Path(self.config.get("output_directory", "./reports"))
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = metadata.generated_at.strftime("%Y%m%d_%H%M%S")
        extensions = {"markdown": "md", "json": "json", "html": "html", "console": "txt"}
        extension = extensions.get(config.output_format, "txt")
        
        component = metadata.source_component.lower().replace(" ", "_")
        analysis_type = metadata.analysis_type.lower().replace(" ", "_")
        
        filename = f"rapport_{component}_{analysis_type}_{timestamp}.{extension}"
        return output_dir / filename
    
    def _save_report(self, content: str, output_path: Path, format_type: str) -> None:
        """Sauvegarde le rapport avec gestion des formats."""
        try:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            if format_type == "html":
                # Pour HTML, sauvegarder directement le contenu
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            else:
                # Pour les autres formats, utiliser save_text_report
                if not save_text_report(content, output_path):
                    raise Exception("√âchec de la sauvegarde via save_text_report")
            
            logger.info(f"Rapport unifi√© sauvegard√©: {output_path}")
            
        except Exception as e:
            logger.error(f"Erreur sauvegarde rapport unifi√© {output_path}: {e}")
            raise
    
    def _print_unified_summary(self, data: Dict[str, Any], metadata: ReportMetadata) -> None:
        """Affiche un r√©sum√© unifi√© sur la console."""
        print("\n" + "="*60)
        print(f"   RESUME - {metadata.source_component.upper()}")
        print("="*60)
        
        if "summary" in data:
            summary = data["summary"]
            print(f"[STATS] Sophistication rhetorique: {summary.get('rhetorical_sophistication', 'N/A')}")
            print(f"[WARN] Niveau de manipulation: {summary.get('manipulation_level', 'N/A')}")
            print(f"[SOPHISMES] Sophismes detectes: {summary.get('total_fallacies', 0)}")
            
            if "orchestration_summary" in summary:
                orch = summary["orchestration_summary"]
                print(f"[ORCH] Agents orchestres: {orch.get('agents_count', 'N/A')}")
                print(f"[STATUS] Statut d'orchestration: {orch.get('execution_status', 'N/A')}")
        
        if "performance_metrics" in data:
            perf = data["performance_metrics"]
            print(f"[PERF] Temps d'execution: {perf.get('total_execution_time_ms', 'N/A')}ms")
            print(f"[MEM] Memoire utilisee: {perf.get('memory_usage_mb', 'N/A')} MB")
        
        print("="*60)
    
    def generate_comparative_report(self,
                                  reports_data: List[Dict[str, Any]],
                                  comparison_metadata: ReportMetadata,
                                  config: Optional[ReportConfiguration] = None) -> str:
        """
        G√©n√®re un rapport comparatif entre plusieurs analyses.
        
        Args:
            reports_data: Liste des donn√©es de rapports √† comparer
            comparison_metadata: M√©tadonn√©es pour le rapport comparatif
            config: Configuration de g√©n√©ration
            
        Returns:
            str: Contenu du rapport comparatif
        """
        if config is None:
            config = ReportConfiguration()
        
        logger.info(f"G√©n√©ration rapport comparatif - {len(reports_data)} rapports")
        
        # Construire les donn√©es comparatives
        comparative_data = {
            "title": "RAPPORT COMPARATIF D'ANALYSES",
            "comparison_summary": self._build_comparison_summary(reports_data),
            "individual_reports": reports_data,
            "recommendations": self._generate_comparative_recommendations(reports_data)
        }
        
        # G√©n√©rer le rapport comparatif
        return self.generate_unified_report(comparative_data, comparison_metadata, config)
    
    def _build_comparison_summary(self, reports_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Construit un r√©sum√© comparatif des rapports."""
        summary = {
            "total_reports": len(reports_data),
            "components_analyzed": [],
            "average_sophistication": 0.0,
            "total_fallacies": 0,
            "performance_comparison": {}
        }
        
        for report in reports_data:
            # Collecter les composants
            metadata = report.get("report_metadata", {})
            component = metadata.get("source_component", "Unknown")
            if component not in summary["components_analyzed"]:
                summary["components_analyzed"].append(component)
            
            # Agr√©ger les m√©triques
            report_summary = report.get("summary", {})
            summary["total_fallacies"] += report_summary.get("total_fallacies", 0)
            
            # Performance
            perf = report.get("performance_metrics", {})
            comp_key = f"{component}_performance"
            summary["performance_comparison"][comp_key] = {
                "execution_time": perf.get("total_execution_time_ms", 0),
                "memory_usage": perf.get("memory_usage_mb", 0)
            }
        
        return summary
    
    def _generate_comparative_recommendations(self, reports_data: List[Dict[str, Any]]) -> List[str]:
        """G√©n√®re des recommandations bas√©es sur la comparaison."""
        recommendations = [
            "Analyse comparative effectu√©e entre multiple composants",
            "√âvaluer la coh√©rence des r√©sultats entre les diff√©rents composants",
            "Optimiser les composants avec des temps d'ex√©cution √©lev√©s",
            "Standardiser les formats de sortie pour am√©liorer la comparabilit√©"
        ]
        
        # Recommandations bas√©es sur les donn√©es
        total_fallacies = sum(
            report.get("summary", {}).get("total_fallacies", 0) 
            for report in reports_data
        )
        
        if total_fallacies > 10:
            recommendations.append("Niveau √©lev√© de sophismes d√©tect√©s - r√©vision approfondie recommand√©e")
        
        return recommendations
    
    def get_available_templates(self) -> List[str]:
        """Retourne la liste des templates disponibles."""
        return list(self.templates.keys())
    
    def get_supported_formats(self) -> List[str]:
        """Retourne les formats support√©s."""
        return ["console", "markdown", "json", "html"]
    
    def get_supported_components(self) -> List[str]:
        """Retourne les composants support√©s."""
        return [
            "RealLLMOrchestrator",
            "ConversationOrchestrator", 
            "UnifiedTextAnalysis",
            "SourceManagement",
            "AdvancedRhetoric",
            "ReportingPipeline"
        ]

# API de convenance pour une utilisation simple
def generate_quick_report(data: Dict[str, Any],
                         source_component: str,
                         analysis_type: str = "general",
                         output_format: str = "markdown") -> str:
    """
    API simplifi√©e pour g√©n√©rer rapidement un rapport.
    
    Args:
        data: Donn√©es d'analyse
        source_component: Nom du composant source
        analysis_type: Type d'analyse
        output_format: Format de sortie d√©sir√©
        
    Returns:
        str: Contenu du rapport g√©n√©r√©
    """
    generator = UnifiedReportGenerator()
    metadata = ReportMetadata(
        source_component=source_component,
        analysis_type=analysis_type,
        generated_at=datetime.now()
    )
    config = ReportConfiguration(output_format=output_format, output_mode="console")
    
    return generator.generate_unified_report(data, metadata, config)

def create_component_report_factory(component_name: str) -> Callable:
    """
    Cr√©e une factory de rapports pour un composant sp√©cifique.
    
    Args:
        component_name: Nom du composant
        
    Returns:
        Callable: Factory configur√©e pour le composant
    """
    def component_report_factory(data: Dict[str, Any],
                                analysis_type: str = "analysis",
                                output_format: str = "markdown",
                                save_to_file: bool = True) -> str:
        generator = UnifiedReportGenerator()
        metadata = ReportMetadata(
            source_component=component_name,
            analysis_type=analysis_type,
            generated_at=datetime.now()
        )
        
        output_mode = "both" if save_to_file else "console"
        config = ReportConfiguration(
            output_format=output_format,
            output_mode=output_mode
        )
        
        return generator.generate_unified_report(data, metadata, config)
    
    return component_report_factory