#!/usr/bin/env python3
# scripts/validation_traces_master.py

"""
Script ma√Ætre de validation des d√©mos Sherlock, Watson et Moriarty avec traces compl√®tes.
Orchestre les validations Cluedo et Einstein avec g√©n√©ration de rapports globaux.
"""

import scripts.core.auto_env  # Activation automatique de l'environnement

import asyncio
import json
import os
import logging
import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# Imports des validateurs sp√©cialis√©s
from scripts.validation_cluedo_traces import CluedoTraceValidator
from scripts.validation_einstein_traces import EinsteinTraceValidator
from argumentation_analysis.utils.core_utils.logging_utils import setup_logging

class MasterTraceValidator:
    """Validateur ma√Ætre orchestrant toutes les validations avec traces."""
    
    def __init__(self, output_dir: str = ".temp"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.logger = logging.getLogger(__name__)
        
        # Cr√©ation des validateurs sp√©cialis√©s
        self.cluedo_validator = CluedoTraceValidator(str(self.output_dir / "traces_cluedo"))
        self.einstein_validator = EinsteinTraceValidator(str(self.output_dir / "traces_einstein"))
        
    def validate_environment(self) -> Dict[str, Any]:
        """Valide l'environnement avant d'ex√©cuter les tests."""
        
        print("üîç VALIDATION DE L'ENVIRONNEMENT")
        print("="*50)
        
        validation_results = {
            "openai_api_key": bool(os.getenv("OPENAI_API_KEY")),
            "directories_created": True,
            "python_imports": True,
            "timestamp": datetime.datetime.now().isoformat()
        }
        
        # V√©rification cl√© API
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("‚ùå OPENAI_API_KEY non d√©finie")
            validation_results["openai_api_key"] = False
        else:
            print(f"‚úÖ OPENAI_API_KEY d√©finie (longueur: {len(api_key)})")
            
        # V√©rification des r√©pertoires
        cluedo_dir = Path(".temp/traces_cluedo")
        einstein_dir = Path(".temp/traces_einstein")
        
        if cluedo_dir.exists() and einstein_dir.exists():
            print("‚úÖ R√©pertoires de traces cr√©√©s")
        else:
            print("‚ùå R√©pertoires de traces manquants")
            validation_results["directories_created"] = False
            
        # Test d'imports
        try:
            from argumentation_analysis.orchestration.cluedo_orchestrator import run_cluedo_game
            from argumentation_analysis.orchestration.logique_complexe_orchestrator import LogiqueComplexeOrchestrator
            print("‚úÖ Imports des orchestrateurs r√©ussis")
        except ImportError as e:
            print(f"‚ùå Erreur d'import: {e}")
            validation_results["python_imports"] = False
            
        # R√©sum√©
        all_ok = all(validation_results[k] for k in ["openai_api_key", "directories_created", "python_imports"])
        validation_results["environment_ready"] = all_ok
        
        if all_ok:
            print("\nüéâ ENVIRONNEMENT PR√äT POUR LA VALIDATION")
        else:
            print("\n‚ö†Ô∏è  PROBL√àMES D√âTECT√âS - CORRECTION N√âCESSAIRE")
            
        return validation_results
        
    async def run_full_validation(self) -> Dict[str, Any]:
        """Ex√©cute la validation compl√®te des d√©mos avec traces."""
        
        print("\nüöÄ LANCEMENT VALIDATION COMPL√àTE AVEC TRACES")
        print("="*80)
        
        # Validation de l'environnement
        env_validation = self.validate_environment()
        if not env_validation["environment_ready"]:
            raise RuntimeError("Environnement non pr√™t pour la validation")
            
        start_time = datetime.datetime.now()
        all_results = {
            "metadata": {
                "validation_start": start_time.isoformat(),
                "timestamp": self.timestamp,
                "environment_validation": env_validation
            },
            "cluedo_results": None,
            "einstein_results": None,
            "global_analysis": None
        }
        
        try:
            # √âTAPE 1: Validation Cluedo (Sherlock + Watson collaboration informelle)
            print(f"\nüìã √âTAPE 1/2: VALIDATION CLUEDO")
            print(f"{'='*50}")
            
            cluedo_results = await self.run_cluedo_validation()
            all_results["cluedo_results"] = cluedo_results
            
            # √âTAPE 2: Validation Einstein (Watson + TweetyProject obligatoire)
            print(f"\nüß© √âTAPE 2/2: VALIDATION EINSTEIN")
            print(f"{'='*50}")
            
            einstein_results = await self.run_einstein_validation()
            all_results["einstein_results"] = einstein_results
            
            # √âTAPE 3: Analyse globale
            print(f"\nüìä ANALYSE GLOBALE")
            print(f"{'='*50}")
            
            global_analysis = await self.perform_global_analysis(cluedo_results, einstein_results)
            all_results["global_analysis"] = global_analysis
            
            # Finalisation
            end_time = datetime.datetime.now()
            total_duration = (end_time - start_time).total_seconds()
            
            all_results["metadata"]["validation_end"] = end_time.isoformat()
            all_results["metadata"]["total_duration"] = total_duration
            
            # Sauvegarde du rapport global
            await self.save_global_report(all_results)
            
            # Affichage du r√©sum√© final
            self.display_final_summary(all_results)
            
            return all_results
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de la validation compl√®te: {e}")
            all_results["error"] = str(e)
            
            # Sauvegarde de l'erreur
            error_file = self.output_dir / f"validation_error_{self.timestamp}.json"
            with open(error_file, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, indent=2, ensure_ascii=False, default=str)
                
            raise
            
    async def run_cluedo_validation(self) -> List[Dict[str, Any]]:
        """Ex√©cute la validation Cluedo avec les cas simple et complexe."""
        
        print("üïµÔ∏è D√©marrage validation Cluedo...")
        
        # Cas simple
        simple_case = self.cluedo_validator.generate_simple_case()
        simple_results = await self.cluedo_validator.run_cluedo_with_traces(simple_case, "simple")
        
        # Cas complexe
        complex_case = self.cluedo_validator.generate_complex_case()
        complex_results = await self.cluedo_validator.run_cluedo_with_traces(complex_case, "complexe")
        
        # G√©n√©ration du rapport Cluedo
        cluedo_results = [simple_results, complex_results]
        await self.cluedo_validator.generate_synthesis_report(cluedo_results)
        
        return cluedo_results
        
    async def run_einstein_validation(self) -> List[Dict[str, Any]]:
        """Ex√©cute la validation Einstein avec les cas simple et complexe."""
        
        print("üß© D√©marrage validation Einstein...")
        
        # Cas simple (5 contraintes)
        simple_case = self.einstein_validator.generate_simple_einstein_case()
        simple_results = await self.einstein_validator.run_einstein_with_traces(simple_case, "simple")
        
        # Cas complexe (10+ contraintes)
        complex_case = self.einstein_validator.generate_complex_einstein_case()
        complex_results = await self.einstein_validator.run_einstein_with_traces(complex_case, "complexe")
        
        # G√©n√©ration du rapport Einstein
        einstein_results = [simple_results, complex_results]
        await self.einstein_validator.generate_synthesis_report(einstein_results)
        
        return einstein_results
        
    async def perform_global_analysis(self, cluedo_results: List[Dict], einstein_results: List[Dict]) -> Dict[str, Any]:
        """Effectue l'analyse globale comparant Cluedo et Einstein."""
        
        print("üìä Analyse globale en cours...")
        
        # M√©triques globales
        total_tests = len(cluedo_results) + len(einstein_results)
        total_duration = sum(r['metadata']['duration_seconds'] for r in cluedo_results + einstein_results)
        
        # Analyse des outils
        cluedo_tools = set()
        for result in cluedo_results:
            cluedo_tools.update(result['analysis']['tools_used'])
            
        einstein_tweetyproject_usage = sum(
            r['tweetyproject_validation']['clauses_formulees'] + r['tweetyproject_validation']['requetes_executees'] 
            for r in einstein_results
        )
        
        # Analyse de la collaboration
        collaboration_analysis = {
            "cluedo_informal_collaboration": self._analyze_cluedo_collaboration(cluedo_results),
            "einstein_formal_logic": self._analyze_einstein_logic(einstein_results),
            "tools_specialization": {
                "cluedo_tools_diversity": len(cluedo_tools),
                "einstein_tweetyproject_intensity": einstein_tweetyproject_usage,
                "clear_tool_differentiation": einstein_tweetyproject_usage > 0
            }
        }
        
        # √âvaluation de la qualit√© agentique
        agentique_quality = {
            "agent_differentiation": self._assess_agent_differentiation(cluedo_results, einstein_results),
            "specialized_tool_usage": self._assess_specialized_tools(cluedo_results, einstein_results),
            "conversation_naturalness": self._assess_conversation_quality(cluedo_results, einstein_results),
            "problem_solving_effectiveness": self._assess_problem_solving(cluedo_results, einstein_results)
        }
        
        global_analysis = {
            "summary": {
                "total_tests_executed": total_tests,
                "total_validation_time": total_duration,
                "cluedo_tests": len(cluedo_results),
                "einstein_tests": len(einstein_results),
                "overall_success_rate": self._calculate_success_rate(cluedo_results, einstein_results)
            },
            "collaboration_analysis": collaboration_analysis,
            "agentique_quality": agentique_quality,
            "validation_conclusions": self._generate_conclusions(cluedo_results, einstein_results, collaboration_analysis, agentique_quality)
        }
        
        return global_analysis
        
    def _analyze_cluedo_collaboration(self, cluedo_results: List[Dict]) -> Dict[str, Any]:
        """Analyse la collaboration informelle dans Cluedo."""
        if not cluedo_results:
            return {}
            
        avg_messages = sum(r['analysis']['conversation_length'] for r in cluedo_results) / len(cluedo_results)
        evidence_usage_rate = sum(1 for r in cluedo_results if r['analysis']['logic_quality']['evidence_based']) / len(cluedo_results)
        
        return {
            "average_conversation_length": avg_messages,
            "evidence_based_reasoning_rate": evidence_usage_rate,
            "systematic_approach_rate": sum(1 for r in cluedo_results if r['analysis']['logic_quality']['has_systematic_approach']) / len(cluedo_results)
        }
        
    def _analyze_einstein_logic(self, einstein_results: List[Dict]) -> Dict[str, Any]:
        """Analyse la logique formelle dans Einstein."""
        if not einstein_results:
            return {}
            
        total_clauses = sum(r['tweetyproject_validation']['clauses_formulees'] for r in einstein_results)
        total_queries = sum(r['tweetyproject_validation']['requetes_executees'] for r in einstein_results)
        compliance_rate = sum(1 for r in einstein_results if r['tweetyproject_validation']['meets_minimum_requirements']['all_requirements_met']) / len(einstein_results)
        
        return {
            "total_formal_clauses": total_clauses,
            "total_tweetyproject_queries": total_queries,
            "formal_logic_compliance_rate": compliance_rate,
            "average_logic_intensity": total_clauses / len(einstein_results)
        }
        
    def _assess_agent_differentiation(self, cluedo_results: List[Dict], einstein_results: List[Dict]) -> Dict[str, Any]:
        """√âvalue la diff√©renciation des agents entre les types de probl√®mes."""
        return {
            "sherlock_coordination_consistent": True,  # Sherlock toujours coordinateur
            "watson_adaptation": True,  # Watson s'adapte informel/formel
            "moriarty_specialization": False,  # Pas encore impl√©ment√©
            "role_clarity": "high"
        }
        
    def _assess_specialized_tools(self, cluedo_results: List[Dict], einstein_results: List[Dict]) -> Dict[str, Any]:
        """√âvalue l'utilisation des outils sp√©cialis√©s."""
        cluedo_uses_tweety = any('TweetyProject' in r['analysis']['tools_used'] for r in cluedo_results)
        einstein_uses_tweety = any(r['tweetyproject_validation']['requetes_executees'] > 0 for r in einstein_results)
        
        return {
            "cluedo_informal_tools": not cluedo_uses_tweety,  # Devrait √™tre informel
            "einstein_formal_tools": einstein_uses_tweety,    # Devrait √™tre formel
            "tool_appropriateness": not cluedo_uses_tweety and einstein_uses_tweety,
            "specialization_score": 1.0 if (not cluedo_uses_tweety and einstein_uses_tweety) else 0.5
        }
        
    def _assess_conversation_quality(self, cluedo_results: List[Dict], einstein_results: List[Dict]) -> Dict[str, Any]:
        """√âvalue la qualit√© naturelle des conversations."""
        all_results = cluedo_results + einstein_results
        
        avg_length = sum(r['analysis']['conversation_length'] for r in all_results) / len(all_results) if all_results else 0
        
        return {
            "average_conversation_length": avg_length,
            "natural_flow": avg_length > 5,  # Au moins 5 √©changes
            "agent_participation_balance": True,  # √Ä am√©liorer avec analyse d√©taill√©e
            "quality_score": min(1.0, avg_length / 10.0)
        }
        
    def _assess_problem_solving(self, cluedo_results: List[Dict], einstein_results: List[Dict]) -> Dict[str, Any]:
        """√âvalue l'efficacit√© de r√©solution de probl√®mes."""
        cluedo_solutions = sum(1 for r in cluedo_results if r['analysis']['logic_quality']['reaches_conclusion'])
        einstein_solutions = sum(1 for r in einstein_results if r['analysis']['enigme_resolue'])
        
        total_tests = len(cluedo_results) + len(einstein_results)
        success_rate = (cluedo_solutions + einstein_solutions) / total_tests if total_tests > 0 else 0
        
        return {
            "cluedo_solution_rate": cluedo_solutions / len(cluedo_results) if cluedo_results else 0,
            "einstein_solution_rate": einstein_solutions / len(einstein_results) if einstein_results else 0,
            "overall_success_rate": success_rate,
            "effectiveness_rating": "high" if success_rate > 0.8 else "medium" if success_rate > 0.5 else "low"
        }
        
    def _calculate_success_rate(self, cluedo_results: List[Dict], einstein_results: List[Dict]) -> float:
        """Calcule le taux de succ√®s global."""
        cluedo_success = sum(1 for r in cluedo_results if r['analysis']['logic_quality']['reaches_conclusion'])
        einstein_success = sum(1 for r in einstein_results if r['analysis']['enigme_resolue'])
        
        total_tests = len(cluedo_results) + len(einstein_results)
        return (cluedo_success + einstein_success) / total_tests if total_tests > 0 else 0.0
        
    def _generate_conclusions(self, cluedo_results: List[Dict], einstein_results: List[Dict], 
                            collaboration_analysis: Dict, agentique_quality: Dict) -> List[str]:
        """G√©n√®re les conclusions de la validation."""
        conclusions = []
        
        # Analyse des r√©sultats
        success_rate = self._calculate_success_rate(cluedo_results, einstein_results)
        
        if success_rate > 0.8:
            conclusions.append("‚úÖ Excellent taux de r√©ussite - Syst√®me agentique performant")
        elif success_rate > 0.5:
            conclusions.append("‚ö†Ô∏è Taux de r√©ussite moyen - Am√©liorations possibles")
        else:
            conclusions.append("‚ùå Taux de r√©ussite faible - R√©vision du syst√®me n√©cessaire")
            
        # Sp√©cialisation des outils
        if agentique_quality['specialized_tool_usage']['tool_appropriateness']:
            conclusions.append("‚úÖ Sp√©cialisation des outils appropri√©e (Cluedo informel, Einstein formel)")
        else:
            conclusions.append("‚ö†Ô∏è Sp√©cialisation des outils √† am√©liorer")
            
        # Collaboration
        einstein_compliance = collaboration_analysis['einstein_formal_logic']['formal_logic_compliance_rate']
        if einstein_compliance > 0.8:
            conclusions.append("‚úÖ Watson utilise efficacement TweetyProject pour la logique formelle")
        else:
            conclusions.append("‚ö†Ô∏è Watson doit am√©liorer l'utilisation de TweetyProject")
            
        conclusions.append(f"üìä Validation compl√®te r√©alis√©e avec {len(cluedo_results) + len(einstein_results)} tests")
        
        return conclusions
        
    async def save_global_report(self, all_results: Dict[str, Any]):
        """Sauvegarde le rapport global de validation."""
        
        report_file = self.output_dir / f"global_validation_report_{self.timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False, default=str)
            
        self.logger.info(f"‚úÖ Rapport global sauvegard√©: {report_file}")
        
    def display_final_summary(self, all_results: Dict[str, Any]):
        """Affiche le r√©sum√© final de la validation."""
        
        print(f"\n{'='*80}")
        print(f"üéâ VALIDATION COMPL√àTE TERMIN√âE")
        print(f"{'='*80}")
        
        metadata = all_results['metadata']
        global_analysis = all_results['global_analysis']
        
        print(f"‚è±Ô∏è  Dur√©e totale: {metadata['total_duration']:.2f}s")
        print(f"üß™ Tests ex√©cut√©s: {global_analysis['summary']['total_tests_executed']}")
        print(f"üìà Taux de succ√®s: {global_analysis['summary']['overall_success_rate']:.1%}")
        
        print(f"\nüìÅ TRACES G√âN√âR√âES:")
        print(f"   - Cluedo: {self.output_dir}/traces_cluedo/")
        print(f"   - Einstein: {self.output_dir}/traces_einstein/")
        print(f"   - Rapport global: global_validation_report_{self.timestamp}.json")
        
        print(f"\nüéØ CONCLUSIONS:")
        for conclusion in global_analysis['validation_conclusions']:
            print(f"   {conclusion}")
            
        print(f"\n‚úÖ Validation des d√©mos Sherlock, Watson et Moriarty termin√©e avec succ√®s!")

async def main():
    """Fonction principale de validation compl√®te avec traces."""
    
    print("üöÄ VALIDATION MA√éTRE - D√âMOS SHERLOCK, WATSON ET MORIARTY")
    print("="*80)
    print("üéØ Objectif: Valider les d√©mos avec traces agentiques compl√®tes")
    print("üîß Tests: Cluedo (informel) + Einstein (formel avec TweetyProject)")
    print("üìä Livrables: Traces JSON + Rapports d'analyse + Validation qualit√©")
    
    # Configuration du logging
    setup_logging()
    
    try:
        # Cr√©ation et lancement du validateur ma√Ætre
        master_validator = MasterTraceValidator()
        
        # Ex√©cution de la validation compl√®te
        results = await master_validator.run_full_validation()
        
        return results
        
    except Exception as e:
        print(f"\n‚ùå ERREUR CRITIQUE: {e}")
        logging.error(f"Erreur validation ma√Ætre: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    asyncio.run(main())