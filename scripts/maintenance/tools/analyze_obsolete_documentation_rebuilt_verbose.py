"""
Script d'analyse de documentation obsol√®te - Version avec logging verbeux
Oracle Enhanced v2.1.0 - Reconstruction apr√®s crash
"""

import argumentation_analysis.core.environment
import os
import re
import json
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime
import logging

# Configuration du logging avec timestamps
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


class DocumentationAnalyzer:
    """Analyseur de documentation obsol√®te - Version robuste avec logging verbeux"""

    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root).resolve()
        self.doc_extensions = {".md", ".rst", ".txt", ".html"}
        self.results = {
            "analysis_timestamp": datetime.now().isoformat(),
            "project_root": str(self.project_root),
            "broken_links": [],
            "valid_links": [],
            "files_analyzed": 0,
            "total_links": 0,
        }

        # R√©pertoires √† ignorer pour √©viter l'enlisement
        self.ignore_dirs = {
            "libs/portable_jdk",
            ".git",
            "__pycache__",
            ".pytest_cache",
            "venv",
            "venv_test",
            ".vscode",
            ".vs",
            "migration_output",
            "_archives",
            "_temp",
            "argumentation_analysis_project.egg-info",
            "argumentation_analysis.egg-info",
        }

    def find_documentation_files(self, quick_scan=False) -> List[Path]:
        """Trouve les fichiers de documentation de mani√®re optimis√©e"""
        logger.info("üîç D√©but recherche fichiers documentation...")
        start_time = datetime.now()
        doc_files = []

        if quick_scan:
            logger.info("‚ö° Mode QUICK SCAN activ√©")
            # Scan ultra-rapide - seulement les fichiers .md prioritaires
            priority_patterns = [
                "README.md",
                "docs/**/*.md",
                "CHANGELOG*.md",
                "GUIDE*.md",
            ]

            for pattern in priority_patterns:
                try:
                    logger.info(f"   Recherche pattern: {pattern}")
                    files = list(self.project_root.glob(pattern))
                    logger.info(f"   ‚úì Trouv√© {len(files)} fichiers pour {pattern}")
                    doc_files.extend(files)
                except Exception as e:
                    logger.warning(f"   ‚ö† Erreur pattern {pattern}: {e}")
                    continue
        else:
            logger.info("üî¨ Mode FULL ANALYSIS activ√©")
            # Scan s√©curis√© avec exclusions
            for ext in self.doc_extensions:
                logger.info(f"   Recherche fichiers {ext}...")
                ext_start = datetime.now()
                try:
                    ext_files = []
                    for file_path in self.project_root.rglob(f"*{ext}"):
                        # V√©rifier si le fichier est dans un r√©pertoire ignor√©
                        if any(
                            ignore_dir in str(file_path)
                            for ignore_dir in self.ignore_dirs
                        ):
                            continue
                        ext_files.append(file_path)
                    doc_files.extend(ext_files)
                    ext_duration = (datetime.now() - ext_start).total_seconds()
                    logger.info(
                        f"   ‚úì {len(ext_files)} fichiers {ext} trouv√©s en {ext_duration:.1f}s"
                    )
                except Exception as e:
                    logger.warning(f"   ‚ö† Erreur recherche {ext}: {e}")
                    continue

        doc_files = list(set(doc_files))  # √âliminer les doublons
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(
            f"‚úÖ Recherche termin√©e: {len(doc_files)} fichiers en {duration:.1f}s"
        )
        return doc_files

    def extract_links_from_file(self, file_path: Path) -> List[Tuple[str, int]]:
        """Extrait les liens d'un fichier de mani√®re robuste"""
        links = []

        try:
            # Essayer plusieurs encodages
            content = None
            for encoding in ["utf-8", "latin-1", "cp1252"]:
                try:
                    with open(file_path, "r", encoding=encoding) as f:
                        content = f.read()
                    break
                except UnicodeDecodeError:
                    continue

            if content is None:
                return links

            # Pattern Markdown: [text](link)
            markdown_pattern = r"\[([^\]]+)\]\(([^)]+)\)"

            # Pattern HTML: <a href="link">
            html_pattern = r'<a\s+[^>]*href=["\']([^"\']+)["\']'

            lines = content.split("\n")
            for line_num, line in enumerate(lines, 1):
                # Extraire liens Markdown
                for match in re.finditer(markdown_pattern, line):
                    link = match.group(2)
                    # Ignorer les liens externes et ancres
                    if not link.startswith(("http://", "https://", "#", "mailto:")):
                        links.append((link, line_num))

                # Extraire liens HTML
                for match in re.finditer(html_pattern, line):
                    link = match.group(1)
                    if not link.startswith(("http://", "https://", "#", "mailto:")):
                        links.append((link, line_num))

        except Exception:
            pass

        return links

    def check_link_exists(self, link: str, source_file: Path) -> bool:
        """V√©rifie qu'un lien pointe vers un fichier existant"""
        try:
            # Nettoyer le lien
            link = link.split("#")[0].strip()

            # Construire le chemin absolu
            if link.startswith("/"):
                target_path = self.project_root / link.lstrip("/")
            else:
                target_path = source_file.parent / link

            # Normaliser et r√©soudre le chemin
            target_path = target_path.resolve()

            return target_path.exists()
        except Exception:
            return False

    def analyze_documentation(self, quick_scan=False) -> Dict:
        """Analyse la documentation et d√©tecte les liens bris√©s"""
        logger.info("=" * 70)
        logger.info("üöÄ D√âMARRAGE ANALYSE DOCUMENTATION")
        logger.info("=" * 70)

        analysis_start = datetime.now()

        doc_files = self.find_documentation_files(quick_scan)
        logger.info(f"üìä {len(doc_files)} fichiers √† analyser")

        self.results["files_analyzed"] = len(doc_files)

        logger.info("=" * 70)
        logger.info("üîó EXTRACTION ET V√âRIFICATION LIENS")
        logger.info("=" * 70)

        for i, doc_file in enumerate(doc_files):
            try:
                # Log tous les 10 fichiers pour plus de feedback
                if i % 10 == 0:
                    progress_pct = i / len(doc_files) * 100
                    elapsed = (datetime.now() - analysis_start).total_seconds()
                    rate = i / elapsed if elapsed > 0 else 0
                    eta = (len(doc_files) - i) / rate if rate > 0 else 0

                    logger.info(
                        f"üìà Progression: {i+1}/{len(doc_files)} ({progress_pct:.1f}%) - "
                        f"Vitesse: {rate:.1f} fichiers/s - "
                        f"ETA: {eta/60:.1f} min"
                    )
                    logger.info(
                        f"   Liens trouv√©s: {self.results['total_links']} "
                        f"(Bris√©s: {len(self.results['broken_links'])}, "
                        f"Valides: {len(self.results['valid_links'])})"
                    )

                links = self.extract_links_from_file(doc_file)

                if links and i % 50 == 0:  # Log √©chantillon tous les 50 fichiers
                    logger.info(f"   üìÑ {doc_file.name}: {len(links)} liens d√©tect√©s")

                for link, line_num in links:
                    self.results["total_links"] += 1

                    link_info = {
                        "link": link,
                        "source_file": str(doc_file.relative_to(self.project_root)),
                        "line_number": line_num,
                    }

                    if self.check_link_exists(link, doc_file):
                        self.results["valid_links"].append(link_info)
                    else:
                        self.results["broken_links"].append(link_info)

            except Exception as e:
                logger.warning(f"‚ö† Erreur analyse {doc_file.name}: {e}")
                continue

        # Calculer les statistiques
        total_links = self.results["total_links"]
        broken_count = len(self.results["broken_links"])
        valid_count = len(self.results["valid_links"])

        self.results["summary"] = {
            "total_links": total_links,
            "broken_links": broken_count,
            "valid_links": valid_count,
            "broken_percentage": (
                (broken_count / total_links * 100) if total_links > 0 else 0
            ),
        }

        duration = (datetime.now() - analysis_start).total_seconds()

        logger.info("=" * 70)
        logger.info("‚úÖ ANALYSE TERMIN√âE")
        logger.info("=" * 70)
        logger.info(f"‚è±  Dur√©e totale: {duration/60:.2f} minutes")
        logger.info(f"üìä Fichiers analys√©s: {self.results['files_analyzed']}")
        logger.info(f"üîó Liens totaux: {total_links}")
        logger.info(
            f"‚ùå Liens bris√©s: {broken_count} ({(broken_count/total_links*100):.1f}%)"
        )
        logger.info(
            f"‚úì  Liens valides: {valid_count} ({(valid_count/total_links*100):.1f}%)"
        )
        logger.info("=" * 70)

        return self.results

    def generate_report(self, output_file: str = None) -> str:
        """G√©n√®re un rapport de documentation"""
        logger.info("üìù G√©n√©ration du rapport...")

        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"logs/documentation_analysis_{timestamp}.md"

        # Cr√©er le dossier si n√©cessaire
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)

        # Sauvegarder en JSON
        json_file = (
            output_file.replace(".md", ".json")
            if output_file.endswith(".md")
            else output_file
        )
        try:
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            logger.info(f"‚úÖ JSON sauvegard√©: {json_file}")
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde JSON: {e}")

        summary = self.results.get("summary", {})

        report = f"""# Rapport d'Analyse de Documentation
## Oracle Enhanced v2.1.0 - Reconstruction

**Date d'analyse :** {self.results['analysis_timestamp']}  
**Racine du projet :** `{self.results['project_root']}`

## R√©sum√©

- **Fichiers analys√©s :** {self.results['files_analyzed']}
- **Liens totaux :** {summary.get('total_links', 0)}
- **Liens bris√©s :** {summary.get('broken_links', 0)} ({summary.get('broken_percentage', 0):.1f}%)
- **Liens valides :** {summary.get('valid_links', 0)}

## Liens Bris√©s (Top 20)

"""

        # Afficher les 20 premiers liens bris√©s
        broken_links = self.results["broken_links"][:20]

        if broken_links:
            for i, broken_link in enumerate(broken_links, 1):
                report += f"""### {i}. {broken_link['link']}
- **Fichier :** `{broken_link['source_file']}` (ligne {broken_link['line_number']})

"""
        else:
            report += "Aucun lien bris√© d√©tect√© !\n\n"

        if len(self.results["broken_links"]) > 20:
            report += f"\n*... et {len(self.results['broken_links']) - 20} autres liens bris√©s*\n\n"

        report += """## Actions Recommand√©es

1. **Examiner les liens bris√©s** list√©s ci-dessus
2. **Corriger ou supprimer** les r√©f√©rences obsol√®tes
3. **Re-ex√©cuter l'analyse** apr√®s corrections

---
*Rapport g√©n√©r√© par Oracle Enhanced v2.1.0 Documentation Analyzer (Version reconstruite)*
"""

        try:
            markdown_file = (
                output_file if output_file.endswith(".md") else output_file + ".md"
            )
            with open(markdown_file, "w", encoding="utf-8") as f:
                f.write(report)
            logger.info(f"‚úÖ Rapport Markdown sauvegard√©: {markdown_file}")
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde rapport: {e}")

        return json_file


def main():
    """Point d'entr√©e principal"""
    parser = argparse.ArgumentParser(
        description="Analyse de documentation obsol√®te (Version reconstruite)"
    )
    parser.add_argument(
        "--quick-scan", action="store_true", help="Analyse rapide (.md prioritaires)"
    )
    parser.add_argument("--full-analysis", action="store_true", help="Analyse compl√®te")
    parser.add_argument("--output", help="Fichier de sortie")

    args = parser.parse_args()

    analyzer = DocumentationAnalyzer()

    # Mode d'analyse
    quick_mode = args.quick_scan or not args.full_analysis

    try:
        results = analyzer.analyze_documentation(quick_scan=quick_mode)
        report_file = analyzer.generate_report(args.output)

        return 0 if results["summary"].get("broken_links", 0) == 0 else 1

    except Exception as e:
        logger.error(f"‚ùå Erreur analyse: {e}", exc_info=True)
        return 2


if __name__ == "__main__":
    exit(main())
