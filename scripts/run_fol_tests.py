#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Script d'exÃ©cution des tests FirstOrderLogicAgent (FOL).

Ce script lance une validation complÃ¨te de l'agent FOL selon les critÃ¨res :
âœ… Tests unitaires complets  
âœ… Tests d'intÃ©gration Tweety
âœ… Validation complÃ¨te avec mÃ©triques
âœ… Tests de migration Modal â†’ FOL
âœ… GÃ©nÃ©ration de rapport dÃ©taillÃ©

Usage:
    python scripts/run_fol_tests.py [options]
    
Options:
    --unit-only       : Tests unitaires seulement
    --integration     : Tests d'intÃ©gration (nÃ©cessite Tweety)
    --validation      : Validation complÃ¨te avec mÃ©triques
    --migration       : Tests migration Modal â†’ FOL
    --all            : Tous les tests (dÃ©faut)
    --real-tweety    : Force utilisation Tweety rÃ©el
    --report-path    : Chemin rapport de sortie
"""

import argparse
import asyncio
import subprocess
import sys
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FOLTestRunner:
    """ExÃ©cuteur de tests pour agent FOL."""
    
    def __init__(self, args):
        self.args = args
        self.results = {}
        self.start_time = time.time()
        
        # Chemins des tests
        self.test_paths = {
            "unit": "tests/unit/agents/test_fol_logic_agent.py",
            "integration": "tests/integration/test_fol_tweety_integration.py", 
            "validation": "tests/validation/test_fol_complete_validation.py",
            "migration": "tests/migration/test_modal_to_fol_migration.py"
        }
        
        # Configuration environnement
        self.env_config = self._setup_environment()
    
    def _setup_environment(self) -> Dict[str, str]:
        """Configure l'environnement pour les tests."""
        import os
        
        env = os.environ.copy()
        
        # Configuration Tweety si demandÃ©
        if self.args.real_tweety:
            env.update({
                "USE_REAL_JPYPE": "true",
                "TWEETY_JAR_PATH": "libs/tweety-full.jar",
                "JVM_MEMORY": "1024m"
            })
            logger.info("ğŸ”§ Configuration Tweety rÃ©el activÃ©e")
        else:
            env.update({
                "USE_REAL_JPYPE": "false"
            })
            logger.info("ğŸ”§ Configuration Tweety mock activÃ©e")
        
        # Configuration tests
        env.update({
            "UNIFIED_LOGIC_TYPE": "fol",
            "UNIFIED_MOCK_LEVEL": "none" if self.args.real_tweety else "partial",
            "PYTHONPATH": str(Path.cwd())
        })
        
        return env
    
    def run_pytest_suite(self, test_path: str, test_name: str) -> Dict[str, Any]:
        """ExÃ©cute une suite de tests pytest."""
        logger.info(f"â–¶ï¸ ExÃ©cution {test_name}...")
        
        start_time = time.time()
        
        # Construction commande pytest
        cmd = [
            sys.executable, "-m", "pytest",
            test_path,
            "-v",
            "--tb=short"
        ]
        
        # Options spÃ©ciales pour intÃ©gration
        if test_name == "integration" and not self.args.real_tweety:
            cmd.extend(["-k", "not test_real_tweety"])
        
        try:
            # CrÃ©ation rÃ©pertoire rapports
            Path("reports").mkdir(exist_ok=True)
            
            # ExÃ©cution
            result = subprocess.run(
                cmd,
                env=self.env_config,
                capture_output=True,
                text=True,
                timeout=300  # 5 minutes max par suite
            )
            
            duration = time.time() - start_time
            
            # Analyse rÃ©sultats
            success = result.returncode == 0
            
            test_result = {
                "success": success,
                "duration": duration,
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "command": " ".join(cmd)
            }
            
            # Lecture rapport JSON si disponible
            json_report_path = f"reports/{test_name}_report.json"
            if Path(json_report_path).exists():
                try:
                    with open(json_report_path, 'r') as f:
                        json_report = json.load(f)
                        test_result["detailed_report"] = json_report
                except Exception as e:
                    logger.warning(f"âš ï¸ Impossible de lire rapport JSON: {e}")
            
            if success:
                logger.info(f"âœ… {test_name} rÃ©ussi en {duration:.2f}s")
            else:
                logger.error(f"âŒ {test_name} Ã©chouÃ© en {duration:.2f}s")
                logger.error(f"Erreur: {result.stderr}")
            
            return test_result
            
        except subprocess.TimeoutExpired:
            logger.error(f"âŒ {test_name} timeout aprÃ¨s 5 minutes")
            return {
                "success": False,
                "duration": 300,
                "return_code": -1,
                "error": "Timeout",
                "command": " ".join(cmd)
            }
        except Exception as e:
            logger.error(f"âŒ Erreur exÃ©cution {test_name}: {e}")
            return {
                "success": False,
                "duration": 0,
                "return_code": -1,
                "error": str(e),
                "command": " ".join(cmd)
            }
    
    async def run_validation_script(self) -> Dict[str, Any]:
        """ExÃ©cute le script de validation complÃ¨te."""
        logger.info("â–¶ï¸ ExÃ©cution validation complÃ¨te...")
        
        start_time = time.time()
        
        try:
            # Import et exÃ©cution du validateur
            sys.path.insert(0, str(Path.cwd()))
            
            from tests.validation.test_fol_complete_validation import FOLCompleteValidator
            
            validator = FOLCompleteValidator()
            report = await validator.run_complete_validation()
            
            duration = time.time() - start_time
            
            validation_result = {
                "success": report.get("overall_success", False),
                "duration": duration,
                "validation_report": report
            }
            
            if validation_result["success"]:
                logger.info(f"âœ… Validation complÃ¨te rÃ©ussie en {duration:.2f}s")
            else:
                logger.warning(f"âš ï¸ Validation complÃ¨te avec problÃ¨mes en {duration:.2f}s")
            
            return validation_result
            
        except Exception as e:
            logger.error(f"âŒ Erreur validation complÃ¨te: {e}")
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e)
            }
    
    def run_unit_tests(self):
        """ExÃ©cute les tests unitaires."""
        if not Path(self.test_paths["unit"]).exists():
            logger.error(f"âŒ Tests unitaires non trouvÃ©s: {self.test_paths['unit']}")
            return {"success": False, "error": "Tests non trouvÃ©s"}
        
        return self.run_pytest_suite(self.test_paths["unit"], "unit")
    
    def run_integration_tests(self):
        """ExÃ©cute les tests d'intÃ©gration."""
        if not Path(self.test_paths["integration"]).exists():
            logger.error(f"âŒ Tests intÃ©gration non trouvÃ©s: {self.test_paths['integration']}")
            return {"success": False, "error": "Tests non trouvÃ©s"}
        
        return self.run_pytest_suite(self.test_paths["integration"], "integration")
    
    async def run_validation_tests(self):
        """ExÃ©cute la validation complÃ¨te."""
        if not Path(self.test_paths["validation"]).exists():
            logger.error(f"âŒ Tests validation non trouvÃ©s: {self.test_paths['validation']}")
            return {"success": False, "error": "Tests non trouvÃ©s"}
        
        return await self.run_validation_script()
    
    def run_migration_tests(self):
        """ExÃ©cute les tests de migration."""
        if not Path(self.test_paths["migration"]).exists():
            logger.error(f"âŒ Tests migration non trouvÃ©s: {self.test_paths['migration']}")
            return {"success": False, "error": "Tests non trouvÃ©s"}
        
        return self.run_pytest_suite(self.test_paths["migration"], "migration")
    
    async def run_all_tests(self):
        """ExÃ©cute tous les tests selon la configuration."""
        logger.info("ğŸš€ DÃ©but exÃ©cution tests FOL")
        
        # SÃ©lection des tests Ã  exÃ©cuter
        if self.args.unit_only:
            test_suites = [("unit", self.run_unit_tests)]
        elif self.args.integration:
            test_suites = [("integration", self.run_integration_tests)]
        elif self.args.validation:
            test_suites = [("validation", self.run_validation_tests)]
        elif self.args.migration:
            test_suites = [("migration", self.run_migration_tests)]
        else:  # --all ou dÃ©faut
            test_suites = [
                ("unit", self.run_unit_tests),
                ("integration", self.run_integration_tests),
                ("validation", self.run_validation_tests),
                ("migration", self.run_migration_tests)
            ]
        
        # ExÃ©cution des suites
        for suite_name, suite_func in test_suites:
            try:
                if asyncio.iscoroutinefunction(suite_func):
                    result = await suite_func()
                else:
                    result = suite_func()
                
                self.results[suite_name] = result
                
            except Exception as e:
                logger.error(f"âŒ Erreur suite {suite_name}: {e}")
                self.results[suite_name] = {
                    "success": False,
                    "error": str(e)
                }
        
        # GÃ©nÃ©ration rapport final
        await self.generate_final_report()
    
    async def generate_final_report(self):
        """GÃ©nÃ¨re le rapport final des tests."""
        total_duration = time.time() - self.start_time
        
        # Calcul statistiques globales
        total_suites = len(self.results)
        successful_suites = sum(1 for r in self.results.values() if r.get("success", False))
        
        # CrÃ©ation rapport
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_duration": total_duration,
            "configuration": {
                "real_tweety": self.args.real_tweety,
                "test_selection": self._get_test_selection()
            },
            "summary": {
                "total_suites": total_suites,
                "successful_suites": successful_suites,
                "success_rate": successful_suites / total_suites if total_suites > 0 else 0.0,
                "overall_success": successful_suites == total_suites
            },
            "results": self.results,
            "recommendations": self._generate_recommendations()
        }
        
        # Sauvegarde rapport
        report_path = Path(self.args.report_path or "reports/fol_tests_complete_report.json")
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        # Affichage rÃ©sumÃ©
        self._print_summary(report, report_path)
        
        return report
    
    def _get_test_selection(self) -> str:
        """Retourne la sÃ©lection de tests."""
        if self.args.unit_only:
            return "unit_only"
        elif self.args.integration:
            return "integration_only"
        elif self.args.validation:
            return "validation_only"
        elif self.args.migration:
            return "migration_only"
        else:
            return "all"
    
    def _generate_recommendations(self) -> List[str]:
        """GÃ©nÃ¨re recommandations basÃ©es sur les rÃ©sultats."""
        recommendations = []
        
        # Analyse des Ã©checs
        failed_suites = [name for name, result in self.results.items() 
                        if not result.get("success", False)]
        
        if not failed_suites:
            recommendations.append("âœ… Tous les tests rÃ©ussis - Agent FOL prÃªt pour production")
        else:
            for suite in failed_suites:
                if suite == "unit":
                    recommendations.append("Corriger les tests unitaires avant dÃ©ploiement")
                elif suite == "integration":
                    recommendations.append("VÃ©rifier intÃ©gration Tweety - possibles problÃ¨mes JAR/JVM")
                elif suite == "validation":
                    recommendations.append("AmÃ©liorer mÃ©triques validation selon rapport dÃ©taillÃ©")
                elif suite == "migration":
                    recommendations.append("Revoir migration Modal Logic - compatibilitÃ© insuffisante")
        
        # Recommandations configuration
        if not self.args.real_tweety:
            recommendations.append("ExÃ©cuter avec --real-tweety pour validation complÃ¨te")
        
        return recommendations
    
    def _print_summary(self, report: Dict[str, Any], report_path: Path):
        """Affiche rÃ©sumÃ© des rÃ©sultats."""
        print("\n" + "="*80)
        print("ğŸ“‹ RAPPORT TESTS AGENT FOL")
        print("="*80)
        
        summary = report["summary"]
        print(f"\nğŸ• DurÃ©e totale: {report['total_duration']:.2f}s")
        print(f"ğŸ¯ SuccÃ¨s global: {'âœ… OUI' if summary['overall_success'] else 'âŒ NON'}")
        print(f"ğŸ“Š Taux rÃ©ussite: {summary['success_rate']:.1%} ({summary['successful_suites']}/{summary['total_suites']})")
        
        print(f"\nğŸ“‹ RÃ©sultats par suite:")
        for suite_name, result in self.results.items():
            status = "âœ…" if result.get("success", False) else "âŒ"
            duration = result.get("duration", 0)
            print(f"  {status} {suite_name.title()}: {duration:.2f}s")
            
            if not result.get("success", False) and "error" in result:
                print(f"      Erreur: {result['error']}")
        
        print(f"\nğŸ’¡ Recommandations:")
        for rec in report["recommendations"]:
            print(f"  â€¢ {rec}")
        
        print(f"\nğŸ’¾ Rapport dÃ©taillÃ©: {report_path}")
        
        if summary["overall_success"]:
            print("\nğŸ‰ Agent FOL validÃ© avec succÃ¨s!")
        else:
            print("\nâš ï¸ Agent FOL nÃ©cessite des corrections")


def main():
    """Point d'entrÃ©e principal."""
    parser = argparse.ArgumentParser(
        description="ExÃ©cuteur de tests pour FirstOrderLogicAgent",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  python scripts/run_fol_tests.py --all
  python scripts/run_fol_tests.py --unit-only
  python scripts/run_fol_tests.py --integration --real-tweety
  python scripts/run_fol_tests.py --validation --report-path reports/custom.json
        """
    )
    
    # Options de sÃ©lection des tests
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--unit-only", action="store_true",
                      help="ExÃ©cuter seulement les tests unitaires")
    group.add_argument("--integration", action="store_true", 
                      help="ExÃ©cuter seulement les tests d'intÃ©gration")
    group.add_argument("--validation", action="store_true",
                      help="ExÃ©cuter seulement la validation complÃ¨te")
    group.add_argument("--migration", action="store_true",
                      help="ExÃ©cuter seulement les tests de migration")
    group.add_argument("--all", action="store_true", default=True,
                      help="ExÃ©cuter tous les tests (dÃ©faut)")
    
    # Options de configuration
    parser.add_argument("--real-tweety", action="store_true",
                       help="Utiliser Tweety rÃ©el (nÃ©cessite JAR)")
    parser.add_argument("--report-path", type=str,
                       help="Chemin du rapport de sortie")
    
    args = parser.parse_args()
    
    # ExÃ©cution
    runner = FOLTestRunner(args)
    
    try:
        asyncio.run(runner.run_all_tests())
        
        # Code de sortie basÃ© sur le succÃ¨s
        overall_success = all(r.get("success", False) for r in runner.results.values())
        sys.exit(0 if overall_success else 1)
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Interruption utilisateur")
        sys.exit(130)
    except Exception as e:
        logger.error(f"âŒ Erreur fatale: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()