import argumentation_analysis.core.environment
ï»¿#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test du systÃ¨me de correction intelligente des erreurs modales avec feedback BNF
===============================================================================

Test complet du systÃ¨me de correction d'erreurs modales avec gÃ©nÃ©ration
de feedback BNF constructif pour guider les corrections.
"""

import asyncio
import sys
from pathlib import Path
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional

# Ajout du rÃ©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent))

from argumentation_analysis.utils.tweety_error_analyzer import TweetyErrorAnalyzer, analyze_tweety_error


class ModalCorrectionTester:
    """
    Testeur pour le systÃ¨me de correction intelligente des erreurs modales.
    
    Teste la capacitÃ© du systÃ¨me Ã  dÃ©tecter et corriger les erreurs
    modales avec gÃ©nÃ©ration de feedback BNF appropriÃ©.
    """
    
    def __init__(self):
        """Initialise le testeur."""
        self.logger = logging.getLogger(__name__)
        self.setup_logging()
        
        self.analyzer = TweetyErrorAnalyzer()
        self.test_results = []
        
        # Erreurs de test
        self.test_errors = [
            {
                'type': 'syntax_error',
                'message': 'syntax error at token "rule"',
                'expected_bnf': ['rule ::= head \':-\' body \'.\'']
            },
            {
                'type': 'atom_error', 
                'message': 'atom "undefined_predicate" not defined',
                'expected_bnf': ['atom ::= predicate \'(\' terms \')\'']
            },
            {
                'type': 'variable_error',
                'message': 'singleton variable X in rule',
                'expected_bnf': ['variable ::= uppercase_identifier']
            },
            {
                'type': 'constraint_error',
                'message': 'integrity constraint violated',
                'expected_bnf': ['constraint ::= \':-\' body \'.\'']
            }
        ]
        
        self.logger.info("Testeur de correction modale initialisÃ©")
    
    def setup_logging(self):
        """Configure le logging."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
    
    def test_error_detection(self) -> Dict[str, Any]:
        """
        Test la dÃ©tection des types d'erreurs.
        
        Returns:
            RÃ©sultats du test de dÃ©tection
        """
        print("ğŸ” Test de dÃ©tection des types d'erreurs...")
        
        results = {
            'total_tests': len(self.test_errors),
            'successful_detections': 0,
            'failed_detections': 0,
            'details': []
        }
        
        for i, test_case in enumerate(self.test_errors, 1):
            print(f"  Test {i}: {test_case['type']}")
            
            try:
                feedback = self.analyzer.analyze_error(test_case['message'])
                detected_type = feedback.error_type
                
                if detected_type == test_case['type']:
                    print(f"    âœ… Type dÃ©tectÃ© correctement: {detected_type}")
                    results['successful_detections'] += 1
                    status = 'success'
                else:
                    print(f"    âŒ Type incorrect: attendu {test_case['type']}, obtenu {detected_type}")
                    results['failed_detections'] += 1
                    status = 'failed'
                
                results['details'].append({
                    'test_case': test_case,
                    'detected_type': detected_type,
                    'status': status,
                    'feedback': feedback
                })
                
            except Exception as e:
                print(f"    ğŸ’¥ Erreur: {e}")
                results['failed_detections'] += 1
                results['details'].append({
                    'test_case': test_case,
                    'error': str(e),
                    'status': 'error'
                })
        
        detection_rate = results['successful_detections'] / results['total_tests'] * 100
        print(f"ğŸ“Š Taux de dÃ©tection: {detection_rate:.1f}%")
        
        return results
    
    def test_bnf_generation(self) -> Dict[str, Any]:
        """
        Test la gÃ©nÃ©ration de rÃ¨gles BNF.
        
        Returns:
            RÃ©sultats du test de gÃ©nÃ©ration BNF
        """
        print("\nğŸ“ Test de gÃ©nÃ©ration des rÃ¨gles BNF...")
        
        results = {
            'total_tests': len(self.test_errors),
            'successful_generations': 0,
            'failed_generations': 0,
            'details': []
        }
        
        for i, test_case in enumerate(self.test_errors, 1):
            print(f"  Test {i}: {test_case['type']}")
            
            try:
                feedback = self.analyzer.analyze_error(test_case['message'])
                generated_rules = feedback.bnf_rules
                
                if generated_rules and len(generated_rules) > 0:
                    print(f"    âœ… {len(generated_rules)} rÃ¨gles BNF gÃ©nÃ©rÃ©es")
                    results['successful_generations'] += 1
                    status = 'success'
                    
                    # VÃ©rifier si une rÃ¨gle attendue est prÃ©sente
                    expected_found = any(
                        expected in rule 
                        for expected in test_case['expected_bnf']
                        for rule in generated_rules
                    )
                    
                    if expected_found:
                        print(f"    âœ… RÃ¨gle attendue trouvÃ©e")
                    else:
                        print(f"    âš ï¸  RÃ¨gle attendue non trouvÃ©e")
                        
                else:
                    print(f"    âŒ Aucune rÃ¨gle BNF gÃ©nÃ©rÃ©e")
                    results['failed_generations'] += 1
                    status = 'failed'
                
                results['details'].append({
                    'test_case': test_case,
                    'generated_rules': generated_rules,
                    'rule_count': len(generated_rules) if generated_rules else 0,
                    'status': status
                })
                
            except Exception as e:
                print(f"    ğŸ’¥ Erreur: {e}")
                results['failed_generations'] += 1
                results['details'].append({
                    'test_case': test_case,
                    'error': str(e),
                    'status': 'error'
                })
        
        generation_rate = results['successful_generations'] / results['total_tests'] * 100
        print(f"ğŸ“Š Taux de gÃ©nÃ©ration BNF: {generation_rate:.1f}%")
        
        return results
    
    def test_feedback_formatting(self) -> Dict[str, Any]:
        """
        Test le formatage du feedback.
        
        Returns:
            RÃ©sultats du test de formatage
        """
        print("\nğŸ’¬ Test de formatage du feedback...")
        
        results = {
            'total_tests': len(self.test_errors),
            'successful_formats': 0,
            'failed_formats': 0,
            'details': []
        }
        
        for i, test_case in enumerate(self.test_errors, 1):
            print(f"  Test {i}: {test_case['type']}")
            
            try:
                # Utiliser la fonction utilitaire
                formatted_feedback = analyze_tweety_error(
                    test_case['message'],
                    attempt_number=i,
                    context={'test': True}
                )
                
                # VÃ©rifier que le feedback contient les sections attendues
                required_sections = [
                    'Analyse d\'erreur Tweety',
                    'Type d\'erreur dÃ©tectÃ©',
                    'RÃ¨gles BNF pertinentes',
                    'Suggestions de correction',
                    'Exemple de correction'
                ]
                
                sections_found = sum(
                    1 for section in required_sections 
                    if section in formatted_feedback
                )
                
                if sections_found >= 4:  # Au moins 4/5 sections
                    print(f"    âœ… Feedback bien formatÃ© ({sections_found}/5 sections)")
                    results['successful_formats'] += 1
                    status = 'success'
                else:
                    print(f"    âŒ Feedback mal formatÃ© ({sections_found}/5 sections)")
                    results['failed_formats'] += 1
                    status = 'failed'
                
                results['details'].append({
                    'test_case': test_case,
                    'formatted_feedback': formatted_feedback,
                    'sections_found': sections_found,
                    'total_sections': len(required_sections),
                    'status': status
                })
                
            except Exception as e:
                print(f"    ğŸ’¥ Erreur: {e}")
                results['failed_formats'] += 1
                results['details'].append({
                    'test_case': test_case,
                    'error': str(e),
                    'status': 'error'
                })
        
        format_rate = results['successful_formats'] / results['total_tests'] * 100
        print(f"ğŸ“Š Taux de formatage rÃ©ussi: {format_rate:.1f}%")
        
        return results
    
    def test_confidence_calculation(self) -> Dict[str, Any]:
        """
        Test le calcul de confiance.
        
        Returns:
            RÃ©sultats du test de confiance
        """
        print("\nğŸ¯ Test de calcul de confiance...")
        
        results = {
            'total_tests': len(self.test_errors),
            'valid_confidences': 0,
            'invalid_confidences': 0,
            'average_confidence': 0,
            'details': []
        }
        
        total_confidence = 0
        
        for i, test_case in enumerate(self.test_errors, 1):
            print(f"  Test {i}: {test_case['type']}")
            
            try:
                feedback = self.analyzer.analyze_error(test_case['message'])
                confidence = feedback.confidence
                
                if 0 <= confidence <= 1:
                    print(f"    âœ… Confiance valide: {confidence:.1%}")
                    results['valid_confidences'] += 1
                    total_confidence += confidence
                    status = 'success'
                else:
                    print(f"    âŒ Confiance invalide: {confidence}")
                    results['invalid_confidences'] += 1
                    status = 'failed'
                
                results['details'].append({
                    'test_case': test_case,
                    'confidence': confidence,
                    'status': status
                })
                
            except Exception as e:
                print(f"    ğŸ’¥ Erreur: {e}")
                results['invalid_confidences'] += 1
                results['details'].append({
                    'test_case': test_case,
                    'error': str(e),
                    'status': 'error'
                })
        
        if results['valid_confidences'] > 0:
            results['average_confidence'] = total_confidence / results['valid_confidences']
        
        confidence_rate = results['valid_confidences'] / results['total_tests'] * 100
        print(f"ğŸ“Š Taux de confiance valide: {confidence_rate:.1f}%")
        print(f"ğŸ“Š Confiance moyenne: {results['average_confidence']:.1%}")
        
        return results
    
    def generate_report(self, all_results: Dict[str, Any]) -> str:
        """
        GÃ©nÃ¨re un rapport complet des tests.
        
        Args:
            all_results: Tous les rÃ©sultats de tests
            
        Returns:
            Rapport formatÃ©
        """
        report = f"""
RAPPORT DE TEST - SYSTÃˆME DE CORRECTION INTELLIGENTE DES ERREURS MODALES
========================================================================
GÃ©nÃ©rÃ© le: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

RÃ‰SUMÃ‰ EXÃ‰CUTIF
===============
Ce rapport prÃ©sente les rÃ©sultats des tests du systÃ¨me de correction
intelligente des erreurs modales avec feedback BNF constructif.

"""
        
        for test_name, results in all_results.items():
            if 'total_tests' in results:
                successful = results.get('successful_detections', 0) + \
                           results.get('successful_generations', 0) + \
                           results.get('successful_formats', 0) + \
                           results.get('valid_confidences', 0)
                
                total = results['total_tests'] * len([k for k in results.keys() if 'successful' in k or 'valid' in k])
                if total == 0:
                    total = results['total_tests']
                
                success_rate = (successful / total * 100) if total > 0 else 0
                
                report += f"\n{test_name.upper().replace('_', ' ')}\n"
                report += "=" * len(test_name) + "\n"
                report += f"â€¢ Tests exÃ©cutÃ©s: {results['total_tests']}\n"
                report += f"â€¢ Taux de succÃ¨s: {success_rate:.1f}%\n"
                
                if 'average_confidence' in results:
                    report += f"â€¢ Confiance moyenne: {results['average_confidence']:.1%}\n"
        
        # Recommandations
        report += f"\nRECOMMANDATIONS\n"
        report += "===============\n"
        
        overall_success = all(
            any(k.startswith('successful') and v > 0 for k, v in result.items() if isinstance(v, int))
            for result in all_results.values() if 'total_tests' in result
        )
        
        if overall_success:
            report += "ğŸ‰ Le systÃ¨me de correction intelligente fonctionne correctement.\n"
            report += "âœ… Tous les composants principaux sont opÃ©rationnels.\n"
        else:
            report += "âš ï¸  Certains composants nÃ©cessitent des amÃ©liorations.\n"
            report += "ğŸ”§ VÃ©rifier la configuration et les patterns d'erreur.\n"
        
        report += f"\nğŸ“‹ PROCHAINES Ã‰TAPES\n"
        report += "====================\n"
        report += "1. IntÃ©grer le systÃ¨me dans le pipeline principal\n"
        report += "2. Tester avec des erreurs rÃ©elles de production\n"
        report += "3. Affiner les patterns de dÃ©tection\n"
        report += "4. Enrichir les rÃ¨gles BNF et corrections\n"
        
        return report
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """
        ExÃ©cute tous les tests.
        
        Returns:
            RÃ©sultats complets de tous les tests
        """
        print("ğŸ§ª TESTS DU SYSTÃˆME DE CORRECTION INTELLIGENTE DES ERREURS MODALES")
        print("=" * 75)
        print("Ce script teste le nouveau systÃ¨me de feedback BNF pour la correction")
        print("d'erreurs modales dans l'analyse d'argumentation.")
        print()
        
        all_results = {}
        
        # Test 1: DÃ©tection des erreurs
        all_results['detection'] = self.test_error_detection()
        
        # Test 2: GÃ©nÃ©ration BNF
        all_results['bnf_generation'] = self.test_bnf_generation()
        
        # Test 3: Formatage du feedback
        all_results['feedback_formatting'] = self.test_feedback_formatting()
        
        # Test 4: Calcul de confiance
        all_results['confidence_calculation'] = self.test_confidence_calculation()
        
        return all_results


async def main():
    """Fonction principale."""
    tester = ModalCorrectionTester()
    
    try:
        # ExÃ©cuter tous les tests
        results = await tester.run_all_tests()
        
        # GÃ©nÃ©rer et afficher le rapport
        report = tester.generate_report(results)
        print("\n" + report)
        
        # Sauvegarder le rapport
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"modal_correction_test_report_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ’¾ Rapport sauvegardÃ©: {report_file}")
        
        # DÃ©terminer le succÃ¨s global
        success_count = sum(
            1 for result in results.values()
            if 'total_tests' in result and any(
                k.startswith('successful') and v > 0 
                for k, v in result.items() if isinstance(v, int)
            )
        )
        
        total_test_categories = len(results)
        success_rate = success_count / total_test_categories * 100 if total_test_categories > 0 else 0
        
        print(f"\nğŸ RÃ‰SULTAT GLOBAL")
        print("=" * 50)
        print(f"ğŸ“Š CatÃ©gories de test rÃ©ussies: {success_count}/{total_test_categories}")
        print(f"ğŸ“ˆ Taux de succÃ¨s global: {success_rate:.1f}%")
        
        if success_rate >= 75:
            print("ğŸ‰ Tests rÃ©ussis! Le systÃ¨me est opÃ©rationnel.")
            return True
        else:
            print("âš ï¸  Tests partiellement rÃ©ussis. AmÃ©liorations nÃ©cessaires.")
            return False
        
    except Exception as e:
        print(f"ğŸ’¥ Erreur fatale lors des tests: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
