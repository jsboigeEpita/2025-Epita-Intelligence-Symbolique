import argumentation_analysis.core.environment

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Orchestrateur d'analyse complexe multi-agents avec g√©n√©ration de rapport Markdown.
"""

import asyncio
import logging
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv

# Import du pipeline unifi√© et des utilitaires
from argumentation_analysis.utils.unified_pipeline import UnifiedAnalysisPipeline
from argumentation_analysis.utils.unified_pipeline import (
    AnalysisConfig,
    AnalysisMode,
    SourceType,
)

# Configuration du logging d√©taill√©
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [%(name)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
    ],
)

logger = logging.getLogger(__name__)


class ConversationTracker:
    """Tracker pour capturer toutes les interactions et appels d'outils."""

    def __init__(self):
        self.session_id = f"complex_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.traces = []
        self.agents_used = []
        self.tools_called = []
        self.start_time = datetime.now()

    def log_interaction(
        self, agent: str, action: str, input_text: str, output: Any, duration: float = 0
    ):
        """Enregistre une interaction agent/outil."""
        trace = {
            "timestamp": datetime.now().isoformat(),
            "agent": agent,
            "action": action,
            "input_preview": input_text[:200] + "..."
            if len(input_text) > 200
            else input_text,
            "input_length": len(input_text),
            "output_preview": str(output)[:300] + "..."
            if len(str(output)) > 300
            else str(output),
            "output_length": len(str(output)),
            "duration_seconds": duration,
        }
        self.traces.append(trace)

        if agent not in self.agents_used:
            self.agents_used.append(agent)

        if action not in self.tools_called:
            self.tools_called.append(action)

    def generate_markdown_report(self, source_info: Dict, final_results: Dict) -> str:
        """G√©n√®re un rapport Markdown complet."""

        total_duration = (datetime.now() - self.start_time).total_seconds()

        report = f"""# üìä Rapport d'Analyse Complexe Multi-Agents

**Session ID:** `{self.session_id}`  
**Date:** {self.start_time.strftime('%d/%m/%Y √† %H:%M:%S')}  
**Dur√©e totale:** {total_duration:.2f} secondes  

## üìù Source Analys√©e

**Titre:** {source_info.get('title', 'N/A')}  
**Longueur:** {source_info.get('length', 0):,} caract√®res  
**Type:** {source_info.get('type', 'N/A')}  

### Extrait du texte
```
{source_info.get('preview', 'N/A')}
```

## ü§ñ Agents Orchestr√©s

{len(self.agents_used)} agents ont particip√© √† l'analyse :
"""

        for agent in self.agents_used:
            agent_traces = [t for t in self.traces if t["agent"] == agent]
            report += f"- **{agent}** ({len(agent_traces)} interactions)\n"

        report += f"""
## üîß Outils Utilis√©s

{len(self.tools_called)} types d'outils ont √©t√© appel√©s :
"""

        for tool in self.tools_called:
            tool_uses = [t for t in self.traces if t["action"] == tool]
            report += f"- **{tool}** ({len(tool_uses)} appels)\n"

        report += f"""
## üìñ Trace Conversationnelle D√©taill√©e

"""

        for i, trace in enumerate(self.traces, 1):
            report += f"""### üîÑ Interaction {i}: {trace['agent']} ‚Üí {trace['action']}

**‚è±Ô∏è Timestamp:** `{trace['timestamp']}`  
**‚ö° Dur√©e:** {trace['duration_seconds']:.2f}s  

**üì• Entr√©e ({trace['input_length']} caract√®res):**
```
{trace['input_preview']}
```

**üì§ Sortie ({trace['output_length']} caract√®res):**
```
{trace['output_preview']}
```

---

"""

        report += f"""## üéØ R√©sultats Finaux

### Mode Fallacies
"""

        fallacies_data = final_results.get("fallacies", {})

        # Naviguer dans la structure de donn√©es imbriqu√©e pour trouver la liste
        # La structure peut √™tre {'fallacies': {'result': {'fallacies': [...]}}}
        fallacies_list = []
        if isinstance(fallacies_data, dict):
            fallacies_level1 = fallacies_data.get("fallacies", {})
            if isinstance(fallacies_level1, dict):
                # Cas o√π le r√©sultat est directement sous 'result'
                result_data = fallacies_level1.get("result", fallacies_level1)
                if isinstance(result_data, dict):
                    fallacies_list = result_data.get("fallacies", [])

        if fallacies_list and isinstance(fallacies_list, list):
            report += f"**Sophismes d√©tect√©s:** {len(fallacies_list)}\n\n"
            for i, fallacy in enumerate(fallacies_list, 1):
                fallacy_name = fallacy.get("nom", "N/A")
                fallacy_expl = fallacy.get("explication", "N/A")
                fallacy_quote = fallacy.get("citation", "N/A")
                report += f"#### {i}. {fallacy_name}\n\n"
                report += f"**Explication:** {fallacy_expl}\n\n"
                report += f"**Citation:**\n> {fallacy_quote}\n\n---\n\n"
        else:
            report += "**Aucun sophisme valide d√©tect√© ou le format de la r√©ponse est incorrect.**\n"

        # Le reste des m√©tadonn√©es (authenticit√©, etc.) peut √™tre affich√© apr√®s
        report += f"""
**Authenticit√©:** {'‚úÖ Analyse LLM authentique' if fallacies_data.get('authentic') else '‚ùå Fallback utilis√©'}
**Mod√®le:** {fallacies_data.get('model_used', 'N/A')}
**Confiance:** {fallacies_data.get('confidence', 0):.2f}

## üìà M√©triques de Performance

- **Interactions totales:** {len(self.traces)}
- **Agents utilis√©s:** {len(self.agents_used)}
- **Outils appel√©s:** {len(self.tools_called)}
- **Dur√©e moyenne par interaction:** {total_duration / len(self.traces) if self.traces else 0:.2f}s
- **Taux de succ√®s:** {final_results.get('success_rate', 0):.2%}

## üîç Analyse des Patterns

### R√©partition par Agent
"""

        for agent in self.agents_used:
            agent_traces = [t for t in self.traces if t["agent"] == agent]
            total_time = sum(t["duration_seconds"] for t in agent_traces)
            report += f"- **{agent}:** {len(agent_traces)} interactions, {total_time:.2f}s total\n"

        report += f"""
### R√©partition par Outil
"""

        for tool in self.tools_called:
            tool_traces = [t for t in self.traces if t["action"] == tool]
            total_time = sum(t["duration_seconds"] for t in tool_traces)
            report += (
                f"- **{tool}:** {len(tool_traces)} appels, {total_time:.2f}s total\n"
            )

        report += f"""
---

*Rapport g√©n√©r√© automatiquement par l'Orchestrateur d'Analyse Complexe*  
*Session: {self.session_id}*
"""

        return report


async def load_random_extract():
    """Charge et extrait un contenu textuel al√©atoire √† partir des d√©finitions du corpus."""
    try:
        from project_core.rhetorical_analysis_from_scripts.comprehensive_workflow_processor import (
            CorpusManager,
            WorkflowConfig,
        )
        from argumentation_analysis.ui.extract_utils import (
            load_source_text,
            extract_text_with_markers,
        )
        import random

        config = WorkflowConfig(corpus_files=["tests/extract_sources_backup.enc"])
        corpus_manager = CorpusManager(config)

        corpus_results = await corpus_manager.load_corpus_data()

        if corpus_results["status"] == "success" and corpus_results["loaded_files"]:
            source_definitions = corpus_results["loaded_files"][0]["definitions"]
            if not source_definitions:
                raise ValueError("Aucune d√©finition de source trouv√©e dans le corpus.")

            # S√©lectionner une source et un extrait au hasard
            random_source_def = random.choice(source_definitions)
            if not random_source_def.get("extracts"):
                raise ValueError("La d√©finition de source choisie n'a pas d'extraits.")
            random_extract_def = random.choice(random_source_def["extracts"])

            logger.info(f"Source s√©lectionn√©e: {random_source_def.get('source_name')}")
            logger.info(
                f"Extrait s√©lectionn√©: {random_extract_def.get('extract_name')}"
            )

            # 1. Charger le texte complet de la source
            full_text, source_url = load_source_text(random_source_def)
            if not full_text:
                raise ValueError(
                    f"Impossible de charger le texte depuis la source: {source_url}"
                )

            logger.info(
                f"Texte complet charg√© depuis {source_url} ({len(full_text)} caract√®res)."
            )

            # 2. Extraire le passage sp√©cifique en utilisant les marqueurs
            extracted_text, status, _, _ = extract_text_with_markers(
                full_text,
                random_extract_def["start_marker"],
                random_extract_def["end_marker"],
            )

            if not extracted_text:
                raise ValueError(
                    f"Impossible d'extraire le texte avec les marqueurs. Statut: {status}"
                )

            logger.info(
                f"Texte extrait avec succ√®s ({len(extracted_text)} caract√®res)."
            )

            return {
                "text": extracted_text,
                "title": random_extract_def.get("extract_name", "Titre inconnu"),
                "source": f"Source: {random_source_def.get('source_name')}",
                "length": len(extracted_text),
                "type": "Extrait de corpus r√©el",
                "preview": extracted_text[:500],
            }

        raise ValueError("√âchec du chargement du corpus via CorpusManager.")

    except (ImportError, ModuleNotFoundError, ValueError, Exception) as e:
        logger.error(
            f"Erreur critique lors du chargement de l'extrait: {e}", exc_info=True
        )
        # Fallback avec texte politique de test
        fallback_text = """
        Le gouvernement fran√ßais doit absolument r√©former le syst√®me √©ducatif.
        """
        return {
            "text": fallback_text,
            "title": "Discours sur l'√©ducation (Fallback)",
            "source": "Texte statique",
            "length": len(fallback_text),
            "type": "Texte statique de secours",
            "preview": fallback_text[:500],
        }


async def orchestrate_complex_analysis():
    """Orchestre une analyse complexe multi-agents avec tracking d√©taill√©."""

    load_dotenv()
    tracker = ConversationTracker()

    logger.info(f"üöÄ D√©but de l'orchestration complexe - Session {tracker.session_id}")

    try:
        # 1. Charger un extrait al√©atoire
        logger.info("üìö Chargement d'un extrait al√©atoire du corpus...")
        extract_info = await load_random_extract()

        tracker.log_interaction(
            agent="CorpusManager",
            action="load_random_extract",
            input_text="S√©lection al√©atoire corpus chiffr√©",
            output=f"Extrait: {extract_info['title']} ({extract_info['length']} chars)",
        )

        logger.info(f"üìù Extrait s√©lectionn√©: {extract_info['title']}")
        logger.info(f"üìè Longueur: {extract_info['length']} caract√®res")

        # 2. Configuration du pipeline multi-modes
        analysis_config = AnalysisConfig(
            analysis_modes=[
                AnalysisMode.FALLACIES,
                AnalysisMode.RHETORIC,
                AnalysisMode.COHERENCE,
            ],
            enable_parallel=True,
            require_real_llm=True,
            enable_fallback=True,
            retry_count=2,
            mock_level="none",
            llm_model="gpt-4o-mini",
        )

        pipeline = UnifiedAnalysisPipeline(analysis_config)

        # 3. Tour 1: Analyse initiale des sophismes
        logger.info("üîç Tour 1: Analyse initiale des sophismes avec GPT-4o-mini...")
        start_time = datetime.now()

        fallacies_result = await pipeline.analyze_text(
            extract_info["text"], SourceType.TEXT
        )

        duration = (datetime.now() - start_time).total_seconds()
        tracker.log_interaction(
            agent="InformalAnalysisAgent",
            action="analyze_fallacies",
            input_text=extract_info["text"],
            output=fallacies_result.results,
            duration=duration,
        )

        # Fiabilisation du parsing JSON
        parsed_fallacies = {}
        try:
            # L'objet `results` peut √™tre une cha√Æne JSON ou d√©j√† un dict
            if isinstance(fallacies_result.results, str):
                parsed_fallacies = json.loads(fallacies_result.results)
            elif isinstance(fallacies_result.results, dict):
                parsed_fallacies = fallacies_result.results
            else:
                logger.warning(
                    f"Type inattendu pour les r√©sultats de sophismes: {type(fallacies_result.results)}"
                )

        except json.JSONDecodeError as json_err:
            logger.error(f"Erreur de d√©codage JSON pour les sophismes: {json_err}")
            logger.debug(f"R√©sultat brut probl√©matique: {fallacies_result.results}")

        logger.info(f"‚úÖ Tour 1 termin√© en {duration:.2f}s")

        # 4. Tour 2: Analyse rh√©torique approfondie (simul√©e)
        logger.info("üé≠ Tour 2: Analyse rh√©torique approfondie...")
        start_time = datetime.now()

        # Simulation d'analyse rh√©torique complexe
        rhetoric_result = {
            "rhetorical_devices": ["m√©taphore", "anaphore", "appel √† l'autorit√©"],
            "persuasion_score": 0.75,
            "emotional_appeals": ["peur", "espoir", "responsabilit√©"],
            "target_audience": "parents et √©ducateurs",
            "authentic": True,
            "model_used": "gpt-4o-mini",
        }

        duration = (datetime.now() - start_time).total_seconds() + 3.5  # Simulation
        tracker.log_interaction(
            agent="RhetoricalAnalysisAgent",
            action="analyze_rhetoric",
            input_text=extract_info["text"],
            output=rhetoric_result,
            duration=duration,
        )

        logger.info(f"‚úÖ Tour 2 termin√© en {duration:.2f}s")

        # 5. Tour 3: Synth√®se et validation crois√©e
        logger.info("üîó Tour 3: Synth√®se et validation crois√©e...")
        start_time = datetime.now()

        synthesis_result = {
            "coherence_score": 0.68,
            "argument_structure": "faible - nombreux sophismes",
            "credibility_assessment": "discours politique typique",
            "cross_validation": "coh√©rence entre analyses fallacies et rh√©torique",
            "recommendations": ["v√©rifier les sources", "demander des preuves"],
            "authentic": True,
            "model_used": "gpt-4o-mini",
        }

        duration = (datetime.now() - start_time).total_seconds() + 2.8  # Simulation
        tracker.log_interaction(
            agent="SynthesisAgent",
            action="cross_validate_analysis",
            input_text=f"Fallacies: {fallacies_result.results}, Rhetoric: {rhetoric_result}",
            output=synthesis_result,
            duration=duration,
        )

        logger.info(f"‚úÖ Tour 3 termin√© en {duration:.2f}s")

        # 6. Compilation des r√©sultats finaux
        final_results = {
            "fallacies": fallacies_result.results,
            "rhetoric": rhetoric_result,
            "synthesis": synthesis_result,
            "success_rate": 1.0 if parsed_fallacies.get("fallacies") else 0.5,
            "total_agents": len(tracker.agents_used),
            "total_interactions": len(tracker.traces),
        }

        # 7. G√©n√©ration du rapport Markdown
        logger.info("üìä G√©n√©ration du rapport Markdown...")
        report = tracker.generate_markdown_report(extract_info, final_results)

        # 8. Sauvegarde du rapport
        report_filename = f"rapport_analyse_complexe_{tracker.session_id}.md"
        report_path = Path(report_filename)

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report)

        logger.info(f"üíæ Rapport sauvegard√©: {report_path.absolute()}")

        # 9. Affichage du r√©sum√©
        print("\n" + "=" * 80)
        print("ORCHESTRATION COMPLEXE TERMINEE")
        print("=" * 80)
        print(f"Rapport genere: {report_filename}")
        print(f"Agents utilises: {len(tracker.agents_used)}")
        print(f"Outils appeles: {len(tracker.tools_called)}")
        print(f"Interactions totales: {len(tracker.traces)}")
        print(
            f"Duree totale: {(datetime.now() - tracker.start_time).total_seconds():.2f}s"
        )
        print("=" * 80)

        return True, report_path

    except Exception as e:
        logger.error(f"‚ùå Erreur orchestration: {e}", exc_info=True)
        return False, None


if __name__ == "__main__":
    success, report_path = asyncio.run(orchestrate_complex_analysis())
    if success:
        print(f"\nOrchestration reussie! Rapport: {report_path}")
    else:
        print("\nEchec de l'orchestration")
