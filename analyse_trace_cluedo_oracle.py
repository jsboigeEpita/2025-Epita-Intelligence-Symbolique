#!/usr/bin/env python3
"""
ANALYSE DE LA TRACE ACTUELLE SHERLOCK-WATSON-MORIARTY

Script d'analyse pour Ã©valuer la qualitÃ© des conversations entre les 3 agents
et identifier les axes d'amÃ©lioration vers une "trace idÃ©ale".

Objectifs:
1. ExÃ©cuter une session complÃ¨te avec workflow 3-agents
2. Capturer intÃ©gralement la trace conversationnelle
3. Analyser la qualitÃ© sur plusieurs dimensions
4. DÃ©finir les critÃ¨res de "trace idÃ©ale"
5. Proposer un plan d'optimisation incrÃ©mentale
"""

import asyncio
import json
import logging
import sys
import traceback
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict

# Imports de l'infrastructure Oracle
from argumentation_analysis.orchestration.cluedo_extended_orchestrator import (
    CluedoExtendedOrchestrator, run_cluedo_oracle_game
)
from argumentation_analysis.core.cluedo_oracle_state import CluedoOracleState
from semantic_kernel import Kernel

# Configuration du logging pour une trace dÃ©taillÃ©e
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('trace_analysis.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

# Forcer l'encodage UTF-8 pour Ã©viter les problÃ¨mes avec les emojis
import os
os.environ['PYTHONIOENCODING'] = 'utf-8'
logger = logging.getLogger(__name__)


@dataclass
class ConversationalMetrics:
    """MÃ©triques d'Ã©valuation de la qualitÃ© conversationnelle."""
    naturalite_score: float = 0.0
    pertinence_score: float = 0.0
    progression_logique_score: float = 0.0
    personnalite_distincte_score: float = 0.0
    fluidite_transitions_score: float = 0.0
    dosage_revelations_score: float = 0.0
    satisfaction_resolution_score: float = 0.0
    
    def moyenne_globale(self) -> float:
        """Calcule la moyenne globale des scores."""
        scores = [
            self.naturalite_score, self.pertinence_score, self.progression_logique_score,
            self.personnalite_distincte_score, self.fluidite_transitions_score,
            self.dosage_revelations_score, self.satisfaction_resolution_score
        ]
        return sum(scores) / len(scores)


@dataclass
class ConversationAnalysis:
    """Analyse complÃ¨te d'une conversation entre agents."""
    total_messages: int = 0
    messages_par_agent: Dict[str, int] = None
    longueur_moyenne_messages: Dict[str, float] = None
    mots_cles_detectes: Dict[str, List[str]] = None
    transitions_abruptes: List[Dict[str, Any]] = None
    repetitions_detectees: List[Dict[str, Any]] = None
    progression_deductive: List[Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.messages_par_agent is None:
            self.messages_par_agent = {}
        if self.longueur_moyenne_messages is None:
            self.longueur_moyenne_messages = {}
        if self.mots_cles_detectes is None:
            self.mots_cles_detectes = {}
        if self.transitions_abruptes is None:
            self.transitions_abruptes = []
        if self.repetitions_detectees is None:
            self.repetitions_detectees = []
        if self.progression_deductive is None:
            self.progression_deductive = []


class TraceQualityAnalyzer:
    """
    Analyseur de qualitÃ© des traces conversationnelles pour le workflow 3-agents.
    """
    
    def __init__(self):
        self.conversation_history: List[Dict[str, Any]] = []
        self.agent_personalities = {
            "Sherlock": ["mÃ©thodique", "dÃ©ductif", "leader", "incisif", "observateur"],
            "Watson": ["logique", "analytique", "assistant", "rigoureux", "technique"],
            "Moriarty": ["mystÃ©rieux", "rÃ©vÃ©lateur", "stratÃ©gique", "manipulateur", "oracle"]
        }
        self.conversation_patterns = {
            "suggestions": [],
            "validations": [], 
            "revelations": [],
            "transitions": []
        }
    
    def capture_conversation(self, workflow_result: Dict[str, Any]) -> None:
        """Capture et structure l'historique conversationnel."""
        logger.info("ðŸ” Capture de l'historique conversationnel...")
        
        self.conversation_history = workflow_result.get("conversation_history", [])
        self.oracle_stats = workflow_result.get("oracle_statistics", {})
        self.solution_analysis = workflow_result.get("solution_analysis", {})
        
        logger.info(f"âœ… {len(self.conversation_history)} messages capturÃ©s")
        
        # Analyse des patterns conversationnels
        self._extract_conversation_patterns()
    
    def _extract_conversation_patterns(self) -> None:
        """Extrait les patterns conversationnels clÃ©s."""
        logger.info("ðŸ” Extraction des patterns conversationnels...")
        
        for i, message in enumerate(self.conversation_history):
            sender = message.get("sender", "Unknown")
            content = message.get("message", "").lower()
            
            # DÃ©tection des suggestions
            if "suggÃ©r" in content or "suspect" in content and "arme" in content:
                self.conversation_patterns["suggestions"].append({
                    "index": i,
                    "sender": sender,
                    "content_preview": message.get("message", "")[:100]
                })
            
            # DÃ©tection des validations logiques
            if "valide" in content or "logique" in content or "formula" in content:
                self.conversation_patterns["validations"].append({
                    "index": i,
                    "sender": sender,
                    "content_preview": message.get("message", "")[:100]
                })
            
            # DÃ©tection des rÃ©vÃ©lations Oracle
            if "rÃ©vÃ¨le" in content or "carte" in content or "rÃ©futation" in content:
                self.conversation_patterns["revelations"].append({
                    "index": i,
                    "sender": sender,
                    "content_preview": message.get("message", "")[:100]
                })
            
            # DÃ©tection des transitions abruptes
            if i > 0:
                prev_sender = self.conversation_history[i-1].get("sender", "")
                if sender != prev_sender:
                    self.conversation_patterns["transitions"].append({
                        "from": prev_sender,
                        "to": sender,
                        "index": i,
                        "prev_content": self.conversation_history[i-1].get("message", "")[:50],
                        "curr_content": message.get("message", "")[:50]
                    })
    
    def analyze_conversational_quality(self) -> ConversationalMetrics:
        """Analyse la qualitÃ© conversationnelle selon les critÃ¨res dÃ©finis."""
        logger.info("ðŸ“Š Analyse de la qualitÃ© conversationnelle...")
        
        metrics = ConversationalMetrics()
        
        # 1. NaturalitÃ© du dialogue
        metrics.naturalite_score = self._evaluate_naturalness()
        
        # 2. Pertinence des interventions
        metrics.pertinence_score = self._evaluate_relevance()
        
        # 3. Logique de progression
        metrics.progression_logique_score = self._evaluate_logical_progression()
        
        # 4. PersonnalitÃ©s distinctes
        metrics.personnalite_distincte_score = self._evaluate_personality_distinction()
        
        # 5. FluiditÃ© des transitions
        metrics.fluidite_transitions_score = self._evaluate_transition_fluidity()
        
        # 6. Dosage des rÃ©vÃ©lations Moriarty
        metrics.dosage_revelations_score = self._evaluate_revelation_dosage()
        
        # 7. Satisfaction de la rÃ©solution
        metrics.satisfaction_resolution_score = self._evaluate_resolution_satisfaction()
        
        logger.info(f"ðŸ“Š Score global: {metrics.moyenne_globale():.2f}/10")
        return metrics
    
    def _evaluate_naturalness(self) -> float:
        """Ã‰value la naturalitÃ© du dialogue (0-10)."""
        score = 5.0  # Score de base
        
        # PÃ©nalitÃ©s pour rÃ©pÃ©titions
        repetitions = self._detect_repetitions()
        score -= min(len(repetitions) * 0.5, 2.0)
        
        # Bonus pour variÃ©tÃ© du vocabulaire
        unique_words = self._count_unique_words()
        if unique_words > 100:
            score += 1.0
        
        # Bonus pour longueur appropriÃ©e des messages
        avg_lengths = self._calculate_average_message_lengths()
        if all(20 <= length <= 200 for length in avg_lengths.values()):
            score += 1.0
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_relevance(self) -> float:
        """Ã‰value la pertinence des interventions (0-10)."""
        score = 5.0
        
        # VÃ©rifier que Sherlock mÃ¨ne l'enquÃªte
        sherlock_suggestions = len([p for p in self.conversation_patterns["suggestions"] 
                                  if p["sender"] == "Sherlock"])
        total_suggestions = len(self.conversation_patterns["suggestions"])
        
        if total_suggestions > 0:
            sherlock_leadership = sherlock_suggestions / total_suggestions
            score += sherlock_leadership * 3.0  # Max +3 si Sherlock fait toutes les suggestions
        
        # VÃ©rifier que Watson fait de la logique
        watson_logic = len([p for p in self.conversation_patterns["validations"] 
                          if p["sender"] == "Watson"])
        if watson_logic > 0:
            score += 1.0
        
        # VÃ©rifier que Moriarty rÃ©vÃ¨le des cartes
        moriarty_revelations = len([p for p in self.conversation_patterns["revelations"] 
                                  if p["sender"] == "Moriarty"])
        if moriarty_revelations > 0:
            score += 1.0
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_logical_progression(self) -> float:
        """Ã‰value la logique de progression de l'enquÃªte (0-10)."""
        score = 5.0
        
        # VÃ©rifier que l'enquÃªte progresse vers la solution
        if self.solution_analysis.get("success", False):
            score += 3.0
        elif self.solution_analysis.get("proposed_solution"):
            score += 1.5  # Tentative mÃªme si incorrecte
        
        # VÃ©rifier l'utilisation progressive des rÃ©vÃ©lations
        revelations_count = len(self.conversation_patterns["revelations"])
        if 2 <= revelations_count <= 5:  # Nombre optimal de rÃ©vÃ©lations
            score += 1.0
        
        # VÃ©rifier la structure cyclique Sherlock->Watson->Moriarty
        cycle_adherence = self._check_cycle_adherence()
        score += cycle_adherence * 1.0
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_personality_distinction(self) -> float:
        """Ã‰value la distinction des personnalitÃ©s (0-10)."""
        score = 5.0
        
        # Analyser les mots-clÃ©s par agent
        agent_keywords = self._extract_agent_keywords()
        
        for agent, keywords in agent_keywords.items():
            expected_keywords = self.agent_personalities.get(agent, [])
            matching_keywords = sum(1 for kw in expected_keywords 
                                  if any(kw in keyword.lower() for keyword in keywords))
            if matching_keywords > 0:
                score += 0.5  # Bonus par agent avec personnalitÃ© distincte
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_transition_fluidity(self) -> float:
        """Ã‰value la fluiditÃ© des transitions entre agents (0-10)."""
        score = 8.0  # Score de base Ã©levÃ© (transitions gÃ©nÃ©ralement fluides)
        
        # Analyser les transitions abruptes
        abrupt_transitions = 0
        for transition in self.conversation_patterns["transitions"]:
            # Transition abrupte si pas de rÃ©fÃ©rence au message prÃ©cÃ©dent
            curr_content = transition["curr_content"].lower()
            prev_content = transition["prev_content"].lower()
            
            # Simple heuristique: chercher des mots de liaison
            liaison_words = ["donc", "ainsi", "cependant", "de plus", "en effet", "par ailleurs"]
            has_liaison = any(word in curr_content for word in liaison_words)
            
            if not has_liaison and len(curr_content) > 20:
                abrupt_transitions += 1
        
        # PÃ©nalitÃ© pour transitions abruptes
        score -= abrupt_transitions * 0.5
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_revelation_dosage(self) -> float:
        """Ã‰value le dosage des rÃ©vÃ©lations Moriarty (0-10)."""
        score = 5.0
        
        revelations = self.conversation_patterns["revelations"]
        total_messages = len(self.conversation_history)
        
        if total_messages > 0:
            revelation_density = len(revelations) / total_messages
            
            # DensitÃ© optimale: 10-30% des messages
            if 0.1 <= revelation_density <= 0.3:
                score += 3.0
            elif 0.05 <= revelation_density <= 0.5:
                score += 1.5
            else:
                score -= 1.0
        
        # VÃ©rifier que les rÃ©vÃ©lations sont bien rÃ©parties
        if len(revelations) >= 2:
            # Calcul de la variance des positions
            positions = [r["index"] for r in revelations]
            if self._is_well_distributed(positions, total_messages):
                score += 2.0
        
        return max(0.0, min(10.0, score))
    
    def _evaluate_resolution_satisfaction(self) -> float:
        """Ã‰value la satisfaction de la rÃ©solution (0-10)."""
        score = 3.0  # Score de base
        
        # Bonus si solution trouvÃ©e
        if self.solution_analysis.get("success", False):
            score += 5.0
        elif self.solution_analysis.get("proposed_solution"):
            score += 2.0  # Tentative
        
        # Bonus si rÃ©solution logique et cohÃ©rente
        oracle_interactions = self.oracle_stats.get("workflow_metrics", {}).get("oracle_interactions", 0)
        if oracle_interactions >= 3:  # Interactions suffisantes
            score += 1.0
        
        # Bonus pour efficacitÃ© (rÃ©solution en moins de 10 tours)
        total_turns = self.oracle_stats.get("agent_interactions", {}).get("total_turns", 0)
        if total_turns <= 10:
            score += 1.0
        
        return max(0.0, min(10.0, score))
    
    # MÃ©thodes utilitaires d'analyse
    
    def _detect_repetitions(self) -> List[Dict[str, Any]]:
        """DÃ©tecte les rÃ©pÃ©titions dans les messages."""
        repetitions = []
        seen_patterns = {}
        
        for i, message in enumerate(self.conversation_history):
            content = message.get("message", "").lower()
            # Simplifier pour dÃ©tecter les patterns rÃ©pÃ©titifs
            words = content.split()[:10]  # Prendre les 10 premiers mots
            pattern = " ".join(words)
            
            if pattern in seen_patterns:
                repetitions.append({
                    "pattern": pattern,
                    "first_occurrence": seen_patterns[pattern],
                    "repeat_at": i,
                    "sender": message.get("sender", "")
                })
            else:
                seen_patterns[pattern] = i
        
        return repetitions
    
    def _count_unique_words(self) -> int:
        """Compte les mots uniques dans toute la conversation."""
        all_words = set()
        for message in self.conversation_history:
            content = message.get("message", "").lower()
            words = content.split()
            all_words.update(words)
        return len(all_words)
    
    def _calculate_average_message_lengths(self) -> Dict[str, float]:
        """Calcule la longueur moyenne des messages par agent."""
        agent_lengths = {}
        agent_counts = {}
        
        for message in self.conversation_history:
            sender = message.get("sender", "Unknown")
            length = len(message.get("message", ""))
            
            if sender not in agent_lengths:
                agent_lengths[sender] = 0
                agent_counts[sender] = 0
            
            agent_lengths[sender] += length
            agent_counts[sender] += 1
        
        return {agent: agent_lengths[agent] / agent_counts[agent] 
                for agent in agent_lengths if agent_counts[agent] > 0}
    
    def _check_cycle_adherence(self) -> float:
        """VÃ©rifie l'adhÃ©rence au cycle Sherlock->Watson->Moriarty (0-1)."""
        if len(self.conversation_history) < 3:
            return 0.0
        
        expected_cycle = ["Sherlock", "Watson", "Moriarty"]
        cycle_matches = 0
        total_cycles = 0
        
        for i in range(0, len(self.conversation_history) - 2, 3):
            total_cycles += 1
            if (i + 2 < len(self.conversation_history) and
                self.conversation_history[i].get("sender") == expected_cycle[0] and
                self.conversation_history[i + 1].get("sender") == expected_cycle[1] and
                self.conversation_history[i + 2].get("sender") == expected_cycle[2]):
                cycle_matches += 1
        
        return cycle_matches / total_cycles if total_cycles > 0 else 0.0
    
    def _extract_agent_keywords(self) -> Dict[str, List[str]]:
        """Extrait les mots-clÃ©s caractÃ©ristiques par agent."""
        agent_keywords = {}
        
        for message in self.conversation_history:
            sender = message.get("sender", "Unknown")
            content = message.get("message", "").lower()
            
            if sender not in agent_keywords:
                agent_keywords[sender] = []
            
            # Extraire mots significatifs (plus de 4 caractÃ¨res, pas de stop words)
            stop_words = {"avec", "dans", "pour", "vous", "nous", "quel", "cette", "faire"}
            words = [word for word in content.split() 
                    if len(word) > 4 and word not in stop_words]
            agent_keywords[sender].extend(words)
        
        return agent_keywords
    
    def _is_well_distributed(self, positions: List[int], total: int) -> bool:
        """VÃ©rifie si les positions sont bien distribuÃ©es."""
        if len(positions) < 2:
            return True
        
        # Calculer les intervalles
        intervals = []
        for i in range(1, len(positions)):
            intervals.append(positions[i] - positions[i-1])
        
        # VÃ©rifier que la variance des intervalles n'est pas trop Ã©levÃ©e
        mean_interval = sum(intervals) / len(intervals)
        variance = sum((x - mean_interval) ** 2 for x in intervals) / len(intervals)
        
        # Seuil arbitraire pour une bonne distribution
        return variance < (mean_interval ** 2)
    
    def analyze_detailed_conversation(self) -> ConversationAnalysis:
        """Analyse dÃ©taillÃ©e de la structure conversationnelle."""
        analysis = ConversationAnalysis()
        
        analysis.total_messages = len(self.conversation_history)
        
        # Messages par agent
        for message in self.conversation_history:
            sender = message.get("sender", "Unknown")
            analysis.messages_par_agent[sender] = analysis.messages_par_agent.get(sender, 0) + 1
        
        # Longueur moyenne des messages
        analysis.longueur_moyenne_messages = self._calculate_average_message_lengths()
        
        # Mots-clÃ©s dÃ©tectÃ©s
        analysis.mots_cles_detectes = self._extract_agent_keywords()
        
        # Transitions abruptes
        analysis.transitions_abruptes = [
            {
                "de": t["from"],
                "vers": t["to"],
                "position": t["index"],
                "fluidite": "abrupte" if "donc" not in t["curr_content"] else "fluide"
            }
            for t in self.conversation_patterns["transitions"][:5]  # Limiter Ã  5
        ]
        
        # RÃ©pÃ©titions dÃ©tectÃ©es
        analysis.repetitions_detectees = self._detect_repetitions()[:5]  # Limiter Ã  5
        
        # Progression dÃ©ductive
        analysis.progression_deductive = [
            {
                "etape": i + 1,
                "type": "suggestion" if "suggÃ©r" in p["content_preview"].lower() else "autre",
                "agent": p["sender"],
                "contenu": p["content_preview"]
            }
            for i, p in enumerate(self.conversation_patterns["suggestions"][:5])
        ]
        
        return analysis
    
    def generate_improvement_recommendations(self, metrics: ConversationalMetrics) -> List[Dict[str, Any]]:
        """GÃ©nÃ¨re des recommandations d'amÃ©lioration basÃ©es sur l'analyse."""
        recommendations = []
        
        # NaturalitÃ©
        if metrics.naturalite_score < 6.0:
            recommendations.append({
                "domaine": "NaturalitÃ© du dialogue",
                "score_actuel": metrics.naturalite_score,
                "probleme": "Dialogue peu naturel, rÃ©pÃ©titions ou longueurs inappropriÃ©es",
                "solutions": [
                    "Enrichir le vocabulaire des prompts agents",
                    "Ajouter de la variÃ©tÃ© dans les expressions",
                    "Calibrer la longueur des rÃ©ponses"
                ],
                "priorite": "Ã©levÃ©e" if metrics.naturalite_score < 4.0 else "moyenne"
            })
        
        # Pertinence
        if metrics.pertinence_score < 6.0:
            recommendations.append({
                "domaine": "Pertinence des interventions",
                "score_actuel": metrics.pertinence_score,
                "probleme": "Agents ne respectent pas leurs rÃ´les dÃ©finis",
                "solutions": [
                    "Renforcer les prompts de rÃ´le spÃ©cifiques",
                    "Ajouter des contraintes comportementales",
                    "AmÃ©liorer la sÃ©lection cyclique"
                ],
                "priorite": "Ã©levÃ©e"
            })
        
        # Progression logique
        if metrics.progression_logique_score < 6.0:
            recommendations.append({
                "domaine": "Progression logique",
                "score_actuel": metrics.progression_logique_score,
                "probleme": "EnquÃªte ne progresse pas logiquement vers la solution",
                "solutions": [
                    "AmÃ©liorer la stratÃ©gie de rÃ©vÃ©lation Moriarty",
                    "Ajouter des mÃ©canismes de progression forcÃ©e",
                    "Optimiser les critÃ¨res de terminaison"
                ],
                "priorite": "Ã©levÃ©e"
            })
        
        # PersonnalitÃ©s distinctes
        if metrics.personnalite_distincte_score < 6.0:
            recommendations.append({
                "domaine": "PersonnalitÃ©s distinctes",
                "score_actuel": metrics.personnalite_distincte_score,
                "probleme": "Agents manquent de personnalitÃ© distinctive",
                "solutions": [
                    "Enrichir les prompts avec des traits de personnalitÃ©",
                    "Ajouter des expressions caractÃ©ristiques",
                    "DÃ©velopper des styles de communication diffÃ©renciÃ©s"
                ],
                "priorite": "moyenne"
            })
        
        # FluiditÃ© des transitions
        if metrics.fluidite_transitions_score < 6.0:
            recommendations.append({
                "domaine": "FluiditÃ© des transitions",
                "score_actuel": metrics.fluidite_transitions_score,
                "probleme": "Transitions abruptes entre agents",
                "solutions": [
                    "Ajouter des mots de liaison dans les prompts",
                    "ImplÃ©menter une mÃ©moire contextuelle",
                    "AmÃ©liorer la passation de tour"
                ],
                "priorite": "moyenne"
            })
        
        # Dosage des rÃ©vÃ©lations
        if metrics.dosage_revelations_score < 6.0:
            recommendations.append({
                "domaine": "Dosage des rÃ©vÃ©lations Moriarty",
                "score_actuel": metrics.dosage_revelations_score,
                "probleme": "RÃ©vÃ©lations mal timÃ©es ou mal dosÃ©es",
                "solutions": [
                    "Calibrer la stratÃ©gie de rÃ©vÃ©lation",
                    "ImplÃ©menter des rÃ©vÃ©lations progressives",
                    "AmÃ©liorer les critÃ¨res de pertinence"
                ],
                "priorite": "Ã©levÃ©e"
            })
        
        # Satisfaction de rÃ©solution
        if metrics.satisfaction_resolution_score < 6.0:
            recommendations.append({
                "domaine": "Satisfaction de la rÃ©solution",
                "score_actuel": metrics.satisfaction_resolution_score,
                "probleme": "RÃ©solution insatisfaisante ou incomplÃ¨te",
                "solutions": [
                    "AmÃ©liorer la logique de terminaison",
                    "Ajouter des vÃ©rifications de cohÃ©rence",
                    "Optimiser l'efficacitÃ© du workflow"
                ],
                "priorite": "Ã©levÃ©e"
            })
        
        return recommendations


async def execute_workflow_analysis():
    """ExÃ©cute une session complÃ¨te et analyse la qualitÃ© conversationnelle."""
    logger.info("[DÃ‰BUT] ANALYSE DE LA TRACE CLUEDO ORACLE")
    logger.info("=" * 60)
    
    try:
        # 1. Configuration du kernel (simulation)
        logger.info("[CONFIG] Configuration du kernel Semantic...")
        kernel = Kernel()
        # NOTE: En production, configurez ici votre service LLM
        
        # 2. ExÃ©cution du workflow 3-agents
        logger.info("ðŸŽ­ Lancement du workflow 3-agents...")
        workflow_result = await run_cluedo_oracle_game(
            kernel=kernel,
            initial_question="L'enquÃªte commence. Sherlock, menez l'investigation !",
            max_turns=12,
            max_cycles=4,
            oracle_strategy="balanced"
        )
        
        logger.info("âœ… Workflow terminÃ© avec succÃ¨s")
        
        # 3. Analyse de la qualitÃ© conversationnelle
        logger.info("ðŸ” Analyse de la qualitÃ© conversationnelle...")
        analyzer = TraceQualityAnalyzer()
        analyzer.capture_conversation(workflow_result)
        
        # MÃ©triques de qualitÃ©
        metrics = analyzer.analyze_conversational_quality()
        
        # Analyse dÃ©taillÃ©e
        detailed_analysis = analyzer.analyze_detailed_conversation()
        
        # Recommandations d'amÃ©lioration
        recommendations = analyzer.generate_improvement_recommendations(metrics)
        
        # 4. GÃ©nÃ©ration du rapport complet
        report = generate_comprehensive_report(
            workflow_result, metrics, detailed_analysis, recommendations
        )
        
        # 5. Sauvegarde des rÃ©sultats
        save_analysis_results(report)
        
        # 6. Affichage du rÃ©sumÃ©
        display_analysis_summary(metrics, recommendations)
        
        logger.info("âœ… ANALYSE TERMINÃ‰E AVEC SUCCÃˆS")
        return report
        
    except Exception as e:
        logger.error(f"âŒ Erreur durant l'analyse: {e}")
        traceback.print_exc()
        raise


def generate_comprehensive_report(
    workflow_result: Dict[str, Any],
    metrics: ConversationalMetrics,
    detailed_analysis: ConversationAnalysis,
    recommendations: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """GÃ©nÃ¨re un rapport complet d'analyse."""
    
    return {
        "metadata": {
            "analyse_timestamp": datetime.now().isoformat(),
            "version_oracle": "1.0.0",
            "workflow_strategy": workflow_result.get("workflow_info", {}).get("strategy", "unknown")
        },
        "trace_complete": {
            "conversation_history": workflow_result.get("conversation_history", []),
            "solution_analysis": workflow_result.get("solution_analysis", {}),
            "oracle_statistics": workflow_result.get("oracle_statistics", {}),
            "final_state": workflow_result.get("final_state", {})
        },
        "analyse_qualitative": {
            "metriques_conversationnelles": asdict(metrics),
            "analyse_detaillee": asdict(detailed_analysis),
            "score_global": metrics.moyenne_globale(),
            "diagnostic": generate_quality_diagnostic(metrics)
        },
        "recommandations_amelioration": recommendations,
        "criteres_trace_ideale": define_ideal_trace_criteria(),
        "plan_optimisation": generate_optimization_plan(recommendations),
        "benchmark_2vs3_agents": compare_with_2agent_system(workflow_result)
    }


def generate_quality_diagnostic(metrics: ConversationalMetrics) -> Dict[str, str]:
    """GÃ©nÃ¨re un diagnostic de qualitÃ© basÃ© sur les mÃ©triques."""
    score = metrics.moyenne_globale()
    
    if score >= 8.0:
        level = "EXCELLENT"
        description = "Conversation de trÃ¨s haute qualitÃ©, proche de la trace idÃ©ale"
    elif score >= 6.5:
        level = "BON"
        description = "Conversation de bonne qualitÃ© avec quelques amÃ©liorations possibles"
    elif score >= 5.0:
        level = "MOYEN"
        description = "Conversation acceptable mais nÃ©cessitant des optimisations"
    elif score >= 3.0:
        level = "FAIBLE"
        description = "Conversation de qualitÃ© insuffisante, optimisations importantes nÃ©cessaires"
    else:
        level = "TRÃˆS FAIBLE"
        description = "Conversation de trÃ¨s mauvaise qualitÃ©, refonte nÃ©cessaire"
    
    return {
        "niveau": level,
        "score": f"{score:.2f}/10",
        "description": description,
        "statut": "ACCEPTABLE" if score >= 5.0 else "Ã€ AMÃ‰LIORER"
    }


def define_ideal_trace_criteria() -> Dict[str, Any]:
    """DÃ©finit les critÃ¨res d'une trace idÃ©ale."""
    return {
        "dialogue_naturel": {
            "description": "Ã‰changes fluides et humains entre agents",
            "criteres": [
                "VariÃ©tÃ© du vocabulaire (>150 mots uniques)",
                "Longueur appropriÃ©e des messages (50-150 mots)",
                "Absence de rÃ©pÃ©titions (< 5% des messages)",
                "Expressions naturelles et variÃ©es"
            ],
            "score_cible": 8.0
        },
        "personnalites_distinctes": {
            "description": "Chaque agent a une personnalitÃ© unique et reconnaissable",
            "criteres": [
                "Sherlock: Leadership, mÃ©thode, dÃ©duction",
                "Watson: Logique, assistance, rigueur",
                "Moriarty: MystÃ¨re, rÃ©vÃ©lation, stratÃ©gie"
            ],
            "score_cible": 7.5
        },
        "progression_logique": {
            "description": "EnquÃªte progresse mÃ©thodiquement vers la solution",
            "criteres": [
                "Suggestions de Sherlock pertinentes",
                "Validations logiques de Watson",
                "RÃ©vÃ©lations stratÃ©giques de Moriarty",
                "Convergence vers la solution correcte"
            ],
            "score_cible": 8.5
        },
        "revelations_strategiques": {
            "description": "Informations rÃ©vÃ©lÃ©es au bon moment",
            "criteres": [
                "Dosage appropriÃ© (20-30% des tours)",
                "RÃ©partition Ã©quilibrÃ©e dans le temps",
                "Pertinence par rapport aux suggestions",
                "Progression vers la rÃ©solution"
            ],
            "score_cible": 8.0
        },
        "resolution_satisfaisante": {
            "description": "Conclusion logique et satisfaisante",
            "criteres": [
                "Solution correcte proposÃ©e",
                "Justification logique claire",
                "EfficacitÃ© temporelle (<10 tours)",
                "CohÃ©rence narrative"
            ],
            "score_cible": 9.0
        }
    }


def generate_optimization_plan(recommendations: List[Dict[str, Any]]) -> Dict[str, Any]:
    """GÃ©nÃ¨re un plan d'optimisation incrÃ©mentale."""
    
    # Tri des recommandations par prioritÃ©
    high_priority = [r for r in recommendations if r.get("priorite") == "Ã©levÃ©e"]
    medium_priority = [r for r in recommendations if r.get("priorite") == "moyenne"]
    
    return {
        "phase_A_prompts_personnalites": {
            "description": "Optimisation des prompts et personnalitÃ©s d'agents",
            "actions": [
                "Enrichir le prompt Sherlock avec expressions caractÃ©ristiques",
                "AmÃ©liorer la diffÃ©renciation Watson/Sherlock",
                "DÃ©velopper la personnalitÃ© mystÃ©rieuse de Moriarty",
                "Ajouter des mots de liaison pour fluiditÃ©"
            ],
            "priorite": "Ã‰LEVÃ‰E",
            "effort_estime": "2-3 jours",
            "impact_attendu": "+1.5 points sur personnalitÃ©s distinctes"
        },
        "phase_B_logique_revelations": {
            "description": "AmÃ©lioration de la logique de rÃ©vÃ©lations Moriarty",
            "actions": [
                "Calibrer la stratÃ©gie de rÃ©vÃ©lation 'balanced'",
                "ImplÃ©menter rÃ©vÃ©lations progressives",
                "AmÃ©liorer timing des rÃ©vÃ©lations",
                "Optimiser critÃ¨res de pertinence"
            ],
            "priorite": "Ã‰LEVÃ‰E",
            "effort_estime": "3-4 jours",
            "impact_attendu": "+2.0 points sur dosage rÃ©vÃ©lations"
        },
        "phase_C_transitions_fluidite": {
            "description": "Affinement des transitions et fluiditÃ©",
            "actions": [
                "ImplÃ©menter mÃ©moire contextuelle entre tours",
                "Ajouter rÃ©fÃ©rences au message prÃ©cÃ©dent",
                "AmÃ©liorer la sÃ©lection adaptative",
                "Optimiser la passation de tour"
            ],
            "priorite": "MOYENNE",
            "effort_estime": "2-3 jours",
            "impact_attendu": "+1.0 point sur fluiditÃ© transitions"
        },
        "phase_D_validation_metriques": {
            "description": "Validation et mÃ©triques qualitatives",
            "actions": [
                "ImplÃ©menter mÃ©triques automatiques",
                "Tests A/B avec diffÃ©rentes stratÃ©gies",
                "Validation utilisateur",
                "Benchmark performance"
            ],
            "priorite": "MOYENNE",
            "effort_estime": "2-3 jours",
            "impact_attendu": "Mesure objective des amÃ©liorations"
        }
    }


def compare_with_2agent_system(workflow_result: Dict[str, Any]) -> Dict[str, Any]:
    """Compare avec le systÃ¨me 2-agents thÃ©orique."""
    oracle_stats = workflow_result.get("oracle_statistics", {})
    
    return {
        "efficacite_temporelle": {
            "tours_3_agents": oracle_stats.get("agent_interactions", {}).get("total_turns", 0),
            "estimation_2_agents": oracle_stats.get("agent_interactions", {}).get("total_turns", 0) * 1.5,
            "gain_estime": "25-30% reduction in turns"
        },
        "richesse_informationnelle": {
            "cartes_revelees": oracle_stats.get("workflow_metrics", {}).get("cards_revealed", 0),
            "interactions_oracle": oracle_stats.get("workflow_metrics", {}).get("oracle_interactions", 0),
            "avantage_3_agents": "RÃ©vÃ©lations stratÃ©giques vs alÃ©atoires"
        },
        "qualite_resolution": {
            "succes_actuel": workflow_result.get("solution_analysis", {}).get("success", False),
            "methode": "DÃ©duction assistÃ©e par Oracle",
            "vs_2_agents": "Ã‰limination pure par suggestions"
        }
    }


def save_analysis_results(report: Dict[str, Any]) -> None:
    """Sauvegarde les rÃ©sultats d'analyse."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Sauvegarde du rapport complet
    with open(f"analyse_trace_complete_{timestamp}.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # Sauvegarde du rÃ©sumÃ© exÃ©cutif
    summary = {
        "timestamp": report["metadata"]["analyse_timestamp"],
        "score_global": report["analyse_qualitative"]["score_global"],
        "diagnostic": report["analyse_qualitative"]["diagnostic"],
        "recommandations_prioritaires": [
            r for r in report["recommandations_amelioration"] 
            if r.get("priorite") == "Ã©levÃ©e"
        ][:3],
        "prochaines_etapes": list(report["plan_optimisation"].keys())[:2]
    }
    
    with open(f"resume_executif_{timestamp}.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ðŸ“„ RÃ©sultats sauvegardÃ©s: analyse_trace_complete_{timestamp}.json")


def display_analysis_summary(metrics: ConversationalMetrics, recommendations: List[Dict[str, Any]]) -> None:
    """Affiche le rÃ©sumÃ© de l'analyse."""
    print("\n" + "="*80)
    print("ðŸŽ¯ RÃ‰SUMÃ‰ DE L'ANALYSE DE LA TRACE SHERLOCK-WATSON-MORIARTY")
    print("="*80)
    
    print(f"\nðŸ“Š SCORE GLOBAL: {metrics.moyenne_globale():.2f}/10")
    print(f"ðŸŽ­ NaturalitÃ©: {metrics.naturalite_score:.1f}/10")
    print(f"ðŸŽ¯ Pertinence: {metrics.pertinence_score:.1f}/10")
    print(f"ðŸ§  Progression logique: {metrics.progression_logique_score:.1f}/10")
    print(f"ðŸ‘¤ PersonnalitÃ©s distinctes: {metrics.personnalite_distincte_score:.1f}/10")
    print(f"ðŸ”„ FluiditÃ© transitions: {metrics.fluidite_transitions_score:.1f}/10")
    print(f"ðŸ’Ž Dosage rÃ©vÃ©lations: {metrics.dosage_revelations_score:.1f}/10")
    print(f"âœ… Satisfaction rÃ©solution: {metrics.satisfaction_resolution_score:.1f}/10")
    
    print(f"\nðŸš¨ POINTS D'AMÃ‰LIORATION PRIORITAIRES:")
    high_priority = [r for r in recommendations if r.get("priorite") == "Ã©levÃ©e"]
    for i, rec in enumerate(high_priority[:3], 1):
        print(f"{i}. {rec['domaine']} (Score: {rec['score_actuel']:.1f}/10)")
        print(f"   â†’ {rec['probleme']}")
    
    print(f"\nðŸ“ˆ PROCHAINES Ã‰TAPES:")
    print("1. Phase A: Optimisation prompts et personnalitÃ©s (2-3 jours)")
    print("2. Phase B: AmÃ©lioration logique rÃ©vÃ©lations Moriarty (3-4 jours)")
    print("3. Phase C: Affinement transitions et fluiditÃ© (2-3 jours)")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    asyncio.run(execute_workflow_analysis())