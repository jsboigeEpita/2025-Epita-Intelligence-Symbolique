import pytest
#!/usr/bin/env python3
"""
Script de test pour Phase C : Optimisation fluiditÃ© transitions.

Ce script valide l'implÃ©mentation de la mÃ©moire contextuelle et des directives
de continuitÃ© narrative pour amÃ©liorer la fluiditÃ© des transitions entre agents.

CritÃ¨res de validation Phase C:
- RÃ©fÃ©rences contextuelles : >90% des messages rÃ©fÃ©rencent le prÃ©cÃ©dent
- RÃ©actions Ã©motionnelles : >70% des rÃ©vÃ©lations suscitent rÃ©action
- ContinuitÃ© narrative : <10% de ruptures narratives
- Score fluiditÃ© : 5.0 â†’ 6.5/10
"""

import asyncio
import logging
import json
from datetime import datetime
from typing import Dict, List, Any
import statistics

# Configuration du logging pour le test
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('test_phase_c_fluidite.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Imports pour simulation (mock pour le test)
class AuthenticGPT4oMiniKernel:
    """Mock du Kernel pour les tests."""
    def add_plugin(self, plugin, name): pass
    def add_filter(self, filter_type, filter_func): pass

class MockChatMessage:
    """Mock des messages de chat."""
    def __init__(self, name: str, content: str):
        self.name = name
        self.content = content

@pytest.mark.asyncio
async def test_phase_c_fluidite_complete():
    """
    Test complet de la Phase C avec 5 conversations simulÃ©es.
    """
    print("DEBUT TEST PHASE C - FLUIDITE TRANSITIONS")
    print("=" * 60)
    
    # Import des modules modifiÃ©s
    from argumentation_analysis.core.cluedo_oracle_state import CluedoOracleState
    from argumentation_analysis.orchestration.cluedo_extended_orchestrator import CluedoExtendedOrchestrator
    
    # Configuration de base
    elements_jeu = {
        "suspects": ["Colonel Moutarde", "Professeur Violet", "Mademoiselle Rose", "Docteur OrchidÃ©e"],
        "armes": ["Poignard", "Chandelier", "Revolver", "Corde"],
        "lieux": ["Salon", "Cuisine", "Bureau", "BibliothÃ¨que"]
    }
    
    # RÃ©sultats de tous les tests
    all_test_results = []
    
    # ExÃ©cution de 5 conversations de test
    for test_num in range(1, 6):
        print(f"\nTEST {test_num}/5 - Conversation {test_num}")
        print("-" * 40)
        
        try:
            # Simulation d'une conversation avec fluiditÃ©
            test_result = await simulate_fluidity_conversation(test_num, elements_jeu)
            all_test_results.append(test_result)
            
            # Affichage des rÃ©sultats immÃ©diats
            print(f"[OK] Test {test_num} termine:")
            print(f"   - Score fluidite: {test_result['fluidity_score']}/10")
            print(f"   - References contextuelles: {test_result['contextual_reference_rate']}%")
            print(f"   - Reactions emotionnelles: {test_result['emotional_reaction_rate']}%")
            
        except Exception as e:
            logger.error(f"Erreur dans test {test_num}: {e}", exc_info=True)
            print(f"[ERREUR] Test {test_num} echoue: {e}")
            continue
    
    # Analyse agrÃ©gÃ©e des rÃ©sultats
    print("\nANALYSE AGREGEE PHASE C")
    print("=" * 60)
    
    if all_test_results:
        aggregate_analysis = analyze_aggregate_results(all_test_results)
        print_phase_c_results(aggregate_analysis)
        
        # Sauvegarde des rÃ©sultats
        save_test_results(all_test_results, aggregate_analysis)
    else:
        print("[ERREUR] Aucun test reussi - impossible d'analyser les resultats")
    
    print("\n[OK] TEST PHASE C TERMINE")

@pytest.mark.asyncio
async def simulate_fluidity_conversation(test_num: int, elements_jeu: Dict[str, List[str]]) -> Dict[str, Any]:
    """
    Simule une conversation avec focus sur la fluiditÃ© des transitions.
    
    Args:
        test_num: NumÃ©ro du test
        elements_jeu: Ã‰lÃ©ments du jeu Cluedo
        
    Returns:
        RÃ©sultats de fluiditÃ© pour cette conversation
    """
    # CrÃ©ation de l'Ã©tat Oracle Ã©tendu
    oracle_state = CluedoOracleState(
        nom_enquete_cluedo=f"Test FluiditÃ© {test_num}",
        elements_jeu_cluedo=elements_jeu,
        description_cas="Test des transitions fluides entre agents",
        initial_context="Test Phase C - MÃ©moire contextuelle",
        oracle_strategy="balanced"
    )
    
    # Simulation de messages avec continuitÃ© narrative
    conversation_scenarios = [
        # Tour 1: Sherlock ouvre
        {
            "agent": "Sherlock",
            "content": "Je commence mon enquÃªte. J'observe des indices dans le salon.",
            "type": "analysis"
        },
        # Tour 2: Watson rÃ©agit avec rÃ©fÃ©rence
        {
            "agent": "Watson", 
            "content": "Suite Ã  votre observation Sherlock, l'analyse logique suggÃ¨re que le salon est crucial. Brillant !",
            "type": "reaction"
        },
        # Tour 3: Moriarty rÃ©vÃ¨le avec rÃ©action
        {
            "agent": "Moriarty",
            "content": "En rÃ©action Ã  vos dÃ©ductions, je rÃ©vÃ¨le que j'ai le Chandelier. Vous brÃ»lez !",
            "type": "revelation"
        },
        # Tour 4: Sherlock continue avec rÃ©fÃ©rence
        {
            "agent": "Sherlock",
            "content": "AprÃ¨s cette rÃ©vÃ©lation de Moriarty, j'affine ma dÃ©duction. Le Chandelier Ã©liminÃ©, focus sur le Revolver.",
            "type": "analysis"
        },
        # Tour 5: Watson approuve avec Ã©motion
        {
            "agent": "Watson",
            "content": "PrÃ©cisÃ©ment Watson ! Cette Ã©limination logique nous rapproche. Ã‡a colle parfaitement !",
            "type": "reaction"
        },
        # Tour 6: Moriarty crÃ©e suspense
        {
            "agent": "Moriarty", 
            "content": "Hmm... pas si vite ! Vous m'impressionnez, mais je possÃ¨de aussi Mademoiselle Rose.",
            "type": "revelation"
        },
        # Tour 7: Sherlock synthÃ©tise
        {
            "agent": "Sherlock",
            "content": "Merci pour cette clarification Moriarty. Avec ces donnÃ©es, je suggÃ¨re : Docteur OrchidÃ©e, Revolver, Bureau.",
            "type": "suggestion"
        },
        # Tour 8: Watson valide avec Ã©motion
        {
            "agent": "Watson",
            "content": "Exactement ! Cette suggestion logique intÃ¨gre toutes nos dÃ©couvertes prÃ©cÃ©dentes. Magistral !",
            "type": "reaction"
        }
    ]
    
    # Simulation de la conversation
    logger.info(f"Simulation conversation {test_num} avec {len(conversation_scenarios)} tours")
    
    for turn, scenario in enumerate(conversation_scenarios, 1):
        agent_name = scenario["agent"]
        content = scenario["content"]
        msg_type = scenario["type"]
        
        # Ajout du message Ã  l'historique conversationnel
        oracle_state.add_conversation_message(agent_name, content, msg_type)
        
        # Simulation de l'analyse contextuelle (normalement faite par l'orchestrateur)
        await simulate_contextual_analysis(oracle_state, agent_name, content, turn)
        
        logger.debug(f"Tour {turn}: {agent_name} ({msg_type}): {content[:50]}...")
    
    # Calcul des mÃ©triques de fluiditÃ©
    fluidity_metrics = oracle_state.get_fluidity_metrics()
    
    # Enrichissement avec donnÃ©es de test
    fluidity_metrics.update({
        "test_number": test_num,
        "total_turns": len(conversation_scenarios),
        "conversation_length": len(conversation_scenarios),
        "test_timestamp": datetime.now().isoformat()
    })
    
    return fluidity_metrics

@pytest.mark.asyncio
async def simulate_contextual_analysis(oracle_state, agent_name: str, content: str, turn: int):
    """
    Simule l'analyse contextuelle qui serait faite par l'orchestrateur.
    """
    content_lower = content.lower()
    
    # Simulation des rÃ©fÃ©rences contextuelles
    reference_keywords = [
        ("suite Ã ", "building_on"),
        ("en rÃ©action Ã ", "reacting_to"),
        ("aprÃ¨s cette", "responding_to"),
        ("merci pour cette clarification", "acknowledging"),
        ("avec ces donnÃ©es", "integrating")
    ]
    
    for keyword, ref_type in reference_keywords:
        if keyword in content_lower and turn > 1:
            oracle_state.record_contextual_reference(
                source_agent=agent_name,
                target_message_turn=turn - 1,
                reference_type=ref_type,
                reference_content=keyword
            )
            break
    
    # Simulation des rÃ©actions Ã©motionnelles
    if turn > 1:
        emotional_patterns = {
            "brillant": "approval",
            "exactement": "approval", 
            "Ã§a colle parfaitement": "approval",
            "prÃ©cisÃ©ment": "approval",
            "magistral": "excitement",
            "vous m'impressionnez": "excitement",
            "vous brÃ»lez": "encouragement",
            "hmm": "suspense"
        }
        
        for pattern, reaction_type in emotional_patterns.items():
            if pattern in content_lower:
                oracle_state.record_emotional_reaction(
                    agent_name=agent_name,
                    trigger_agent="Previous_Agent",  # Simulation
                    trigger_content="Previous message content",  # Simulation
                    reaction_type=reaction_type,
                    reaction_content=content[:100]
                )
                break

def analyze_aggregate_results(all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Analyse les rÃ©sultats agrÃ©gÃ©s de tous les tests.
    """
    if not all_results:
        return {}
    
    # Extraction des mÃ©triques
    fluidity_scores = [r['fluidity_score'] for r in all_results]
    ref_rates = [r['contextual_reference_rate'] for r in all_results]
    reaction_rates = [r['emotional_reaction_rate'] for r in all_results]
    
    # Calculs statistiques
    aggregate = {
        "total_tests": len(all_results),
        "successful_tests": len([r for r in all_results if r['fluidity_score'] > 0]),
        
        "fluidity_scores": {
            "mean": statistics.mean(fluidity_scores),
            "median": statistics.median(fluidity_scores),
            "min": min(fluidity_scores),
            "max": max(fluidity_scores),
            "target": 6.5,
            "target_achieved": statistics.mean(fluidity_scores) >= 6.5
        },
        
        "contextual_references": {
            "mean_rate": statistics.mean(ref_rates),
            "median_rate": statistics.median(ref_rates),
            "min_rate": min(ref_rates),
            "max_rate": max(ref_rates),
            "target": 90.0,
            "target_achieved": statistics.mean(ref_rates) >= 90.0
        },
        
        "emotional_reactions": {
            "mean_rate": statistics.mean(reaction_rates),
            "median_rate": statistics.median(reaction_rates),
            "min_rate": min(reaction_rates),
            "max_rate": max(reaction_rates),
            "target": 70.0,
            "target_achieved": statistics.mean(reaction_rates) >= 70.0
        },
        
        "phase_c_success": {
            "fluidity_improved": statistics.mean(fluidity_scores) > 5.0,
            "references_sufficient": statistics.mean(ref_rates) >= 90.0,
            "reactions_sufficient": statistics.mean(reaction_rates) >= 70.0,
        }
    }
    
    # Ã‰valuation globale
    success_criteria = [
        aggregate["fluidity_scores"]["target_achieved"],
        aggregate["contextual_references"]["target_achieved"], 
        aggregate["emotional_reactions"]["target_achieved"]
    ]
    
    aggregate["overall_success"] = all(success_criteria)
    aggregate["success_percentage"] = (sum(success_criteria) / len(success_criteria)) * 100
    
    return aggregate

def print_phase_c_results(aggregate: Dict[str, Any]):
    """
    Affiche les rÃ©sultats d'analyse Phase C.
    """
    print(f"Tests exÃ©cutÃ©s: {aggregate['total_tests']}")
    print(f"Tests rÃ©ussis: {aggregate['successful_tests']}")
    print()
    
    print("ğŸ“Š MÃ‰TRIQUES DE FLUIDITÃ‰:")
    fluidity = aggregate["fluidity_scores"]
    print(f"   Score moyen: {fluidity['mean']:.1f}/10 (cible: {fluidity['target']})")
    print(f"   Cible atteinte: {'[OK] OUI' if fluidity['target_achieved'] else '[FAIL] NON'}")
    print(f"   Plage: {fluidity['min']:.1f} - {fluidity['max']:.1f}")
    print()
    
    print("ğŸ”— RÃ‰FÃ‰RENCES CONTEXTUELLES:")
    refs = aggregate["contextual_references"]
    print(f"   Taux moyen: {refs['mean_rate']:.1f}% (cible: {refs['target']}%)")
    print(f"   Cible atteinte: {'[OK] OUI' if refs['target_achieved'] else '[FAIL] NON'}")
    print(f"   Plage: {refs['min_rate']:.1f}% - {refs['max_rate']:.1f}%")
    print()
    
    print("â¤ï¸ RÃ‰ACTIONS Ã‰MOTIONNELLES:")
    reactions = aggregate["emotional_reactions"]
    print(f"   Taux moyen: {reactions['mean_rate']:.1f}% (cible: {reactions['target']}%)")
    print(f"   Cible atteinte: {'[OK] OUI' if reactions['target_achieved'] else '[FAIL] NON'}")
    print(f"   Plage: {reactions['min_rate']:.1f}% - {reactions['max_rate']:.1f}%")
    print()
    
    print("ğŸ¯ SUCCÃˆS PHASE C:")
    print(f"   AmÃ©lioration fluiditÃ© (>5.0): {'[OK]' if aggregate['phase_c_success']['fluidity_improved'] else '[FAIL]'}")
    print(f"   RÃ©fÃ©rences suffisantes (â‰¥90%): {'[OK]' if aggregate['phase_c_success']['references_sufficient'] else '[FAIL]'}")
    print(f"   RÃ©actions suffisantes (â‰¥70%): {'[OK]' if aggregate['phase_c_success']['reactions_sufficient'] else '[FAIL]'}")
    print()
    
    print(f"ğŸ† RÃ‰SULTAT GLOBAL: {'[OK] SUCCÃˆS' if aggregate['overall_success'] else '[FAIL] Ã‰CHEC'}")
    print(f"   Pourcentage de rÃ©ussite: {aggregate['success_percentage']:.1f}%")

def save_test_results(all_results: List[Dict[str, Any]], aggregate: Dict[str, Any]):
    """
    Sauvegarde les rÃ©sultats de test dans un fichier JSON.
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"phase_c_fluidite_results_{timestamp}.json"
    
    output_data = {
        "metadata": {
            "test_type": "Phase C - FluiditÃ© Transitions",
            "timestamp": datetime.now().isoformat(),
            "total_tests": len(all_results)
        },
        "individual_results": all_results,
        "aggregate_analysis": aggregate
    }
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s: {filename}")
    except Exception as e:
        logger.error(f"Erreur sauvegarde: {e}")
        print(f"\n[FAIL] Erreur sauvegarde: {e}")

if __name__ == "__main__":
    asyncio.run(test_phase_c_fluidite_complete())