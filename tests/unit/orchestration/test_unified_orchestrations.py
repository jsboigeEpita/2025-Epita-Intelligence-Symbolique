
#!/usr/bin/env python3
"""
Tests unitaires avancÃ©s pour les orchestrations unifiÃ©es
======================================================

Suite finale de tests pour ConversationOrchestrator, RealLLMOrchestrator,
et coordination systÃ¨me complÃ¨te avec composants authentiques.
"""

# Authentic gpt-4o-mini imports (replacing mocks)
import openai
from semantic_kernel.contents import ChatHistory
from semantic_kernel.core_plugins import ConversationSummaryPlugin
from config.unified_config import UnifiedConfig
from unittest.mock import MagicMock, AsyncMock

import pytest
import asyncio
import sys
import time
from pathlib import Path

from typing import Dict, Any, List

# Ajout du chemin pour les imports
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

try:
    from argumentation_analysis.orchestration.conversation_orchestrator import ConversationOrchestrator
    from argumentation_analysis.orchestration.real_llm_orchestrator import RealLLMOrchestrator
    from argumentation_analysis.utils.tweety_error_analyzer import TweetyErrorAnalyzer, TweetyErrorFeedback
    from config.unified_config import UnifiedConfig
    from argumentation_analysis.agents.core.logic.fol_logic_agent import FirstOrderLogicAgent
    REAL_COMPONENTS_AVAILABLE = True
except ImportError as e:
    print(f"Avertissement: Composants rÃ©els non disponibles: {e}")
    REAL_COMPONENTS_AVAILABLE = False
    
    # Mocks de fallback
    class ConversationOrchestrator:
        def __init__(self, mode="demo", config=None):
            self.mode = mode
            self.config = config
            self.agents = []
            self.state = {}
            
        def run_orchestration(self, text: str) -> str:
            return f"Mock orchestration {self.mode}: {text[:50]}..."
            
        def get_agents(self) -> List:
            return self.agents
            
        def get_state(self) -> Dict:
            return self.state
    
    class RealLLMOrchestrator:
        def __init__(self, mode="real", llm_service=None, config=None):
            self.mode = mode
            self.llm_service = llm_service
            self.config = config
            self.agents = {}
            
        async def initialize(self) -> bool:
            return True
            
        async def run_real_llm_orchestration(self, text: str) -> Dict[str, Any]:
            return {
                "status": "success",
                "analysis": f"Mock real LLM analysis: {text[:50]}...",
                "agents_used": ["RealAgent1", "RealAgent2"],
                "trace": "Mock trace",
                "bnf_feedback": None
            }
    
    class TweetyErrorAnalyzer:
        def analyze_error(self, error_text: str) -> Any:
            return type('TweetyErrorFeedback', (), {
                'error_type': 'MOCK_ERROR',
                'corrections': ['Mock correction'],
                'bnf_rules': ['Mock BNF rule'],
                'confidence': 0.9
            })()
    
    class UnifiedConfig:
        def __init__(self, **kwargs):
            self.logic_type = kwargs.get('logic_type', 'FOL')
            self.mock_level = kwargs.get('mock_level', 'PARTIAL')
            self.orchestration_type = kwargs.get('orchestration_type', 'CONVERSATION')
            self.require_real_gpt = kwargs.get('require_real_gpt', False)
            self.require_real_tweety = kwargs.get('require_real_tweety', False)
    
    class FirstOrderLogicAgent:
        def __init__(self, **kwargs):
            self.agent_name = "MockFOLAgent"


class TestUnifiedOrchestrations:
    async def _create_authentic_gpt4o_mini_instance(self):
        """CrÃ©e une instance authentique de gpt-4o-mini au lieu d'un mock."""
        config = UnifiedConfig()
        return config.get_kernel_with_gpt4o_mini()
        
    async def _make_authentic_llm_call(self, prompt: str) -> str:
        """Fait un appel authentique Ã  gpt-4o-mini."""
        try:
            kernel = await self._create_authentic_gpt4o_mini_instance()
            result = await kernel.invoke("chat", input=prompt)
            return str(result)
        except Exception as e:
            logger.warning(f"Appel LLM authentique Ã©chouÃ©: {e}")
            return "Authentic LLM call failed"

    """Tests avancÃ©s pour les orchestrations unifiÃ©es."""
    
    def setup_method(self):
        """Configuration initiale pour chaque test."""
        self.test_text = "L'Ukraine a Ã©tÃ© crÃ©Ã©e par la Russie. Donc Poutine a raison."
        self.test_config = UnifiedConfig(
            logic_type='FOL',
            mock_level='NONE',
            orchestration_type='UNIFIED',
            require_real_gpt=True,
            require_real_tweety=True
        )
    
    def test_conversation_orchestrator_initialization(self):
        """Test d'initialisation avancÃ©e du ConversationOrchestrator."""
        orchestrator = ConversationOrchestrator(mode="demo", config=self.test_config)
        
        assert orchestrator.mode == "demo"
        assert hasattr(orchestrator, 'agents')
        assert hasattr(orchestrator, 'state')
        
        # Test configuration intÃ©grÃ©e
        if hasattr(orchestrator, 'config'):
            assert orchestrator.config is not None
    
    def test_real_llm_orchestrator_configuration(self):
        """Test de configuration du RealLLMOrchestrator."""
        orchestrator = RealLLMOrchestrator(
            mode="real", 
            config=self.test_config
        )
        
        assert orchestrator.mode == "real"
        assert hasattr(orchestrator, 'agents')
        
        # Test que la configuration est respectÃ©e
        if hasattr(orchestrator, 'config'):
            assert orchestrator.config.logic_type == 'FOL'
    
    def test_multi_agent_coordination(self):
        """Test de coordination multi-agents."""
        orchestrator = ConversationOrchestrator(mode="demo")
        
        # VÃ©rifier que les agents sont configurÃ©s
        agents = orchestrator.get_agents() if hasattr(orchestrator, 'get_agents') else orchestrator.agents
        assert isinstance(agents, list)
        
        # Test de coordination basique
        result = orchestrator.run_orchestration(self.test_text)
        assert isinstance(result, str)
        assert len(result) > 0
    
    def test_shared_state_management(self):
        """Test de gestion d'Ã©tat partagÃ© entre agents."""
        orchestrator = ConversationOrchestrator(mode="demo")
        
        # Test de l'Ã©tat initial
        if hasattr(orchestrator, 'get_state'):
            state = orchestrator.get_state()
            assert isinstance(state, dict)
        
        # Test de mise Ã  jour d'Ã©tat aprÃ¨s orchestration
        result = orchestrator.run_orchestration(self.test_text)
        
        # VÃ©rifier que l'Ã©tat a Ã©tÃ© mis Ã  jour
        if hasattr(orchestrator, 'state'):
            assert orchestrator.state is not None
    
    def test_error_recovery_mechanisms(self):
        """Test des mÃ©canismes de rÃ©cupÃ©ration d'erreur."""
        orchestrator = ConversationOrchestrator(mode="demo")
        
        # Test avec texte problÃ©matique
        problematic_texts = [
            "",  # Texte vide
            "A" * 10000,  # Texte trÃ¨s long
            "ðŸ¤”ðŸ’­ðŸ§ ",  # Emojis uniquement
            None  # Valeur None (si gÃ©rÃ©)
        ]
        
        for text in problematic_texts:
            try:
                if text is not None:
                    result = orchestrator.run_orchestration(text)
                    assert isinstance(result, str)
            except Exception as e:
                # VÃ©rifier que l'erreur est appropriÃ©e
                assert isinstance(e, (ValueError, TypeError, AttributeError))
    
    def test_trace_generation_quality(self):
        """Test de la qualitÃ© de gÃ©nÃ©ration des traces."""
        orchestrator = ConversationOrchestrator(mode="trace")
        
        result = orchestrator.run_orchestration(self.test_text)
        
        # VÃ©rifier la structure de la trace
        assert isinstance(result, str)
        assert len(result) > len(self.test_text)  # La trace doit Ãªtre plus riche
        
        # VÃ©rifier les Ã©lÃ©ments attendus dans une trace
        trace_indicators = ["trace", "agent", "step", "analysis", "â†’", "â€¢"]
        has_trace_elements = any(indicator in result.lower() for indicator in trace_indicators)
        assert has_trace_elements
    
    def test_performance_orchestration(self):
        """Test de performance de l'orchestration."""
        orchestrator = ConversationOrchestrator(mode="micro")
        
        start_time = time.time()
        
        # ExÃ©cuter plusieurs orchestrations
        for i in range(3):
            text = f"Test {i}: Analyse rapide nÃ©cessaire."
            result = orchestrator.run_orchestration(text)
            assert isinstance(result, str)
        
        elapsed_time = time.time() - start_time
        
        # Performance : moins de 3 secondes pour 3 orchestrations micro
        assert elapsed_time < 3.0
    
    def test_resource_management(self):
        """Test de gestion des ressources."""
        # Test de crÃ©ation/destruction multiple d'orchestrateurs
        orchestrators = []
        
        for i in range(5):
            orch = ConversationOrchestrator(mode="micro")
            orchestrators.append(orch)
        
        # Tous les orchestrateurs doivent Ãªtre crÃ©Ã©s correctement
        assert len(orchestrators) == 5
        
        # Test de nettoyage
        for orch in orchestrators:
            if hasattr(orch, 'cleanup'):
                orch.cleanup()


class TestRealLLMOrchestrationAdvanced:
    """Tests avancÃ©s pour RealLLMOrchestrator."""
    
    def setup_method(self):
        """Configuration initiale pour chaque test."""
        self.test_text = "L'Ukraine a Ã©tÃ© crÃ©Ã©e par la Russie. Donc Poutine a raison."
        self.mock_llm_service = MagicMock()
        self.mock_llm_service.invoke = AsyncMock(return_value="Mock LLM response")
    
    @pytest.mark.asyncio
    async def test_real_llm_orchestrator_initialization(self):
        """Test d'initialisation complÃ¨te du RealLLMOrchestrator."""
        orchestrator = RealLLMOrchestrator(
            mode="real", 
            llm_service=self.mock_llm_service
        )
        
        # Test d'initialisation
        if hasattr(orchestrator, 'initialize'):
            init_success = await orchestrator.initialize()
            assert isinstance(init_success, bool)
        
        assert hasattr(orchestrator, 'agents')
        assert hasattr(orchestrator, 'llm_service')
    
    @pytest.mark.asyncio
    async def test_bnf_feedback_integration(self):
        """Test d'intÃ©gration avec TweetyErrorAnalyzer pour feedback BNF."""
        orchestrator = RealLLMOrchestrator(llm_service=self.mock_llm_service)
        
        # Simuler une erreur Tweety
        error_text = "Predicate 'invalid_pred' has not been declared"
        
        analyzer = TweetyErrorAnalyzer()
        feedback = analyzer.analyze_error(error_text)
        
        assert hasattr(feedback, 'error_type')
        assert hasattr(feedback, 'corrections')
        assert hasattr(feedback, 'bnf_rules')
    
    @pytest.mark.asyncio
    async def test_intelligent_retry_mechanism(self):
        """Test du mÃ©canisme de retry intelligent."""
        # Mock LLM qui Ã©choue puis rÃ©ussit
        failing_llm = await self._create_authentic_gpt4o_mini_instance()
        failing_llm.invoke = AsyncMock(side_effect=[
            Exception("First attempt fails"),
            "Success on retry"
        ])
        
        orchestrator = RealLLMOrchestrator(llm_service=failing_llm)
        
        # Test de retry (si implÃ©mentÃ©)
        try:
            result = await orchestrator.run_real_llm_orchestration(self.test_text)
            # Si retry rÃ©ussi
            assert isinstance(result, dict)
        except Exception:
            # Si pas de retry, l'erreur est normale
            pass
    
    @pytest.mark.asyncio
    async def test_semantic_kernel_integration(self):
        """Test d'intÃ©gration avec Semantic Kernel."""
        orchestrator = RealLLMOrchestrator(llm_service=self.mock_llm_service)
        
        # Test d'initialisation du kernel
        if hasattr(orchestrator, 'initialize'):
            await orchestrator.initialize()
        
        # VÃ©rifier que le kernel est configurÃ©
        if hasattr(orchestrator, 'kernel'):
            assert orchestrator.kernel is not None or orchestrator.kernel is None  # Selon implÃ©mentation


class TestUnifiedSystemCoordination:
    """Tests de coordination systÃ¨me unifiÃ©e."""
    
    def setup_method(self):
        """Configuration initiale pour chaque test."""
        self.test_text = "L'Ukraine a Ã©tÃ© crÃ©Ã©e par la Russie. Donc Poutine a raison."
        self.unified_config = UnifiedConfig(
            logic_type='FOL',
            mock_level='NONE',
            orchestration_type='UNIFIED'
        )
    
    def test_config_to_orchestration_mapping(self):
        """Test du mapping configuration vers orchestration."""
        # Test avec configuration conversation
        conv_config = UnifiedConfig(orchestration_type='CONVERSATION')
        conv_orchestrator = ConversationOrchestrator(config=conv_config)
        
        assert conv_orchestrator is not None
        
        # Test avec configuration LLM rÃ©el
        real_config = UnifiedConfig(orchestration_type='REAL_LLM')
        real_orchestrator = RealLLMOrchestrator(config=real_config)
        
        assert real_orchestrator is not None
    
    def test_agent_to_orchestrator_communication(self):
        """Test de communication agent vers orchestrateur."""
        orchestrator = ConversationOrchestrator(mode="demo")
        
        # Test que les agents peuvent communiquer avec l'orchestrateur
        if hasattr(orchestrator, 'agents') and orchestrator.agents:
            # VÃ©rifier que les agents ont accÃ¨s Ã  l'orchestrateur
            for agent in orchestrator.agents:
                if hasattr(agent, 'orchestrator'):
                    assert agent.orchestrator is not None
    
    @pytest.mark.asyncio
    async def test_conversation_to_real_llm_handoff(self):
        """Test de handoff entre orchestrateurs."""
        # Phase 1: Orchestration conversationnelle
        conv_orchestrator = ConversationOrchestrator(mode="demo")
        conv_result = conv_orchestrator.run_orchestration(self.test_text)
        
        assert isinstance(conv_result, str)
        
        # Phase 2: Transfert vers orchestration LLM rÃ©elle
        mock_llm = MagicMock()
        real_orchestrator = RealLLMOrchestrator(llm_service=mock_llm)
        
        # Simuler le transfert d'Ã©tat
        if hasattr(real_orchestrator, 'load_state') and hasattr(conv_orchestrator, 'get_state'):
            state = conv_orchestrator.get_state()
            if state:
                real_orchestrator.load_state(state)
        
        # Test de continuitÃ©
        real_result = await real_orchestrator.run_real_llm_orchestration(self.test_text)
        assert isinstance(real_result, dict)
    
    def test_authentic_mode_validation(self):
        """Test de validation du mode authentique."""
        # Configuration authentique complÃ¨te
        authentic_config = UnifiedConfig(
            logic_type='FOL',
            mock_level='NONE',
            require_real_gpt=True,
            require_real_tweety=True
        )
        
        orchestrator = ConversationOrchestrator(config=authentic_config)
        
        # VÃ©rifier que le mode authentique est respectÃ©
        if hasattr(orchestrator, 'is_authentic_mode'):
            assert orchestrator.is_authentic_mode()
        
        # VÃ©rifier qu'aucun mock n'est utilisÃ© en mode authentique
        if hasattr(orchestrator, 'config'):
            assert orchestrator.config.mock_level == 'NONE'


class TestOrchestrationPerformanceAndRobustness:
    """Tests de performance et robustesse des orchestrations."""
    
    def test_concurrent_orchestrations(self):
        """Test d'orchestrations concurrentes."""
        import threading
        import queue
        
        results = queue.Queue()
        
        def run_orchestration(text_id):
            orchestrator = ConversationOrchestrator(mode="micro")
            result = orchestrator.run_orchestration(f"Test concurrent {text_id}")
            results.put((text_id, result))
        
        # Lancer plusieurs orchestrations en parallÃ¨le
        threads = []
        for i in range(3):
            thread = threading.Thread(target=run_orchestration, args=(i,))
            threads.append(thread)
            thread.start()
        
        # Attendre la fin
        for thread in threads:
            thread.join(timeout=5.0)
        
        # VÃ©rifier les rÃ©sultats
        assert results.qsize() == 3
        
        collected_results = []
        while not results.empty():
            collected_results.append(results.get())
        
        assert len(collected_results) == 3
    
    @pytest.mark.asyncio
    async def test_memory_usage_stability(self):
        """Test de stabilitÃ© de l'utilisation mÃ©moire."""
        import gc
        
        orchestrator = ConversationOrchestrator(mode="micro")
        
        # ExÃ©cuter plusieurs orchestrations
        for i in range(10):
            text = f"Test mÃ©moire {i}: " + "Data " * 100
            result = orchestrator.run_orchestration(text)
            assert isinstance(result, str)
            
            # Forcer garbage collection
            if i % 3 == 0:
                gc.collect()
        
        # Test que l'orchestrateur est toujours fonctionnel
        final_result = orchestrator.run_orchestration("Test final")
        assert isinstance(final_result, str)
    
    def test_error_propagation_and_handling(self):
        """Test de propagation et gestion d'erreurs."""
        orchestrator = ConversationOrchestrator(mode="demo")
        
        # Test avec diffÃ©rents types d'erreurs
        error_cases = [
            ("", "empty_text"),
            ("A" * 50000, "very_long_text"),
            ("ðŸ”¥ðŸ’¥âš¡", "special_chars_only")
        ]
        
        for text, case_name in error_cases:
            try:
                result = orchestrator.run_orchestration(text)
                # Si aucune erreur, vÃ©rifier que le rÃ©sultat est valide
                assert isinstance(result, str)
                print(f"âœ“ Cas {case_name}: GÃ©rÃ© gracieusement")
            except Exception as e:
                # Si erreur, vÃ©rifier qu'elle est appropriÃ©e
                assert isinstance(e, (ValueError, TypeError, RuntimeError))
                print(f"âœ“ Cas {case_name}: Erreur appropriÃ©e - {type(e).__name__}")
    
    def test_orchestration_chain_stability(self):
        """Test de stabilitÃ© des chaÃ®nes d'orchestration."""
        texts = [
            "Premier texte d'analyse.",
            "DeuxiÃ¨me texte plus complexe avec argumentation.",
            "TroisiÃ¨me texte final pour vÃ©rifier la stabilitÃ©."
        ]
        
        orchestrator = ConversationOrchestrator(mode="demo")
        
        results = []
        for i, text in enumerate(texts):
            result = orchestrator.run_orchestration(text)
            results.append(result)
            
            # VÃ©rifier la consistance
            assert isinstance(result, str)
            assert len(result) > 0
            
            # L'orchestrateur doit rester stable
            assert hasattr(orchestrator, 'mode')
            assert orchestrator.mode == "demo"
        
        # Tous les rÃ©sultats doivent Ãªtre uniques et valides
        assert len(results) == 3
        assert len(set(results)) >= 1  # Au moins un rÃ©sultat unique


@pytest.mark.skipif(not REAL_COMPONENTS_AVAILABLE, reason="Composants rÃ©els non disponibles")
class TestAuthenticOrchestrationIntegration:
    """Tests d'intÃ©gration authentique (sans mocks)."""
    
    def test_fol_agent_integration(self):
        """Test d'intÃ©gration avec FirstOrderLogicAgent rÃ©el."""
        config = UnifiedConfig(logic_type='FOL', mock_level='NONE')
        orchestrator = ConversationOrchestrator(config=config)
        
        # VÃ©rifier que l'agent FOL est configurÃ©
        if hasattr(orchestrator, 'agents'):
            fol_agents = [a for a in orchestrator.agents if 'FOL' in str(type(a).__name__)]
            if fol_agents:
                assert len(fol_agents) > 0
    
    def test_tweety_error_analyzer_integration(self):
        """Test d'intÃ©gration avec TweetyErrorAnalyzer rÃ©el."""
        analyzer = TweetyErrorAnalyzer()
        
        # Test avec erreur Tweety rÃ©elle
        error_text = "Predicate 'unknown_pred' has not been declared in rule"
        feedback = analyzer.analyze_error(error_text)
        
        assert feedback.error_type is not None
        assert len(feedback.corrections) > 0
        assert feedback.confidence > 0.0
    
    def test_unified_config_full_integration(self):
        """Test d'intÃ©gration complÃ¨te avec UnifiedConfig."""
        config = UnifiedConfig(
            logic_type='FOL',
            mock_level='NONE',
            orchestration_type='UNIFIED',
            require_real_gpt=True,
            require_real_tweety=True
        )
        
        assert config.logic_type == 'FOL'
        assert config.mock_level == 'NONE'
        assert config.require_real_gpt == True
        assert config.require_real_tweety == True


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
