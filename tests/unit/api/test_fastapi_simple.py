#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tests unitaires simples pour validation API FastAPI avec GPT-4o-mini
==================================================================

Tests simplifiés pour Point d'Entrée 2 : Applications Web
"""

# AUTO_ENV: Activation automatique environnement
try:
    import scripts.core.auto_env  # Auto-activation environnement intelligent
except ImportError:
    print("[WARNING] auto_env non disponible - environnement non activé")

import pytest
import time
import requests
import subprocess
import os
import threading
from pathlib import Path
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Configuration du test
API_BASE_URL = "http://localhost:8001"
API_PORT = 8001
TEST_TIMEOUT = 45

class TestAPIFastAPISimple:
    """Tests unitaires simplifiés pour l'API FastAPI avec GPT-4o-mini authentique."""
    
    @classmethod
    def setup_class(cls):
        """Configuration initiale des tests."""
        cls.api_process = None
        cls.api_started = False
        
    @classmethod
    def teardown_class(cls):
        """Nettoyage après tous les tests."""
        if cls.api_process:
            cls.api_process.terminate()
            try:
                cls.api_process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                cls.api_process.kill()
    
    def test_01_environment_verification(self):
        """Test 1: Vérification de l'environnement de base."""
        # Vérifier la clé OpenAI
        api_key = os.getenv('OPENAI_API_KEY')
        assert api_key is not None, "OPENAI_API_KEY non trouvée"
        assert len(api_key) > 20, "OPENAI_API_KEY semble invalide"
        
        # Vérifier que les fichiers API existent
        api_files = [
            'api/main_simple.py',
            'api/endpoints_simple.py', 
            'api/dependencies_simple.py'
        ]
        
        for file_path in api_files:
            assert Path(file_path).exists(), f"Fichier API manquant: {file_path}"
    
    def test_02_start_api_via_command(self):
        """Test 2: Démarrage du serveur API via ligne de commande."""
        def start_api_server():
            """Démarre l'API via uvicorn en ligne de commande."""
            try:
                cmd = [
                    "python", "-m", "uvicorn", 
                    "api.main_simple:app",
                    "--host", "127.0.0.1",
                    "--port", str(API_PORT),
                    "--log-level", "info"
                ]
                
                self.__class__.api_process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    cwd=os.getcwd(),
                    env=dict(os.environ, PYTHONPATH=os.getcwd())
                )
                print(f"API process started with PID: {self.api_process.pid}")
                
            except Exception as e:
                print(f"Erreur démarrage API: {e}")
        
        # Démarrer l'API dans un thread
        api_thread = threading.Thread(target=start_api_server, daemon=True)
        api_thread.start()
        
        # Attendre que l'API soit prête
        start_time = time.time()
        while time.time() - start_time < TEST_TIMEOUT:
            try:
                response = requests.get(f"{API_BASE_URL}/health", timeout=3)
                if response.status_code == 200:
                    self.__class__.api_started = True
                    print(f"API démarrée avec succès après {time.time() - start_time:.1f}s")
                    break
            except (requests.ConnectionError, requests.Timeout):
                pass
            time.sleep(2)
        
        assert self.api_started, f"API n'a pas démarré dans les {TEST_TIMEOUT}s"
    
    def test_03_health_check(self):
        """Test 3: Vérification de l'endpoint de santé."""
        if not self.api_started:
            pytest.skip("API non démarrée")
            
        response = requests.get(f"{API_BASE_URL}/health", timeout=10)
        assert response.status_code == 200
        
        data = response.json()
        assert "status" in data
        assert data["status"] == "healthy"
        print(f"Health check OK: {data}")
    
    def test_04_examples_endpoint(self):
        """Test 4: Vérification de l'endpoint des exemples."""
        if not self.api_started:
            pytest.skip("API non démarrée")
            
        response = requests.get(f"{API_BASE_URL}/examples", timeout=10)
        assert response.status_code == 200
        
        data = response.json()
        assert "examples" in data
        assert len(data["examples"]) > 0
        print(f"Exemples trouvés: {len(data['examples'])}")
    
    def test_05_simple_analysis(self):
        """Test 5: Analyse simple avec GPT-4o-mini."""
        if not self.api_started:
            pytest.skip("API non démarrée")
            
        test_text = "Si il pleut, alors la route est mouillée. Il pleut. Donc la route est mouillée."
        
        print(f"Test d'analyse avec: {test_text}")
        start_time = time.time()
        
        response = requests.post(
            f"{API_BASE_URL}/analyze",
            json={"text": test_text},
            timeout=60  # GPT-4o-mini peut prendre du temps
        )
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        assert response.status_code == 200, f"Erreur API: {response.text}"
        
        data = response.json()
        assert "analysis_id" in data
        assert "analysis" in data
        assert "service_used" in data
        
        # Vérifier que l'analyse contient du contenu substantiel
        analysis = data["analysis"]
        assert len(analysis) > 20, f"Analyse trop courte ({len(analysis)} chars): {analysis}"
        
        # Vérifier que le service GPT-4o-mini est utilisé
        assert data["service_used"] == "openai_gpt4o_mini", f"Service incorrect: {data['service_used']}"
        
        # Vérifier temps de traitement (authentique vs mock)
        assert processing_time > 1.0, f"Temps trop rapide ({processing_time:.2f}s), possiblement un mock"
        
        print(f"Analyse réussie en {processing_time:.2f}s")
        print(f"Service utilisé: {data['service_used']}")
        print(f"Longueur analyse: {len(analysis)} caractères")
    
    def test_06_fallacy_detection(self):
        """Test 6: Détection de sophisme avec GPT-4o-mini."""
        if not self.api_started:
            pytest.skip("API non démarrée")
            
        # Texte avec sophisme ad hominem
        test_text = "Cette théorie est fausse parce que son auteur est un charlatan."
        
        print(f"Test détection sophisme: {test_text}")
        start_time = time.time()
        
        response = requests.post(
            f"{API_BASE_URL}/analyze",
            json={"text": test_text},
            timeout=60
        )
        
        processing_time = time.time() - start_time
        
        assert response.status_code == 200
        data = response.json()
        
        analysis = data["analysis"].lower()
        
        # Mots-clés indicatifs de détection de sophisme
        sophisme_indicators = [
            "sophisme", "fallacy", "ad hominem", "attaque personnelle", 
            "argument", "fallacieux", "erreur logique", "raisonnement"
        ]
        
        found_indicators = [kw for kw in sophisme_indicators if kw in analysis]
        
        print(f"Indicateurs trouvés: {found_indicators}")
        print(f"Extrait analyse: {analysis[:200]}...")
        
        # Au moins un indicateur de détection logique
        assert len(found_indicators) > 0, f"Aucun indicateur de détection logique trouvé dans: {analysis[:100]}"
        
        print(f"Détection sophisme réussie en {processing_time:.2f}s")
    
    def test_07_api_consistency(self):
        """Test 7: Vérification de la cohérence des réponses."""
        if not self.api_started:
            pytest.skip("API non démarrée")
            
        test_text = "Tous les hommes sont mortels. Socrate est un homme. Donc Socrate est mortel."
        
        # Faire plusieurs appels pour vérifier la variabilité
        responses = []
        times = []
        
        for i in range(2):  # Réduire à 2 appels pour accélérer
            start = time.time()
            response = requests.post(
                f"{API_BASE_URL}/analyze",
                json={"text": f"{test_text} (Test {i+1})"},
                timeout=60
            )
            times.append(time.time() - start)
            
            assert response.status_code == 200
            data = response.json()
            responses.append(data["analysis"])
        
        # Vérifier que les réponses sont différentes (signe d'authenticité GPT)
        if len(set(responses)) > 1:
            print("✓ Réponses variées - signe d'authenticité GPT")
        else:
            print("⚠ Réponses identiques - possible mock ou cache")
        
        avg_time = sum(times) / len(times)
        print(f"Temps moyen: {avg_time:.2f}s")
        
        # Vérifier que les temps sont réalistes
        assert avg_time > 0.5, f"Temps moyen trop rapide: {avg_time:.2f}s"

def pytest_main():
    """Point d'entrée pour exécuter les tests."""
    return pytest.main([__file__, "-v", "--tb=short", "-s"])

if __name__ == "__main__":
    pytest_main()