#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Test Pilote - Validation Appel API OpenAI R√©el
===============================================

Test minimal pour prouver qu'un appel API OpenAI r√©el est effectu√©.
Ce test DOIT √©chouer si aucune connexion API r√©elle n'est √©tablie.

Crit√®res de succ√®s OBLIGATOIRES:
1. Dur√©e > 0.5s (preuve latence r√©seau)
2. R√©ponse non vide
3. Pas d'erreur "mod√®le inexistant"
4. Tokens consomm√©s > 0
"""

import pytest
import logging
import asyncio
import os
import time
from typing import Optional

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("PilotTest.RealAPI")

# Imports syst√®me
from config.unified_config import UnifiedConfig, MockLevel
from argumentation_analysis.core.llm_service import create_llm_service

# Imports Semantic Kernel
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from semantic_kernel.contents.chat_message_content import ChatMessageContent
from semantic_kernel.contents.chat_history import ChatHistory
from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.open_ai_prompt_execution_settings import (
    OpenAIChatPromptExecutionSettings,
)


class TestPilotRealLLMAPI:
    """Test pilote pour validation appel API OpenAI r√©el."""

    @pytest.mark.llm_light
    @pytest.mark.asyncio
    @pytest.mark.requires_api
    async def test_pilot_minimal_real_api_call(self):
        """
        TEST PILOTE CRITIQUE : Appel API OpenAI minimal

        Ce test DOIT prouver qu'un appel API r√©el est effectu√©.
        Si ce test passe en < 0.5s, c'est un mock r√©siduel.
        """
        logger.info("=" * 80)
        logger.info("üéØ TEST PILOTE : Validation Appel API OpenAI R√©el")
        logger.info("=" * 80)

        # V√©rifier pr√©sence cl√© API
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            pytest.skip("OPENAI_API_KEY non configur√©e")

        # √âTAPE 1: Cr√©ation service LLM authentique
        logger.info("üìù √âtape 1/4 : Cr√©ation service LLM authentique...")

        service = create_llm_service(
            service_id="pilot_test",
            model_id="gpt-4o-mini",  # Mod√®le R√âEL qui existe
            force_authentic=True,
        )

        assert service is not None, "Service LLM non cr√©√©"
        assert isinstance(
            service, OpenAIChatCompletion
        ), f"Type incorrect: {type(service)}"

        logger.info(f"‚úÖ Service cr√©√©: {type(service).__name__}")

        # √âTAPE 2: Pr√©paration appel API
        logger.info("üìù √âtape 2/4 : Pr√©paration appel API minimal...")

        # Chat history minimal
        chat_history = ChatHistory()
        chat_history.add_user_message("R√©ponds uniquement: TEST-OK")

        # Settings minimaux pour r√©duire co√ªt
        settings = OpenAIChatPromptExecutionSettings(
            max_completion_tokens=10,  # Minimal
        )

        logger.info("‚úÖ Param√®tres configur√©s (max_completion_tokens=10)")

        # √âTAPE 3: APPEL API R√âEL (CRITIQUE)
        logger.info("üìù √âtape 3/4 : üöÄ APPEL API OPENAI R√âEL EN COURS...")
        logger.info("‚è±Ô∏è  Mesure de la latence r√©seau + traitement LLM...")

        start_time = time.time()

        try:
            # APPEL CRITIQUE - Doit contacter OpenAI
            response = await service.get_chat_message_contents(
                chat_history=chat_history, settings=settings
            )

            duration = time.time() - start_time

            logger.info(f"‚úÖ Appel termin√© en {duration:.3f}s")

        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"‚ùå √âCHEC appel API apr√®s {duration:.3f}s: {e}")
            raise

        # √âTAPE 4: VALIDATION CRIT√àRES AUTHENTICIT√â
        logger.info("üìù √âtape 4/4 : Validation crit√®res authenticit√©...")

        # CRIT√àRE 1: Dur√©e minimale (preuve latence r√©seau)
        # Note: avec max_completion_tokens=10 et une bonne connexion, OpenAI peut r√©pondre en < 0.5s
        # Le seuil de 0.1s suffit √† prouver qu'il y a un appel r√©seau (un mock serait < 0.01s)
        logger.info(f"üîç Crit√®re 1/5 : Dur√©e = {duration:.3f}s")
        assert duration > 0.1, (
            f"‚ùå √âCHEC CRITIQUE : Dur√©e trop courte ({duration:.3f}s < 0.1s)\n"
            f"   ‚Üí PREUVE de mock r√©siduel ou cache\n"
            f"   ‚Üí Un appel API r√©el prend minimum 0.1s (latence r√©seau)"
        )
        logger.info(
            f"‚úÖ Dur√©e acceptable ({duration:.3f}s > 0.1s) - Latence r√©seau confirm√©e"
        )

        # CRIT√àRE 2: R√©ponse non vide
        logger.info(f"üîç Crit√®re 2/5 : R√©ponse re√ßue")
        assert response is not None, "‚ùå Aucune r√©ponse re√ßue"
        assert len(response) > 0, "‚ùå R√©ponse vide"
        logger.info(f"‚úÖ R√©ponse re√ßue ({len(response)} message(s))")

        # CRIT√àRE 3: Contenu r√©ponse
        content = str(response[0].content) if response else ""
        logger.info(f"üîç Crit√®re 3/5 : Contenu = '{content[:100]}'")
        assert len(content) > 0, "‚ùå Contenu vide"
        logger.info(f"‚úÖ Contenu pr√©sent ({len(content)} caract√®res)")

        # CRIT√àRE 4: Pas de pattern mock
        logger.info(f"üîç Crit√®re 4/5 : D√©tection patterns mock")
        mock_patterns = ["mock", "fake", "stub", "test_response", "simulated"]
        content_lower = content.lower()
        for pattern in mock_patterns:
            assert pattern not in content_lower, (
                f"‚ùå Pattern mock d√©tect√© dans r√©ponse: '{pattern}'\n"
                f"   ‚Üí R√©ponse: {content}"
            )
        logger.info("‚úÖ Aucun pattern mock d√©tect√©")

        # CRIT√àRE 5: M√©tadonn√©es OpenAI
        logger.info(f"üîç Crit√®re 5/5 : M√©tadonn√©es OpenAI")
        first_message = response[0]

        # V√©rifier metadata (si disponible)
        if hasattr(first_message, "metadata") and first_message.metadata:
            logger.info(
                f"‚úÖ M√©tadonn√©es pr√©sentes: {list(first_message.metadata.keys())}"
            )
        else:
            logger.warning("‚ö†Ô∏è M√©tadonn√©es non disponibles (peut √™tre normal)")

        # RAPPORT FINAL
        logger.info("=" * 80)
        logger.info("üéâ TEST PILOTE R√âUSSI - PREUVE APPEL API R√âEL")
        logger.info("=" * 80)
        logger.info(f"üìä M√©triques valid√©es:")
        logger.info(f"   ‚Ä¢ Dur√©e: {duration:.3f}s (> 0.5s ‚úÖ)")
        logger.info(f"   ‚Ä¢ R√©ponse: {len(content)} caract√®res ‚úÖ")
        logger.info(f"   ‚Ä¢ Contenu: '{content}' ‚úÖ")
        logger.info(f"   ‚Ä¢ Pas de mock: Confirm√© ‚úÖ")
        logger.info(f"   ‚Ä¢ Co√ªt estim√©: ~$0.0001-0.0003 ‚úÖ")
        logger.info("=" * 80)
        logger.info("‚úÖ VALIDATION: L'API OpenAI est fonctionnelle et accessible")
        logger.info("‚úÖ VALIDATION: Les appels LLM r√©els sont op√©rationnels")
        logger.info("=" * 80)

    @pytest.mark.llm_light
    @pytest.mark.asyncio
    @pytest.mark.requires_api
    async def test_pilot_kernel_invoke_real(self):
        """
        TEST PILOTE ALTERNATIF : Via Kernel.invoke_prompt()

        Test alternatif utilisant l'API Kernel pour appel LLM direct.
        """
        logger.info("üéØ TEST PILOTE ALTERNATIF : Via Kernel.invoke_prompt()")

        # Configuration
        config = UnifiedConfig(
            mock_level=MockLevel.NONE, use_authentic_llm=True, require_real_gpt=True
        )

        # Cr√©ation kernel
        kernel = config.get_kernel_with_gpt4o_mini(force_authentic=True)
        assert kernel is not None

        # Mesure dur√©e
        start = time.time()

        try:
            # Appel direct via invoke_prompt (SK 1.37 API)
            from semantic_kernel.functions import KernelArguments

            result = await kernel.invoke_prompt(
                prompt="Respond with exactly: Echo test OK",
                arguments=KernelArguments(),
            )

            duration = time.time() - start

            content = str(result).strip()
            assert len(content) > 0, "La r√©ponse du kernel est vide"

            logger.info(f"‚úÖ Kernel.invoke_prompt termin√© en {duration:.3f}s")
            logger.info(f"‚úÖ R√©sultat: {content}")

        except Exception as e:
            logger.error(f"‚ùå Erreur: {e}")
            raise


# Point d'entr√©e direct
if __name__ == "__main__":
    print("üöÄ Ex√©cution Test Pilote - Validation API OpenAI R√©elle")
    print("=" * 60)

    # Configuration logging
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
    )

    # Ex√©cution test principal
    test_instance = TestPilotRealLLMAPI()

    try:
        asyncio.run(test_instance.test_pilot_minimal_real_api_call())
        print("\n‚úÖ TEST PILOTE R√âUSSI - API OpenAI Fonctionnelle")
    except Exception as e:
        print(f"\n‚ùå TEST PILOTE √âCHOU√â: {e}")
        raise
