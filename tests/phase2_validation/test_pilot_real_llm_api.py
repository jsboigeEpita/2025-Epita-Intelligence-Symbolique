#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Test Pilote - Validation Appel API OpenAI R√©el
===============================================

Test minimal pour prouver qu'un appel API OpenAI r√©el est effectu√©.
Ce test DOIT √©chouer si aucune connexion API r√©elle n'est √©tablie.

Crit√®res de succ√®s OBLIGATOIRES:
1. Dur√©e > 0.5s (preuve latence r√©seau)
2. R√©ponse non vide
3. Pas d'erreur "mod√®le inexistant"
4. Tokens consomm√©s > 0
"""

import pytest
import logging
import asyncio
import os
import time
from typing import Optional

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("PilotTest.RealAPI")

# Imports syst√®me
from config.unified_config import UnifiedConfig, MockLevel
from argumentation_analysis.core.llm_service import create_llm_service

# Imports Semantic Kernel
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from semantic_kernel.contents.chat_message_content import ChatMessageContent
from semantic_kernel.contents.chat_history import ChatHistory
from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.open_ai_prompt_execution_settings import (
    OpenAIChatPromptExecutionSettings,
)


class TestPilotRealLLMAPI:
    """Test pilote pour validation appel API OpenAI r√©el."""

    @pytest.mark.llm_light
    @pytest.mark.asyncio
    @pytest.mark.requires_api
    async def test_pilot_minimal_real_api_call(self):
        """
        TEST PILOTE CRITIQUE : Appel API OpenAI minimal
        
        Ce test DOIT prouver qu'un appel API r√©el est effectu√©.
        Si ce test passe en < 0.5s, c'est un mock r√©siduel.
        """
        logger.info("=" * 80)
        logger.info("üéØ TEST PILOTE : Validation Appel API OpenAI R√©el")
        logger.info("=" * 80)

        # V√©rifier pr√©sence cl√© API
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            pytest.skip("OPENAI_API_KEY non configur√©e")

        # √âTAPE 1: Cr√©ation service LLM authentique
        logger.info("üìù √âtape 1/4 : Cr√©ation service LLM authentique...")
        
        service = create_llm_service(
            service_id="pilot_test",
            model_id="gpt-4o-mini",  # Mod√®le R√âEL qui existe
            force_authentic=True
        )
        
        assert service is not None, "Service LLM non cr√©√©"
        assert isinstance(service, OpenAIChatCompletion), f"Type incorrect: {type(service)}"
        
        logger.info(f"‚úÖ Service cr√©√©: {type(service).__name__}")

        # √âTAPE 2: Pr√©paration appel API
        logger.info("üìù √âtape 2/4 : Pr√©paration appel API minimal...")
        
        # Chat history minimal
        chat_history = ChatHistory()
        chat_history.add_user_message("R√©ponds uniquement: TEST-OK")
        
        # Settings minimaux pour r√©duire co√ªt
        settings = OpenAIChatPromptExecutionSettings(
            max_tokens=10,  # Minimal
            temperature=0,  # D√©terministe
        )
        
        logger.info("‚úÖ Param√®tres configur√©s (max_tokens=10, temperature=0)")

        # √âTAPE 3: APPEL API R√âEL (CRITIQUE)
        logger.info("üìù √âtape 3/4 : üöÄ APPEL API OPENAI R√âEL EN COURS...")
        logger.info("‚è±Ô∏è  Mesure de la latence r√©seau + traitement LLM...")
        
        start_time = time.time()
        
        try:
            # APPEL CRITIQUE - Doit contacter OpenAI
            response = await service.get_chat_message_contents(
                chat_history=chat_history,
                settings=settings
            )
            
            duration = time.time() - start_time
            
            logger.info(f"‚úÖ Appel termin√© en {duration:.3f}s")
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"‚ùå √âCHEC appel API apr√®s {duration:.3f}s: {e}")
            raise

        # √âTAPE 4: VALIDATION CRIT√àRES AUTHENTICIT√â
        logger.info("üìù √âtape 4/4 : Validation crit√®res authenticit√©...")
        
        # CRIT√àRE 1: Dur√©e minimale (preuve latence r√©seau)
        logger.info(f"üîç Crit√®re 1/5 : Dur√©e = {duration:.3f}s")
        assert duration > 0.5, (
            f"‚ùå √âCHEC CRITIQUE : Dur√©e trop courte ({duration:.3f}s < 0.5s)\n"
            f"   ‚Üí PREUVE de mock r√©siduel ou cache\n"
            f"   ‚Üí Un appel API r√©el prend minimum 0.5-1s (latence r√©seau)"
        )
        logger.info(f"‚úÖ Dur√©e acceptable ({duration:.3f}s > 0.5s) - Latence r√©seau confirm√©e")
        
        # CRIT√àRE 2: R√©ponse non vide
        logger.info(f"üîç Crit√®re 2/5 : R√©ponse re√ßue")
        assert response is not None, "‚ùå Aucune r√©ponse re√ßue"
        assert len(response) > 0, "‚ùå R√©ponse vide"
        logger.info(f"‚úÖ R√©ponse re√ßue ({len(response)} message(s))")
        
        # CRIT√àRE 3: Contenu r√©ponse
        content = str(response[0].content) if response else ""
        logger.info(f"üîç Crit√®re 3/5 : Contenu = '{content[:100]}'")
        assert len(content) > 0, "‚ùå Contenu vide"
        logger.info(f"‚úÖ Contenu pr√©sent ({len(content)} caract√®res)")
        
        # CRIT√àRE 4: Pas de pattern mock
        logger.info(f"üîç Crit√®re 4/5 : D√©tection patterns mock")
        mock_patterns = ["mock", "fake", "stub", "test_response", "simulated"]
        content_lower = content.lower()
        for pattern in mock_patterns:
            assert pattern not in content_lower, (
                f"‚ùå Pattern mock d√©tect√© dans r√©ponse: '{pattern}'\n"
                f"   ‚Üí R√©ponse: {content}"
            )
        logger.info("‚úÖ Aucun pattern mock d√©tect√©")
        
        # CRIT√àRE 5: M√©tadonn√©es OpenAI
        logger.info(f"üîç Crit√®re 5/5 : M√©tadonn√©es OpenAI")
        first_message = response[0]
        
        # V√©rifier metadata (si disponible)
        if hasattr(first_message, 'metadata') and first_message.metadata:
            logger.info(f"‚úÖ M√©tadonn√©es pr√©sentes: {list(first_message.metadata.keys())}")
        else:
            logger.warning("‚ö†Ô∏è M√©tadonn√©es non disponibles (peut √™tre normal)")
        
        # RAPPORT FINAL
        logger.info("=" * 80)
        logger.info("üéâ TEST PILOTE R√âUSSI - PREUVE APPEL API R√âEL")
        logger.info("=" * 80)
        logger.info(f"üìä M√©triques valid√©es:")
        logger.info(f"   ‚Ä¢ Dur√©e: {duration:.3f}s (> 0.5s ‚úÖ)")
        logger.info(f"   ‚Ä¢ R√©ponse: {len(content)} caract√®res ‚úÖ")
        logger.info(f"   ‚Ä¢ Contenu: '{content}' ‚úÖ")
        logger.info(f"   ‚Ä¢ Pas de mock: Confirm√© ‚úÖ")
        logger.info(f"   ‚Ä¢ Co√ªt estim√©: ~$0.0001-0.0003 ‚úÖ")
        logger.info("=" * 80)
        logger.info("‚úÖ VALIDATION: L'API OpenAI est fonctionnelle et accessible")
        logger.info("‚úÖ VALIDATION: Les appels LLM r√©els sont op√©rationnels")
        logger.info("=" * 80)
        
    @pytest.mark.llm_light
    @pytest.mark.asyncio
    @pytest.mark.requires_api
    async def test_pilot_kernel_invoke_real(self):
        """
        TEST PILOTE ALTERNATIF : Via Kernel.invoke() 
        
        Test alternatif utilisant l'API Kernel pour appel LLM.
        """
        logger.info("üéØ TEST PILOTE ALTERNATIF : Via Kernel.invoke()")
        
        # Configuration
        config = UnifiedConfig(
            mock_level=MockLevel.NONE,
            use_authentic_llm=True,
            require_real_gpt=True
        )
        
        # Cr√©ation kernel
        kernel = config.get_kernel_with_gpt4o_mini(force_authentic=True)
        assert kernel is not None
        
        # Fonction simple
        @kernel.function(name="test_func", description="Test function")
        def simple_test(input: str) -> str:
            """Simple echo function"""
            return f"Echo: {input}"
        
        # Mesure dur√©e
        start = time.time()
        
        try:
            # Appel via kernel (devrait utiliser LLM si configur√©)
            result = await kernel.invoke(
                function=simple_test,
                input="test"
            )
            
            duration = time.time() - start
            
            # Note: kernel.invoke avec function peut ne pas utiliser LLM
            # C'est surtout un test de configuration
            logger.info(f"‚úÖ Kernel.invoke termin√© en {duration:.3f}s")
            logger.info(f"‚úÖ R√©sultat: {result}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur: {e}")
            raise


# Point d'entr√©e direct
if __name__ == "__main__":
    print("üöÄ Ex√©cution Test Pilote - Validation API OpenAI R√©elle")
    print("=" * 60)
    
    # Configuration logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s"
    )
    
    # Ex√©cution test principal
    test_instance = TestPilotRealLLMAPI()
    
    try:
        asyncio.run(test_instance.test_pilot_minimal_real_api_call())
        print("\n‚úÖ TEST PILOTE R√âUSSI - API OpenAI Fonctionnelle")
    except Exception as e:
        print(f"\n‚ùå TEST PILOTE √âCHOU√â: {e}")
        raise