"""
Utilitaires pour l'interface utilisateur (UI) de l'analyse d'argumentation.

Ce module regroupe diverses fonctions d'assistance utilis√©es par les composants
de l'interface utilisateur, telles que la reconstruction d'URL, la gestion
d'un cache de fichiers simple, la r√©cup√©ration de contenu textuel depuis
diff√©rentes sources (direct, Jina, Tika), et la v√©rification des
d√©finitions d'extraits.
"""
# ui/utils.py
import requests
import json
import gzip
import hashlib
import logging
import base64 # NOUVEAU: Pour d√©coder la cl√© avant de l'utiliser avec Fernet
from pathlib import Path
from typing import Optional, List, Dict, Any, Union
# cryptography.fernet et exceptions sont maintenant g√©r√©s dans project_core.utils.crypto_utils
# from cryptography.fernet import Fernet, InvalidToken # SUPPRIM√â
# from cryptography.exceptions import InvalidSignature # SUPPRIM√â

# Le reste des imports et du code...

# Import config depuis le m√™me package ui
from . import config as ui_config

utils_logger = logging.getLogger("App.UI.Utils")
if not utils_logger.handlers and not utils_logger.propagate:
     handler = logging.StreamHandler(); formatter = logging.Formatter('%(asctime)s [%(levelname)s] [%(name)s] %(message)s', datefmt='%H:%M:%S'); handler.setFormatter(formatter); utils_logger.addHandler(handler); utils_logger.setLevel(logging.INFO)

# Import des fonctions de chiffrement d√©plac√©es
from project_core.utils.crypto_utils import encrypt_data_with_fernet, decrypt_data_with_fernet

# --- Fonctions Utilitaires (Cache, Crypto, Fetch, Verify) ---

def reconstruct_url(schema: str, host_parts: List[str], path: Optional[str]) -> Optional[str]:
    """Reconstruit une URL compl√®te √† partir de ses composants.

    :param schema: Le sch√©ma de l'URL (par exemple, "http", "https").
    :type schema: str
    :param host_parts: Une liste des parties composant le nom d'h√¥te.
    :type host_parts: List[str]
    :param path: Le chemin de la ressource sur le serveur. Peut √™tre None ou vide.
    :type path: Optional[str]
    :return: L'URL reconstruite, ou None si `schema` ou `host_parts` sont invalides.
    :rtype: Optional[str]
    """
    if not schema or not host_parts: return None # Path peut √™tre vide et g√©r√© ci-dessous
    host = ".".join(part for part in host_parts if part)
    # Si path est None, on le traite comme une cha√Æne vide pour la logique suivante
    current_path = path if path is not None else ""
    current_path = current_path if current_path.startswith('/') or not current_path else '/' + current_path
    # S'assurer qu'un path vide apr√®s traitement devienne au moins "/"
    if not current_path:
        current_path = "/"
    return f"{schema}://{host}{current_path}"

def get_cache_filepath(url: str) -> Path:
    """G√©n√®re le chemin du fichier cache pour une URL donn√©e.

    Le nom du fichier est un hachage SHA256 de l'URL, stock√© dans `ui_config.CACHE_DIR`.

    :param url: L'URL pour laquelle g√©n√©rer le chemin du fichier cache.
    :type url: str
    :return: Le chemin (objet Path) vers le fichier cache.
    :rtype: Path
    """
    url_hash = hashlib.sha256(url.encode()).hexdigest()
    # Utilise CACHE_DIR import√© depuis config
    return ui_config.CACHE_DIR / f"{url_hash}.txt"

def load_from_cache(url: str) -> Optional[str]:
    """Charge le contenu textuel depuis le cache fichier si disponible pour une URL donn√©e.

    :param url: L'URL √† rechercher dans le cache.
    :type url: str
    :return: Le contenu textuel en tant que cha√Æne si trouv√©, sinon None.
    :rtype: Optional[str]
    """
    filepath = get_cache_filepath(url)
    if filepath.exists():
        try:
            utils_logger.info(f"   -> Lecture depuis cache : {filepath.name}")
            return filepath.read_text(encoding='utf-8')
        except Exception as e:
            utils_logger.warning(f"   -> Erreur lecture cache {filepath.name}: {e}")
            return None
    utils_logger.debug(f"Cache miss pour URL: {url}")
    return None

def save_to_cache(url: str, text: str) -> None:
    """Sauvegarde le contenu textuel dans le cache fichier pour une URL donn√©e.

    Ne fait rien si le texte est vide. Cr√©e le r√©pertoire de cache si n√©cessaire.

    :param url: L'URL associ√©e au contenu.
    :type url: str
    :param text: Le contenu textuel √† sauvegarder.
    :type text: str
    :return: None
    :rtype: None
    """
    if not text:
        utils_logger.info("   -> Texte vide, non sauvegard√©.")
        return
    filepath = get_cache_filepath(url)
    try:
        # S'assurer que le dossier cache existe
        filepath.parent.mkdir(parents=True, exist_ok=True)
        filepath.write_text(text, encoding='utf-8')
        utils_logger.info(f"   -> Texte sauvegard√© : {filepath.name}")
    except Exception as e:
        utils_logger.error(f"   -> Erreur sauvegarde cache {filepath.name}: {e}")

# Les fonctions encrypt_data et decrypt_data ont √©t√© d√©plac√©es vers project_core.utils.crypto_utils
# et renomm√©es en encrypt_data_with_fernet et decrypt_data_with_fernet.
# Les appels dans file_operations.py devront √™tre mis √† jour.

# Les fonctions load_extract_definitions et save_extract_definitions ont √©t√© d√©plac√©es
# vers argumentation_analysis/ui/file_operations.py pour √©viter les imports circulaires.
def fetch_direct_text(source_url: str, timeout: int = 60) -> Optional[str]:
    """R√©cup√®re le contenu texte brut d'une URL par t√©l√©chargement direct.

    Utilise le cache fichier (`load_from_cache`, `save_to_cache`).

    :param source_url: L'URL √† partir de laquelle r√©cup√©rer le texte.
    :type source_url: str
    :param timeout: Le d√©lai d'attente en secondes pour la requ√™te HTTP.
    :type timeout: int
    :return: Le contenu textuel de la r√©ponse, ou None en cas d'erreur.
    :rtype: Optional[str]
    :raises ConnectionError: Si une erreur de requ√™te HTTP survient.
    """
    # Utilise les fonctions de cache de ce module
    cached_text = load_from_cache(source_url)
    if cached_text is not None: return cached_text
    utils_logger.info(f"-> T√©l√©chargement direct depuis : {source_url}...")
    headers = {'User-Agent': 'ArgumentAnalysisApp/1.0'}
    try:
        response = requests.get(source_url, headers=headers, timeout=timeout)
        response.raise_for_status()
        texte_brut = response.content.decode('utf-8', errors='ignore')
        utils_logger.info(f"   -> Contenu direct r√©cup√©r√© (longueur {len(texte_brut)}).")
        save_to_cache(source_url, texte_brut)
        return texte_brut
    except requests.exceptions.RequestException as e:
        utils_logger.error(f"Erreur t√©l√©chargement direct ({source_url}): {e}")
        raise ConnectionError(f"Erreur t√©l√©chargement direct ({source_url}): {e}") from e

def fetch_with_jina(
    source_url: str,
    timeout: int = 90,
    jina_reader_prefix_override: Optional[str] = None
) -> Optional[str]:
    """R√©cup√®re et extrait le contenu textuel d'une URL via le service Jina Reader.

    Utilise le cache fichier. Le pr√©fixe Jina peut √™tre surcharg√©.

    :param source_url: L'URL originale √† traiter.
    :type source_url: str
    :param timeout: Le d√©lai d'attente pour la requ√™te HTTP vers Jina.
    :type timeout: int
    :param jina_reader_prefix_override: Pr√©fixe optionnel pour surcharger celui de `ui_config`.
    :type jina_reader_prefix_override: Optional[str]
    :return: Le contenu textuel extrait (potentiellement Markdown), ou None en cas d'erreur.
    :rtype: Optional[str]
    :raises ConnectionError: Si une erreur de requ√™te HTTP survient avec Jina.
    """
    # Utilise les fonctions de cache de ce module
    cached_text = load_from_cache(source_url)
    if cached_text is not None: return cached_text

    _jina_reader_prefix = jina_reader_prefix_override if jina_reader_prefix_override is not None else ui_config.JINA_READER_PREFIX
    jina_url = f"{_jina_reader_prefix}{source_url}"

    utils_logger.info(f"-> R√©cup√©ration via Jina : {jina_url}...")
    headers = {'Accept': 'text/markdown', 'User-Agent': 'ArgumentAnalysisApp/1.0'}
    try:
        response = requests.get(jina_url, headers=headers, timeout=timeout)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        utils_logger.error(f"Erreur Jina ({jina_url}): {e}")
        raise ConnectionError(f"Erreur Jina ({jina_url}): {e}") from e
    content = response.text
    md_start_marker = "Markdown Content:"
    md_start_index = content.find(md_start_marker)
    texte_brut = content[md_start_index + len(md_start_marker):].strip() if md_start_index != -1 else content
    utils_logger.info(f"   -> Contenu Jina r√©cup√©r√© (longueur {len(texte_brut)}).")
    save_to_cache(source_url, texte_brut)
    return texte_brut

def fetch_with_tika(
    source_url: Optional[str] = None,
    file_content: Optional[bytes] = None,
    file_name: str = "fichier",
    raw_file_cache_path: Optional[Union[Path, str]] = None,
    timeout_dl: int = 60,
    timeout_tika: int = 600,
    tika_server_url_override: Optional[str] = None,
    plaintext_extensions_override: Optional[List[str]] = None,
    temp_download_dir_override: Optional[Path] = None
    ) -> Optional[str]:
    """Traite une source (URL ou contenu binaire) via un serveur Apache Tika.

    G√®re le cache du texte extrait et optionnellement un cache pour le fichier brut t√©l√©charg√©.
    Tente un fetch direct pour les types de fichiers texte simple reconnus.

    :param source_url: URL optionnelle du fichier √† traiter.
    :type source_url: Optional[str]
    :param file_content: Contenu binaire optionnel du fichier.
    :type file_content: Optional[bytes]
    :param file_name: Nom du fichier (utilis√© si `file_content` est fourni).
    :type file_name: str
    :param raw_file_cache_path: Chemin optionnel pour le cache du fichier brut.
    :type raw_file_cache_path: Optional[Union[Path, str]]
    :param timeout_dl: Timeout pour le t√©l√©chargement si `source_url` est utilis√©.
    :type timeout_dl: int
    :param timeout_tika: Timeout pour la requ√™te au serveur Tika.
    :type timeout_tika: int
    :param tika_server_url_override: URL optionnelle pour surcharger celle de `ui_config`.
    :type tika_server_url_override: Optional[str]
    :param plaintext_extensions_override: Liste optionnelle pour surcharger celles de `ui_config`.
    :type plaintext_extensions_override: Optional[List[str]]
    :param temp_download_dir_override: Chemin optionnel pour surcharger celui de `ui_config`.
    :type temp_download_dir_override: Optional[Path]
    :return: Le texte extrait par Tika, ou une cha√Æne vide si Tika ne retourne rien,
             ou None en cas d'erreur majeure.
    :rtype: Optional[str]
    :raises ValueError: Si ni `source_url` ni `file_content` ne sont fournis.
    :raises ConnectionError: Si une erreur de t√©l√©chargement ou de communication Tika survient.
    """
    _tika_server_url = tika_server_url_override if tika_server_url_override is not None else ui_config.TIKA_SERVER_URL
    _plaintext_extensions = plaintext_extensions_override if plaintext_extensions_override is not None else ui_config.PLAINTEXT_EXTENSIONS
    _temp_download_dir = temp_download_dir_override if temp_download_dir_override is not None else ui_config.TEMP_DOWNLOAD_DIR

    cache_key = source_url if source_url else f"file://{file_name}"
    cached_text = load_from_cache(cache_key)
    if cached_text is not None: return cached_text

    content_to_send = None
    # temp_download_dir = ui_config.TEMP_DOWNLOAD_DIR # Utiliser le chemin depuis config
    # Remplac√© par _temp_download_dir

    if source_url:
        original_filename = Path(source_url).name
        if any(source_url.lower().endswith(ext) for ext in _plaintext_extensions):
            utils_logger.info(f"   -> URL d√©tect√©e comme texte simple ({source_url}). Fetch direct.")
            return fetch_direct_text(source_url)

        url_hash = hashlib.sha256(source_url.encode()).hexdigest()
        file_extension = Path(original_filename).suffix if Path(original_filename).suffix else ".download"
        effective_raw_cache_path = Path(raw_file_cache_path) if raw_file_cache_path else _temp_download_dir / f"{url_hash}{file_extension}"

        if effective_raw_cache_path.exists() and effective_raw_cache_path.stat().st_size > 0:
             try:
                utils_logger.info(f"   -> Lecture fichier brut depuis cache local : {effective_raw_cache_path.name}")
                content_to_send = effective_raw_cache_path.read_bytes()
             except Exception as e_read_raw:
                utils_logger.warning(f"   -> Erreur lecture cache brut {effective_raw_cache_path.name}: {e_read_raw}. Re-t√©l√©chargement...")
                content_to_send = None

        if content_to_send is None:
             utils_logger.info(f"-> T√©l√©chargement (pour Tika) depuis : {source_url}...")
             try:
                 response_dl = requests.get(source_url, stream=True, timeout=timeout_dl)
                 response_dl.raise_for_status()
                 content_to_send = response_dl.content
                 utils_logger.info(f"   -> Doc t√©l√©charg√© ({len(content_to_send)} bytes).")
                 try:
                     effective_raw_cache_path.parent.mkdir(parents=True, exist_ok=True)
                     effective_raw_cache_path.write_bytes(content_to_send)
                     utils_logger.info(f"   -> Doc brut sauvegard√©: {effective_raw_cache_path.resolve()}")
                 except Exception as e_save:
                     utils_logger.error(f"   -> Erreur sauvegarde brut: {e_save}")
             except requests.exceptions.RequestException as e:
                 utils_logger.error(f"Erreur t√©l√©chargement {source_url}: {e}")
                 raise ConnectionError(f"Erreur t√©l√©chargement {source_url}: {e}") from e

    elif file_content:
        utils_logger.info(f"-> Utilisation contenu fichier '{file_name}' ({len(file_content)} bytes)...")
        content_to_send = file_content
        if any(file_name.lower().endswith(ext) for ext in _plaintext_extensions):
            utils_logger.info("   -> Fichier upload√© d√©tect√© comme texte simple. Lecture directe.")
            try:
                texte_brut = file_content.decode('utf-8', errors='ignore')
                save_to_cache(cache_key, texte_brut)
                return texte_brut
            except Exception as e_decode:
                utils_logger.warning(f"   -> Erreur d√©codage fichier texte '{file_name}': {e_decode}. Tentative avec Tika...")
    else:
        raise ValueError("fetch_with_tika: Il faut soit source_url soit file_content.")

    if not content_to_send:
        utils_logger.warning("   -> Contenu brut vide ou non r√©cup√©r√©. Impossible d'envoyer √† Tika.")
        save_to_cache(cache_key, "") # Sauvegarder une cha√Æne vide pour √©viter re-fetch inutile
        return ""

    utils_logger.info(f"-> Envoi contenu √† Tika ({_tika_server_url})... (Timeout={timeout_tika}s)")
    headers = { 'Accept': 'text/plain', 'Content-Type': 'application/octet-stream', 'X-Tika-OCRLanguage': 'fra+eng' }
    try:
        response_tika = requests.put(_tika_server_url, data=content_to_send, headers=headers, timeout=timeout_tika)
        response_tika.raise_for_status()
        texte_brut = response_tika.text
        if not texte_brut: utils_logger.warning(f"   -> Warning: Tika status {response_tika.status_code} sans texte.")
        else: utils_logger.info(f"   -> Texte Tika extrait (longueur {len(texte_brut)}).")
        save_to_cache(cache_key, texte_brut) # Appel fonction de ce module
        return texte_brut
    except requests.exceptions.Timeout:
        utils_logger.error(f"   -> ‚ùå Timeout Tika ({timeout_tika}s).")
        raise ConnectionError(f"Timeout Tika")
    except requests.exceptions.RequestException as e:
        utils_logger.error(f"Erreur Tika: {e}")
        raise ConnectionError(f"Erreur Tika: {e}") from e


        if not texte_brut: utils_logger.warning(f"   -> Warning: Tika status {response_tika.status_code} sans texte.")
        else: utils_logger.info(f"   -> Texte Tika extrait (longueur {len(texte_brut)}).")
        save_to_cache(cache_key, texte_brut)
        return texte_brut
    except requests.exceptions.Timeout:
        utils_logger.error(f"   -> ‚ùå Timeout Tika ({timeout_tika}s).")
        raise ConnectionError(f"Timeout Tika ({timeout_tika}s)") # Renvoyer une erreur plus sp√©cifique
    except requests.exceptions.RequestException as e:
        utils_logger.error(f"Erreur Tika: {e}")
        raise ConnectionError(f"Erreur Tika: {e}") from e


def get_full_text_for_source(source_info: Dict[str, Any], app_config: Optional[Dict[str, Any]] = None) -> Optional[str]:
    """
    R√©cup√®re le texte complet pour une source donn√©e, en utilisant le cache et les configurations appropri√©es.

    Centralise la logique de r√©cup√©ration de texte en utilisant `fetch_direct_text`,
    `fetch_with_jina`, ou `fetch_with_tika` en fonction de `source_info`.
    G√®re √©galement la lecture de fichiers locaux si `fetch_method` est "file".

    :param source_info: Dictionnaire contenant les informations de la source.
                        Doit contenir des cl√©s comme "schema", "host_parts", "path",
                        "source_type", et optionnellement "fetch_method" ou "url".
    :type source_info: Dict[str, Any]
    :param app_config: Dictionnaire optionnel de configuration de l'application,
                       utilis√© pour surcharger les configurations globales de Jina, Tika, etc.
    :type app_config: Optional[Dict[str, Any]]
    :return: Le texte complet de la source (str), ou None en cas d'erreur de r√©cup√©ration
             ou si l'URL/chemin est invalide.
    :rtype: Optional[str]
    """
    source_name_for_log = source_info.get('source_name', source_info.get('id', 'Source inconnue')) # Utiliser id si source_name manque
    utils_logger.debug(f"get_full_text_for_source appel√©e pour: {source_name_for_log}")

    fetch_method = source_info.get("fetch_method", source_info.get("source_type"))
 
    if fetch_method == "file":
        file_path_str = source_info.get("path")
        if not file_path_str:
            utils_logger.error(f"Champ 'path' manquant pour la source locale: {source_name_for_log}")
            return None
        
        # On suppose que file_path_str est relatif au CWD (qui devrait √™tre la racine du projet
        # lorsque embed_all_sources.py est ex√©cut√©) ou un chemin absolu.
        absolute_file_path = Path(file_path_str).resolve() # .resolve() pour obtenir le chemin absolu et normalis√©

        utils_logger.info(f"Tentative de lecture du fichier local: {absolute_file_path}")
        if absolute_file_path.exists() and absolute_file_path.is_file():
            try:
                text_content = absolute_file_path.read_text(encoding='utf-8')
                utils_logger.info(f"Contenu du fichier local '{absolute_file_path.name}' lu avec succ√®s (longueur: {len(text_content)}).")
                return text_content
            except Exception as e:
                utils_logger.error(f"Erreur lors de la lecture du fichier local '{absolute_file_path}': {e}")
                return None
        else:
            utils_logger.error(f"Fichier local non trouv√© ou n'est pas un fichier: {absolute_file_path}")
            return None

    # Tenter d'obtenir l'URL directement du champ "url", sinon la reconstruire
    target_url = source_info.get("url")
    if not target_url:
        utils_logger.debug(f"Champ 'url' non trouv√© pour {source_name_for_log}, tentative de reconstruction avec schema/host/path.")
        target_url = reconstruct_url(
            source_info.get("schema"), source_info.get("host_parts", []), source_info.get("path")
        )
    
    if not target_url:
        # Si target_url est toujours None ou vide apr√®s les deux tentatives
        utils_logger.error(f"URL non disponible ou invalide pour source: {source_name_for_log} (fetch_method: {fetch_method}) apr√®s v√©rification 'url' et reconstruction.")
        return None
    
    utils_logger.debug(f"URL cible d√©termin√©e pour {source_name_for_log}: {target_url}")

    # Essayer de charger depuis le cache fichier d'abord en utilisant target_url comme cl√©
    cached_text = load_from_cache(target_url)
    if cached_text is not None:
        utils_logger.info(f"Texte charg√© depuis cache fichier pour URL '{target_url}' ({source_name_for_log})")
        if source_name_for_log == "Source_1": # Ajout du logging sp√©cifique pour Source_1
            utils_logger.info(f"--- LOGGING SP√âCIFIQUE POUR Source_1 (depuis cache) ---")
            utils_logger.info(f"Full text length: {len(cached_text)}")
            utils_logger.info(f"Premiers 500 chars:\n{cached_text[:500]}")
            utils_logger.info(f"Derniers 500 chars:\n{cached_text[-500:]}")
            utils_logger.info(f"--- FIN LOGGING SP√âCIFIQUE POUR Source_1 ---")
        return cached_text

    # Utiliser fetch_method si disponible, sinon fallback sur source_type
    # fetch_method a d√©j√† √©t√© r√©cup√©r√© au d√©but
    # source_type reste utile pour des logiques de fallback ou si fetch_method n'est pas exhaustif
    source_type_original = source_info.get("source_type")
    
    texte_brut_source: Optional[str] = None

    # R√©cup√©rer les configurations, en privil√©giant app_config si fourni
    jina_prefix_val = ui_config.JINA_READER_PREFIX
    tika_server_url_val = ui_config.TIKA_SERVER_URL
    plaintext_extensions_val = ui_config.PLAINTEXT_EXTENSIONS
    temp_download_dir_val = ui_config.TEMP_DOWNLOAD_DIR

    if app_config:
        jina_prefix_val = app_config.get('JINA_READER_PREFIX', jina_prefix_val)
        tika_server_url_val = app_config.get('TIKA_SERVER_URL', tika_server_url_val)
        plaintext_extensions_val = app_config.get('PLAINTEXT_EXTENSIONS', plaintext_extensions_val)
        # Pour TEMP_DOWNLOAD_DIR, s'assurer que c'est un objet Path si surcharg√©
        temp_download_dir_str_or_path = app_config.get('TEMP_DOWNLOAD_DIR')
        if temp_download_dir_str_or_path is not None:
            temp_download_dir_val = Path(temp_download_dir_str_or_path)


    utils_logger.info(f"Cache texte absent pour '{target_url}' ({source_name_for_log}). R√©cup√©ration (m√©thode: {fetch_method}, type original: {source_type_original})...")
    try:
        if fetch_method == "jina":
            texte_brut_source = fetch_with_jina(
                target_url,
                jina_reader_prefix_override=jina_prefix_val
            )
        elif fetch_method == "tika":
            texte_brut_source = fetch_with_tika(
                source_url=target_url,
                tika_server_url_override=tika_server_url_val,
                plaintext_extensions_override=plaintext_extensions_val,
                temp_download_dir_override=temp_download_dir_val
            )
        elif fetch_method == "direct_download":
            texte_brut_source = fetch_direct_text(target_url)
        # Fallbacks bas√©s sur source_type_original si fetch_method n'est pas l'un des ci-dessus ou est manquant
        elif source_type_original == "web": # Ancien type "web" pourrait √™tre trait√© comme jina ou direct
             utils_logger.info(f"Fallback pour source_type 'web' vers fetch_with_jina pour {target_url}")
             texte_brut_source = fetch_with_jina(target_url, jina_reader_prefix_override=jina_prefix_val)
        elif source_type_original == "pdf": # Ancien type "pdf" devrait √™tre trait√© par tika
             utils_logger.info(f"Fallback pour source_type 'pdf' vers fetch_with_tika pour {target_url}")
             texte_brut_source = fetch_with_tika(source_url=target_url, tika_server_url_override=tika_server_url_val, plaintext_extensions_override=plaintext_extensions_val, temp_download_dir_override=temp_download_dir_val)
        else:
            utils_logger.warning(f"M√©thode de fetch/type de source inconnu ou non g√©r√©: '{fetch_method}' / '{source_type_original}' pour '{target_url}' ({source_name_for_log}). Impossible de r√©cup√©rer le texte.")
            return None

        if texte_brut_source is not None:
            utils_logger.info(f"Texte r√©cup√©r√© pour '{target_url}' ({source_name_for_log}), sauvegarde dans le cache...")
            save_to_cache(target_url, texte_brut_source)
        else:
            utils_logger.warning(f"Aucun texte brut retourn√© par la fonction fetch pour '{target_url}' ({source_name_for_log}).")

        if source_name_for_log == "Source_1" and texte_brut_source is not None: # Ajout du logging sp√©cifique pour Source_1
            utils_logger.info(f"--- LOGGING SP√âCIFIQUE POUR Source_1 (apr√®s fetch) ---")
            utils_logger.info(f"Full text length: {len(texte_brut_source)}")
            utils_logger.info(f"Premiers 500 chars:\n{texte_brut_source[:500]}")
            utils_logger.info(f"Derniers 500 chars:\n{texte_brut_source[-500:]}")
            utils_logger.info(f"--- FIN LOGGING SP√âCIFIQUE POUR Source_1 ---")
        
        return texte_brut_source

    except ConnectionError as e: # Erreurs sp√©cifiques lev√©es par les fetch_*
        utils_logger.error(f"Erreur de connexion lors de la r√©cup√©ration de '{target_url}' ({source_name_for_log}, m√©thode: {fetch_method}): {e}")
        return None
    except Exception as e:
        utils_logger.error(f"Erreur inattendue lors de la r√©cup√©ration de '{target_url}' ({source_name_for_log}, m√©thode: {fetch_method}): {e}", exc_info=True)
        return None

def verify_extract_definitions(definitions_list: List[Dict[str, Any]]) -> str:
    """V√©rifie la pr√©sence des marqueurs de d√©but et de fin pour chaque extrait d√©fini.

    Pour chaque source dans `definitions_list`, r√©cup√®re le texte complet (en utilisant
    le cache ou les fonctions fetch), puis v√©rifie si les `start_marker` et `end_marker`
    de chaque extrait sont pr√©sents dans le texte source.

    :param definitions_list: Une liste de dictionnaires, chaque dictionnaire
                             repr√©sentant une d√©finition de source avec ses extraits.
    :type definitions_list: List[Dict[str, Any]]
    :return: Une cha√Æne HTML r√©sumant les r√©sultats de la v√©rification, indiquant
             le nombre d'erreurs et listant les probl√®mes sp√©cifiques.
    :rtype: str
    """
    # Utilise les fonctions reconstruct_url, load_from_cache, fetch_* de ce module
    # Et les constantes de ui.config
    utils_logger.info("\nüî¨ Lancement de la v√©rification des marqueurs d'extraits...")
    results = []
    total_checks = 0
    total_errors = 0

    # Utilise la constante depuis config
    if not definitions_list or definitions_list == ui_config.DEFAULT_EXTRACT_SOURCES:
        return "Aucune d√©finition valide √† v√©rifier."

    for source_idx, source_info in enumerate(definitions_list):
        source_name = source_info.get("source_name", f"Source Inconnue #{source_idx+1}")
        utils_logger.info(f"\n--- V√©rification Source: '{source_name}' ---")
        source_errors = 0
        source_checks = 0
        texte_brut_source = None
        reconstructed_url = None

        try:
            reconstructed_url = reconstruct_url(
                source_info.get("schema"), source_info.get("host_parts", []), source_info.get("path")
            )
            if not reconstructed_url:
                utils_logger.error("   -> ‚ùå Erreur: URL Invalide.")
                results.append(f"<li>{source_name}: URL invalide</li>")
                num_extracts = len(source_info.get("extracts", []))
                total_errors += num_extracts
                total_checks += num_extracts
                continue

            source_type = source_info.get("source_type")
            cache_key = reconstructed_url
            texte_brut_source = load_from_cache(cache_key)

            if texte_brut_source is None:
                utils_logger.info(f"   -> Cache texte absent. R√©cup√©ration (type: {source_type})...")
                try:
                    if source_type == "jina": texte_brut_source = fetch_with_jina(reconstructed_url)
                    elif source_type == "direct_download": texte_brut_source = fetch_direct_text(reconstructed_url)
                    elif source_type == "tika":
                        is_plaintext = any(source_info.get("path", "").lower().endswith(ext) for ext in ui_config.PLAINTEXT_EXTENSIONS)
                        if is_plaintext: texte_brut_source = fetch_direct_text(reconstructed_url)
                        else:
                            utils_logger.warning("   -> ‚ö†Ô∏è V√©rification marqueurs saut√©e pour source Tika binaire.")
                            texte_brut_source = None
                    else:
                        utils_logger.warning(f"   -> ‚ö†Ô∏è Type source inconnu '{source_type}'. V√©rification impossible.")
                        texte_brut_source = None
                except Exception as e_fetch_verify:
                     utils_logger.error(f"   -> ‚ùå Erreur fetch pendant v√©rification pour '{source_name}': {e_fetch_verify}")
                     texte_brut_source = None

            # ... [Reste de la logique de v√©rification des marqueurs - IDENTIQUE A AVANT] ...
            if texte_brut_source is not None:
                utils_logger.info(f"   -> Texte complet r√©cup√©r√© (longueur: {len(texte_brut_source)}). V√©rification des extraits...")
                extracts = source_info.get("extracts", [])
                if not extracts: utils_logger.info("      -> Aucun extrait d√©fini.")

                for extract_idx, extract_info in enumerate(extracts):
                    extract_name = extract_info.get("extract_name", f"Extrait #{extract_idx+1}")
                    start_marker = extract_info.get("start_marker")
                    end_marker = extract_info.get("end_marker")

                    # --- D√âBUT LOGS D√âTAILL√âS (SIMULATION EXTRACT_SEGMENT) ---
                    # Uniquement pour l'extrait cibl√© pour ne pas polluer les logs inutilement
                    # Note: Cette condition est sp√©cifique √† la t√¢che de diagnostic actuelle.
                    # Pour un logging g√©n√©ral, il faudrait l'enlever ou la rendre configurable.
                    is_target_extract = source_name == "Source_1" and extract_name == "1. DAcbat Complet (Ottawa, 1858)"

                    if is_target_extract:
                        utils_logger.info(f"[LOGGING DIAGNOSTIC POUR {source_name} -> {extract_name}]")
                        utils_logger.info(f"  extract_name: {extract_name}")
                        utils_logger.info(f"  start_marker (re√ßu): '{start_marker}'")
                        utils_logger.info(f"  end_marker (re√ßu): '{end_marker}'")

                    current_start_index = -1 # Renomm√© pour √©viter conflit avec une variable potentielle 'start_index'
                    current_end_index = -1   # Renomm√© pour √©viter conflit

                    if texte_brut_source and start_marker:
                        actual_start_marker_log = start_marker
                        
                        if is_target_extract:
                            utils_logger.info(f"  AVANT RECHERCHE start_marker:")
                            utils_logger.info(f"    actual_start_marker: '{actual_start_marker_log}'")
                            # Tentative de trouver la position attendue pour un meilleur contexte
                            # Utiliser une sous-cha√Æne plus petite pour l'aper√ßu si le texte est immense
                            approx_start_pos = texte_brut_source.find(actual_start_marker_log)
                            if approx_start_pos != -1:
                                context_window = 200 # Augment√© pour plus de contexte
                                start_slice = max(0, approx_start_pos - context_window)
                                end_slice = approx_start_pos + len(actual_start_marker_log) + context_window
                                context_text_start = texte_brut_source[start_slice:end_slice]
                                utils_logger.info(f"    contexte source_text (autour de pos {approx_start_pos}, fenetre +/-{context_window}):\n'''{context_text_start}'''")
                            else:
                                utils_logger.info(f"    start_marker non trouv√© (estimation), contexte source_text (d√©but):\n'''{texte_brut_source[:500]}'''")
                        
                        try:
                            found_pos = texte_brut_source.index(actual_start_marker_log)
                            current_start_index = found_pos # Position de d√©but du marqueur
                            if is_target_extract:
                                utils_logger.info(f"  start_index TROUV√â: {current_start_index}")
                        except ValueError:
                            if is_target_extract:
                                utils_logger.info(f"  start_index NON TROUV√â pour '{actual_start_marker_log}'")
                            current_start_index = -1
                    
                    if texte_brut_source and end_marker and current_start_index != -1:
                        actual_end_marker_log = end_marker
                        # Zone de recherche pour end_marker commence apr√®s le start_marker complet
                        search_area_start_for_end_marker = current_start_index + len(start_marker)

                        if is_target_extract:
                            utils_logger.info(f"  AVANT RECHERCHE end_marker (recherche √† partir de position {search_area_start_for_end_marker}):")
                            utils_logger.info(f"    actual_end_marker: '{actual_end_marker_log}'")
                            
                            approx_end_pos_in_search_area = texte_brut_source[search_area_start_for_end_marker:].find(actual_end_marker_log)
                            if approx_end_pos_in_search_area != -1:
                                approx_end_pos_global = search_area_start_for_end_marker + approx_end_pos_in_search_area
                                context_window = 200 # Augment√©
                                start_slice = max(0, approx_end_pos_global - context_window)
                                end_slice = approx_end_pos_global + len(actual_end_marker_log) + context_window
                                context_text_end = texte_brut_source[start_slice:end_slice]
                                utils_logger.info(f"    contexte source_text (autour de pos globale {approx_end_pos_global}, fenetre +/-{context_window}):\n'''{context_text_end}'''")
                            else:
                                utils_logger.info(f"    end_marker non trouv√© (estimation) dans la zone, contexte source_text (zone de recherche concern√©e):\n'''{texte_brut_source[search_area_start_for_end_marker : search_area_start_for_end_marker + 500]}'''")

                        try:
                            # .find() est utilis√© ici car la logique originale est `end_marker in texte_brut_source[...]`
                            found_pos_end = texte_brut_source.find(actual_end_marker_log, search_area_start_for_end_marker)
                            if found_pos_end != -1:
                                # Conventionnellement, end_index est la position *apr√®s* le marqueur de fin.
                                current_end_index = found_pos_end + len(actual_end_marker_log)
                                if is_target_extract:
                                    utils_logger.info(f"  end_index TROUV√â (marqueur trouv√© √† {found_pos_end}, fin du segment √† {current_end_index})")
                            else:
                                if is_target_extract:
                                    utils_logger.info(f"  end_index NON TROUV√â pour '{actual_end_marker_log}' apr√®s start_marker.")
                                current_end_index = -1
                        except Exception as e_find_end: # Bien que .find() ne l√®ve pas ValueError, s√©curit√©
                            if is_target_extract:
                                utils_logger.error(f"  Erreur inattendue recherche end_marker: {e_find_end}")
                            current_end_index = -1
                    
                    if is_target_extract: # Log final des index trouv√©s pour l'extrait cible
                        utils_logger.info(f"  Valeurs finales pour {extract_name}: current_start_index = {current_start_index}, current_end_index = {current_end_index}")
                    # --- FIN LOGS D√âTAILL√âS ---

                    total_checks += 1
                    source_checks += 1
                    marker_errors = []

                    if not start_marker or not end_marker:
                        marker_errors.append("Marqueur(s) Manquant(s)")
                    else:
                        start_found = start_marker in texte_brut_source
                        end_found_after_start = False
                        if start_found:
                            try:
                                start_pos = texte_brut_source.index(start_marker)
                                end_found_after_start = end_marker in texte_brut_source[start_pos + len(start_marker):]
                            except ValueError: start_found = False
                        if not start_found: marker_errors.append("D√©but NON TROUV√â")
                        if not end_found_after_start: marker_errors.append("Fin NON TROUV√âE (apr√®s d√©but)")

                    if marker_errors:
                        utils_logger.warning(f"      -> ‚ùå Probl√®me Extrait '{extract_name}': {', '.join(marker_errors)}")
                        results.append(f"<li>{source_name} -> {extract_name}: <strong style='color:red;'>{', '.join(marker_errors)}</strong></li>")
                        source_errors += 1
                        total_errors += 1
                    else:
                        utils_logger.info(f"      -> ‚úÖ OK: Extrait '{extract_name}'")

            else:
                num_extracts = len(source_info.get("extracts",[]))
                if source_type != 'tika' or is_plaintext:
                    results.append(f"<li>{source_name}: V√©rification impossible (texte source non obtenu)</li>")
                    total_errors += num_extracts
                else:
                     results.append(f"<li>{source_name}: V√©rification marqueurs saut√©e (source Tika binaire)</li>")
                total_checks += num_extracts

        except Exception as e_verify_global:
            utils_logger.error(f"   -> ‚ùå Erreur inattendue v√©rification source '{source_name}': {e_verify_global}", exc_info=True)
            num_extracts = len(source_info.get("extracts",[]))
            results.append(f"<li>{source_name}: Erreur V√©rification G√©n√©rale ({type(e_verify_global).__name__})</li>")
            total_errors += num_extracts
            total_checks += num_extracts

    # ... [Reste de la logique de formatage du r√©sum√© - IDENTIQUE A AVANT] ...
    summary = f"--- R√©sultat V√©rification ---<br/>{total_checks} extraits v√©rifi√©s. <strong style='color: {'red' if total_errors > 0 else 'green'};'>{total_errors} erreur(s) trouv√©e(s).</strong>"
    if results:
        summary += "<br/>D√©tails :<ul>" + "".join(results) + "</ul>"
    else:
         if total_checks > 0: summary += "<br/>Tous les marqueurs semblent corrects."
         else: summary += "<br/>Aucun extrait n'a pu √™tre v√©rifi√©."

    utils_logger.info("\n" + f"{summary.replace('<br/>', chr(10)).replace('<li>', '- ').replace('</li>', '').replace('<ul>', '').replace('</ul>', '').replace('<strong>', '').replace('</strong>', '')}")
    return summary


utils_logger.info("Fonctions utilitaires UI d√©finies.")