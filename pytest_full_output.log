============================= test session starts =============================
platform win32 -- Python 3.13.2, pytest-8.3.5, pluggy-1.6.0
rootdir: c:\dev\2025-Epita-Intelligence-Symbolique
configfile: pytest.ini
testpaths: tests, services, argumentation_analysis/agents, argumentation_analysis/core, argumentation_analysis/models, argumentation_analysis/orchestration, #argumentation_analysis/tests, argumentation_analysis/ui, argumentation_analysis/utils
plugins: anyio-4.9.0, mock-3.14.1, asyncio-1.0.0, cov-6.1.1
asyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function
collected 1005 items

tests\agents\core\informal\test_informal_agent.py .............          [  1%]
tests\agents\core\informal\test_informal_agent_creation.py ...           [  1%]
tests\agents\core\informal\test_informal_analysis_methods.py ....        [  1%]
tests\agents\core\informal\test_informal_definitions.py .sss.            [  2%]
tests\agents\core\informal\test_informal_error_handling.py ....          [  2%]
tests\agents\core\logic\test_abstract_logic_agent.py ........            [  3%]
tests\agents\core\logic\test_belief_set.py ..........                    [  4%]
tests\agents\core\logic\test_examples.py .....                           [  5%]
tests\agents\core\logic\test_first_order_logic_agent.py ............     [  6%]
tests\agents\core\logic\test_logic_factory.py ........                   [  7%]
tests\agents\core\logic\test_modal_logic_agent.py ............           [  8%]
tests\agents\core\logic\test_propositional_logic_agent.py .FFF..F....F.  [  9%]
tests\agents\core\logic\test_query_executor.py .........                 [ 10%]
tests\agents\core\logic\test_tweety_bridge.py ...s....s..s               [ 11%]
tests\agents\core\logic\test_watson_logic_assistant.py ....              [ 12%]
tests\agents\core\pm\test_sherlock_enquete_agent.py .....                [ 12%]
tests\agents\tools\analysis\enhanced\test_enhanced_complex_fallacy_analyzer.py . [ 12%]
.........                                                                [ 13%]
tests\agents\tools\analysis\enhanced\test_enhanced_contextual_fallacy_analyzer.py . [ 13%]
.........                                                                [ 14%]
tests\agents\tools\analysis\enhanced\test_enhanced_fallacy_severity_evaluator.py . [ 14%]
............                                                             [ 15%]
tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py .FF..F [ 16%]
..                                                                       [ 16%]
tests\core\test_enquete_states.py .......................                [ 19%]
tests\environment_checks\test_core_dependencies.py ........FF.........   [ 20%]
tests\environment_checks\test_project_module_imports.py ..........       [ 21%]
tests\minimal_jvm_pytest_test.py .                                       [ 21%]
tests\mocks\test_jpype_mock.py ..                                        [ 22%]
tests\mocks\test_numpy_mock.py .                                         [ 22%]
tests\orchestration\hierarchical\operational\adapters\test_extract_agent_adapter.py . [ 22%]
...................                                                      [ 24%]
tests\orchestration\hierarchical\tactical\test_coordinator.py .......... [ 25%]
...........                                                              [ 26%]
tests\orchestration\plugins\test_enquete_state_manager_plugin.py ....... [ 27%]
.                                                                        [ 27%]
tests\orchestration\tactical\test_tactical_coordinator.py .......        [ 27%]
tests\orchestration\tactical\test_tactical_coordinator_advanced.py ..... [ 28%]
..                                                                       [ 28%]
tests\orchestration\tactical\test_tactical_coordinator_coverage.py ..... [ 29%]
                                                                         [ 29%]
tests\orchestration\tactical\test_tactical_monitor.py .........          [ 29%]
tests\orchestration\tactical\test_tactical_monitor_advanced.py .......   [ 30%]
tests\project_core\dev_utils\test_verification_utils.py .......          [ 31%]
tests\project_core\service_setup\test_core_services.py .                 [ 31%]
tests\test_extract_agent_adapter.py sssssssss                            [ 32%]
tests\test_numpy_rec_mock.py .....                                       [ 32%]
tests\test_tactical_resolver.py ................                         [ 34%]
tests\test_tactical_resolver_advanced.py .......                         [ 35%]
tests\test_tactical_state.py ......................                      [ 37%]
tests\test_validation_errors.py .....                                    [ 37%]
tests\ui\test_extract_definition_persistence.py ....Fs..F                [ 38%]
tests\ui\test_utils.py .....................................             [ 42%]
tests\unit\argumentation_analysis\analytics\test_stats_calculator.py ... [ 42%]
....                                                                     [ 43%]
tests\unit\argumentation_analysis\analytics\test_text_analyzer.py ....   [ 43%]
tests\unit\argumentation_analysis\mocks\test_advanced_tools.py ......    [ 44%]
tests\unit\argumentation_analysis\mocks\test_argument_mining.py ........ [ 44%]
F.FF.F.FF                                                                [ 45%]
tests\unit\argumentation_analysis\mocks\test_bias_detection.py .....F... [ 46%]
......                                                                   [ 47%]
tests\unit\argumentation_analysis\mocks\test_claim_mining.py .....FF..F. [ 48%]
...                                                                      [ 48%]
tests\unit\argumentation_analysis\mocks\test_clarity_scoring.py ....F... [ 49%]
F..F                                                                     [ 49%]
tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py ...FF [ 50%]
FFFF.                                                                    [ 50%]
tests\unit\argumentation_analysis\mocks\test_emotional_tone_analysis.py . [ 50%]
....FF.F..                                                               [ 51%]
tests\unit\argumentation_analysis\mocks\test_engagement_analysis.py .... [ 52%]
..FF...F.                                                                [ 53%]
tests\unit\argumentation_analysis\mocks\test_evidence_detection.py ..... [ 53%]
..F.....FF                                                               [ 54%]
tests\unit\argumentation_analysis\mocks\test_fallacy_categorization.py . [ 54%]
..........                                                               [ 55%]
tests\unit\argumentation_analysis\mocks\test_fallacy_detection.py ...... [ 56%]
.                                                                        [ 56%]
tests\unit\argumentation_analysis\mocks\test_rhetorical_analysis.py .... [ 56%]
..F.F..                                                                  [ 57%]
tests\unit\argumentation_analysis\nlp\test_embedding_utils.py .........  [ 58%]
tests\unit\argumentation_analysis\orchestration\test_advanced_analyzer.py . [ 58%]
.....                                                                    [ 59%]
tests\unit\argumentation_analysis\pipelines\test_advanced_rhetoric.py .. [ 59%]
...                                                                      [ 59%]
tests\unit\argumentation_analysis\pipelines\test_analysis_pipeline.py .. [ 59%]
...                                                                      [ 60%]
tests\unit\argumentation_analysis\reporting\test_summary_generator.py .. [ 60%]
                                                                         [ 60%]
tests\unit\argumentation_analysis\service_setup\test_analysis_services.py . [ 60%]
......                                                                   [ 60%]
tests\unit\argumentation_analysis\utils\core_utils\test_cli_utils.py ... [ 61%]
.....................                                                    [ 63%]
tests\unit\argumentation_analysis\utils\core_utils\test_crypto_utils.py . [ 63%]
............                                                             [ 64%]
tests\unit\argumentation_analysis\utils\core_utils\test_file_utils.py .. [ 64%]
..........................................................               [ 70%]
tests\unit\argumentation_analysis\utils\core_utils\test_logging_utils.py . [ 70%]
.........                                                                [ 71%]
tests\unit\argumentation_analysis\utils\core_utils\test_network_utils.py . [ 71%]
.......                                                                  [ 72%]
tests\unit\argumentation_analysis\utils\core_utils\test_reporting_utils.py . [ 72%]
................                                                         [ 74%]
tests\unit\argumentation_analysis\utils\core_utils\test_system_utils.py . [ 74%]
.......                                                                  [ 74%]
tests\unit\argumentation_analysis\utils\core_utils\test_text_utils.py .. [ 75%]
..................                                                       [ 76%]
tests\unit\argumentation_analysis\utils\dev_tools\test_code_formatting_utils.py . [ 77%]
.....                                                                    [ 77%]
tests\unit\argumentation_analysis\utils\dev_tools\test_code_validation.py . [ 77%]
....s..                                                                  [ 78%]
tests\unit\argumentation_analysis\utils\dev_tools\test_encoding_utils.py . [ 78%]
.......                                                                  [ 79%]
tests\unit\argumentation_analysis\utils\dev_tools\test_format_utils.py . [ 79%]
.....                                                                    [ 79%]
tests\unit\argumentation_analysis\utils\dev_tools\test_mock_utils.py ... [ 80%]
....                                                                     [ 80%]
tests\unit\argumentation_analysis\utils\test_analysis_comparison.py .... [ 80%]
....                                                                     [ 81%]
tests\unit\argumentation_analysis\utils\test_data_generation.py ........ [ 81%]
..                                                                       [ 82%]
tests\unit\argumentation_analysis\utils\test_data_loader.py .....        [ 82%]
tests\unit\argumentation_analysis\utils\test_data_processing_utils.py .. [ 82%]
.....                                                                    [ 83%]
tests\unit\argumentation_analysis\utils\test_error_estimation.py ....    [ 83%]
tests\unit\argumentation_analysis\utils\test_metrics_aggregation.py ...  [ 84%]
tests\unit\argumentation_analysis\utils\test_metrics_extraction.py ..... [ 84%]
..................                                                       [ 86%]
tests\unit\argumentation_analysis\utils\test_report_generator.py ...     [ 86%]
tests\unit\argumentation_analysis\utils\test_text_processing.py ........ [ 87%]
....                                                                     [ 87%]
tests\unit\argumentation_analysis\utils\test_visualization_generator.py . [ 87%]
sF                                                                       [ 88%]
tests\utils\test_crypto_utils.py ..................                      [ 89%]
tests\utils\test_fetch_service_errors.py ............                    [ 91%]
argumentation_analysis\core\communication\tests\test_channel_interface.py . [ 91%]
.........                                                                [ 92%]
argumentation_analysis\core\communication\tests\test_communication_integration.py . [ 92%]
...                                                                      [ 92%]
argumentation_analysis\core\communication\tests\test_message.py ........ [ 93%]
.......                                                                  [ 94%]
argumentation_analysis\core\communication\tests\test_middleware.py ..... [ 94%]
..........                                                               [ 95%]
argumentation_analysis\core\tests\test_shared_state.py ................. [ 97%]
..                                                                       [ 97%]
argumentation_analysis\core\tests\test_state_manager_plugin.py ......... [ 98%]
...........                                                              [ 99%]
argumentation_analysis\core\utils\tests\test_error_recovery_manager.py . [ 99%]
....                                                                     [ 99%]
argumentation_analysis\utils\test_taxonomy_loader.py .                   [100%]

================================== FAILURES ===================================
_________ TestPropositionalLogicAgent.test_execute_query_error_tweety _________

self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_execute_query_error_tweety>

    def test_execute_query_error_tweety(self):
        """Test de l'exécution d'une requête avec erreur de Tweety."""
        belief_set_obj = PropositionalBeliefSet("a => b")
        if not os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1'):
            # Assurer que le mock retourne la chaîne attendue par l'agent pour ce test
            self.mock_tweety_bridge_instance.execute_pl_query.return_value = (False, "FUNC_ERROR: Erreur de syntaxe Tweety")
            self.mock_tweety_bridge_instance.validate_formula.return_value = (True, "Formule valide")
    
        result, message = self.agent.execute_query(belief_set_obj, "a")
    
        self.mock_tweety_bridge_instance.validate_formula.assert_called_once_with(formula_string="a")
        self.mock_tweety_bridge_instance.execute_pl_query.assert_called_once_with(
            belief_set_content="a => b",
            query_string="a"
        )
    
        # Avec la vraie JVM, 'a' est rejeté par 'a => b'.
>       self.assertFalse(result) # 'a' n'est pas une conséquence de 'a => b', donc result est False.
E       AssertionError: True is not false

tests\agents\core\logic\test_propositional_logic_agent.py:234: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:09:33,471 - tests.mocks.jpype_setup - INFO - Test test_execute_query_error_tweety: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:09:33,471 - tests.mocks.jpype_setup - INFO - Test test_execute_query_error_tweety demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
---------------------------- Captured stdout call -----------------------------
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] TweetyBridge initialisé directement dans PropositionalLogicAgent.__init__ pour TestPLAgent. JVM prête: True
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Configuration des composants sémantiques pour TestPLAgent...
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.TextToPLBeliefSet ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.GeneratePLQueries ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.InterpretPLResults ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Composants pour TestPLAgent configurés.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Exécution de la requête PL: 'a' sur le BeliefSet.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Résultat de l'exécution pour 'a': True, Output brut: 'Tweety Result: Query 'a => b' is ACCEPTED (True).'
------------------------------ Captured log call ------------------------------
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:69 TweetyBridge initialisé directement dans PropositionalLogicAgent.__init__ pour TestPLAgent. JVM prête: True
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:100 Configuration des composants sémantiques pour TestPLAgent...
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.TextToPLBeliefSet ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.GeneratePLQueries ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.InterpretPLResults ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:151 Composants pour TestPLAgent configurés.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:260 Exécution de la requête PL: 'a' sur le BeliefSet.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:288 Résultat de l'exécution pour 'a': True, Output brut: 'Tweety Result: Query 'a => b' is ACCEPTED (True).'
_______ TestPropositionalLogicAgent.test_execute_query_invalid_formula ________

self = <MagicMock name='TweetyBridge().execute_pl_query' id='1746725188480'>

    def assert_not_called(self):
        """assert that the mock was never called.
        """
        if self.call_count != 0:
            msg = ("Expected '%s' to not have been called. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'execute_pl_query' to not have been called. Called 1 times.
E           Calls: [call(belief_set_content='a => b', query_string='invalid_query {')].

C:\Python313\Lib\unittest\mock.py:938: AssertionError

During handling of the above exception, another exception occurred:

self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_execute_query_invalid_formula>

    def test_execute_query_invalid_formula(self):
        """Test de l'exécution d'une requête avec une formule invalide."""
        belief_set_obj = PropositionalBeliefSet("a => b")
        if not os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1'):
            self.mock_tweety_bridge_instance.validate_formula.return_value = (False, "Syntax Error in query")
    
        result, message = self.agent.execute_query(belief_set_obj, "invalid_query {")
    
        self.mock_tweety_bridge_instance.validate_formula.assert_called_once_with(formula_string="invalid_query {")
>       self.mock_tweety_bridge_instance.execute_pl_query.assert_not_called()
E       AssertionError: Expected 'execute_pl_query' to not have been called. Called 1 times.
E       Calls: [call(belief_set_content='a => b', query_string='invalid_query {')].
E       
E       pytest introspection follows:
E       
E       Kwargs:
E       assert {'belief_set_...alid_query {'} == {}
E         
E         Left contains 2 more items:
E         {'belief_set_content': 'a => b', 'query_string': 'invalid_query {'}
E         Use -v to get more diff

tests\agents\core\logic\test_propositional_logic_agent.py:246: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:09:33,489 - tests.mocks.jpype_setup - INFO - Test test_execute_query_invalid_formula: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:09:33,489 - tests.mocks.jpype_setup - INFO - Test test_execute_query_invalid_formula demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
---------------------------- Captured stdout call -----------------------------
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] TweetyBridge initialisé directement dans PropositionalLogicAgent.__init__ pour TestPLAgent. JVM prête: True
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Configuration des composants sémantiques pour TestPLAgent...
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.TextToPLBeliefSet ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.GeneratePLQueries ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.InterpretPLResults ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Composants pour TestPLAgent configurés.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Exécution de la requête PL: 'invalid_query {' sur le BeliefSet.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Résultat de l'exécution pour 'invalid_query {': True, Output brut: 'Tweety Result: Query 'a => b' is ACCEPTED (True).'
------------------------------ Captured log call ------------------------------
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:69 TweetyBridge initialisé directement dans PropositionalLogicAgent.__init__ pour TestPLAgent. JVM prête: True
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:100 Configuration des composants sémantiques pour TestPLAgent...
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.TextToPLBeliefSet ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.GeneratePLQueries ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.InterpretPLResults ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:151 Composants pour TestPLAgent configurés.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:260 Exécution de la requête PL: 'invalid_query {' sur le BeliefSet.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:288 Résultat de l'exécution pour 'invalid_query {': True, Output brut: 'Tweety Result: Query 'a => b' is ACCEPTED (True).'
___________ TestPropositionalLogicAgent.test_execute_query_rejected ___________

self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_execute_query_rejected>

    def test_execute_query_rejected(self):
        """Test de l'exécution d'une requête rejetée."""
        belief_set_obj = PropositionalBeliefSet("a => b")
        if not os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1'):
            # Assurer que le mock retourne la chaîne attendue par l'agent pour ce test
            self.mock_tweety_bridge_instance.execute_pl_query.return_value = (False, "Tweety Result: Query 'c' is REJECTED (False).")
            self.mock_tweety_bridge_instance.validate_formula.return_value = (True, "Formule valide")
    
        result, message = self.agent.execute_query(belief_set_obj, "c")
    
        self.mock_tweety_bridge_instance.validate_formula.assert_called_once_with(formula_string="c")
        self.mock_tweety_bridge_instance.execute_pl_query.assert_called_once_with(
            belief_set_content="a => b",
            query_string="c"
        )
>       self.assertFalse(result)
E       AssertionError: True is not false

tests\agents\core\logic\test_propositional_logic_agent.py:214: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:09:33,598 - tests.mocks.jpype_setup - INFO - Test test_execute_query_rejected: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:09:33,598 - tests.mocks.jpype_setup - INFO - Test test_execute_query_rejected demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
---------------------------- Captured stdout call -----------------------------
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] TweetyBridge initialisé directement dans PropositionalLogicAgent.__init__ pour TestPLAgent. JVM prête: True
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Configuration des composants sémantiques pour TestPLAgent...
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.TextToPLBeliefSet ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.GeneratePLQueries ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.InterpretPLResults ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Composants pour TestPLAgent configurés.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Exécution de la requête PL: 'c' sur le BeliefSet.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Résultat de l'exécution pour 'c': True, Output brut: 'Tweety Result: Query 'a => b' is ACCEPTED (True).'
------------------------------ Captured log call ------------------------------
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:69 TweetyBridge initialisé directement dans PropositionalLogicAgent.__init__ pour TestPLAgent. JVM prête: True
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:100 Configuration des composants sémantiques pour TestPLAgent...
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.TextToPLBeliefSet ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.GeneratePLQueries ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.InterpretPLResults ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:151 Composants pour TestPLAgent configurés.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:260 Exécution de la requête PL: 'c' sur le BeliefSet.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:288 Résultat de l'exécution pour 'c': True, Output brut: 'Tweety Result: Query 'a => b' is ACCEPTED (True).'
__________ TestPropositionalLogicAgent.test_initialization_and_setup __________

self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_initialization_and_setup>

    def test_initialization_and_setup(self):
        """Test de l'initialisation et de la configuration de l'agent."""
        self.assertEqual(self.agent.name, self.agent_name)
        self.assertEqual(self.agent.sk_kernel, self.kernel)
        self.assertEqual(self.agent.logic_type, "PL") # Vérifié dans BaseLogicAgent
        self.assertEqual(self.agent.system_prompt, PL_AGENT_INSTRUCTIONS)
    
        # Vérifie que TweetyBridge a été instancié
        self.mock_tweety_bridge_class.assert_called_once_with()
    
        # is_jvm_ready est appelé deux fois dans setup_agent_components de PropositionalLogicAgent
>       self.assertEqual(self.mock_tweety_bridge_instance.is_jvm_ready.call_count, 2)
E       AssertionError: 3 != 2

tests\agents\core\logic\test_propositional_logic_agent.py:67: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:09:33,661 - tests.mocks.jpype_setup - INFO - Test test_initialization_and_setup: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:09:33,661 - tests.mocks.jpype_setup - INFO - Test test_initialization_and_setup demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
---------------------------- Captured stdout call -----------------------------
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] TweetyBridge initialisé directement dans PropositionalLogicAgent.__init__ pour TestPLAgent. JVM prête: True
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Configuration des composants sémantiques pour TestPLAgent...
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.TextToPLBeliefSet ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.GeneratePLQueries ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.InterpretPLResults ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Composants pour TestPLAgent configurés.
------------------------------ Captured log call ------------------------------
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:69 TweetyBridge initialisé directement dans PropositionalLogicAgent.__init__ pour TestPLAgent. JVM prête: True
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:100 Configuration des composants sémantiques pour TestPLAgent...
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.TextToPLBeliefSet ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.GeneratePLQueries ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.InterpretPLResults ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:151 Composants pour TestPLAgent configurés.
__________ TestPropositionalLogicAgent.test_validate_formula_invalid __________

self = <test_propositional_logic_agent.TestPropositionalLogicAgent testMethod=test_validate_formula_invalid>

    def test_validate_formula_invalid(self):
        """Test de la validation d'une formule invalide."""
        if not os.environ.get('USE_REAL_JPYPE', 'false').lower() in ('true', '1'):
            self.mock_tweety_bridge_instance.validate_formula.return_value = (False, "Erreur de syntaxe")
            is_valid, _ = self.agent.tweety_bridge.validate_formula(formula_string="a => (b") # Appel mocké avec argument nommé
            self.mock_tweety_bridge_instance.validate_formula.assert_called_once_with(formula_string="a => (b")
        else:
            # Appel direct à la vraie méthode de l'instance de l'agent
            is_valid, _ = self.agent.tweety_bridge.validate_formula("a => (b")
    
        # Pour une formule invalide, ou si la JVM n'est pas prête, is_valid devrait être False.
>       self.assertFalse(is_valid)
E       AssertionError: True is not false

tests\agents\core\logic\test_propositional_logic_agent.py:299: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:09:33,705 - tests.mocks.jpype_setup - INFO - Test test_validate_formula_invalid: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:09:33,705 - tests.mocks.jpype_setup - INFO - Test test_validate_formula_invalid demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
---------------------------- Captured stdout call -----------------------------
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] TweetyBridge initialisé directement dans PropositionalLogicAgent.__init__ pour TestPLAgent. JVM prête: True
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Configuration des composants sémantiques pour TestPLAgent...
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.TextToPLBeliefSet ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.GeneratePLQueries ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Fonction sémantique TestPLAgent.InterpretPLResults ajoutée.
20:09:33 [INFO] [agent.PropositionalLogicAgent.TestPLAgent] Composants pour TestPLAgent configurés.
------------------------------ Captured log call ------------------------------
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:69 TweetyBridge initialisé directement dans PropositionalLogicAgent.__init__ pour TestPLAgent. JVM prête: True
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:100 Configuration des composants sémantiques pour TestPLAgent...
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.TextToPLBeliefSet ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.GeneratePLQueries ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:147 Fonction sémantique TestPLAgent.InterpretPLResults ajoutée.
INFO     agent.PropositionalLogicAgent.TestPLAgent:propositional_logic_agent.py:151 Composants pour TestPLAgent configurés.
_________ test_run_extract_repair_pipeline_with_save_and_json_export __________

self = <MagicMock name='generate_report' id='1749998816624'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'generate_report' to have been called once. Called 0 times.

C:\Python313\Lib\unittest\mock.py:956: AssertionError

During handling of the above exception, another exception occurred:

mock_generate_marker_repair_report = <MagicMock name='generate_report' id='1749998816624'>
mock_repair_extract_markers = <AsyncMock name='repair_extract_markers' id='1749998816288'>
MockFetchService = <MagicMock name='FetchService' id='1750000144064'>
MockExtractService = <MagicMock name='ExtractService' id='1750000143056'>
MockCacheService = <MagicMock name='CacheService' id='1750000142720'>
MockCryptoService = <MagicMock name='CryptoService' id='1750000142384'>
MockDefinitionService = <MagicMock name='DefinitionService' id='1750000142048'>
mock_create_llm_service = <MagicMock name='create_llm_service' id='1750000145408'>
mock_project_root = WindowsPath('C:/Users/jsboi/AppData/Local/Temp/pytest-of-jsboi/pytest-204/test_run_extract_repair_pipeli1')
mock_llm_service = <MagicMock name='create_llm_service()' id='1749998818640'>
mock_definition_service = <MagicMock name='DefinitionService()' id='1749998817968'>

    @patch("argumentation_analysis.utils.dev_tools.repair_utils.create_llm_service")
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.DefinitionService")
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.CryptoService")
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.CacheService")
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.ExtractService")
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.FetchService")
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.repair_extract_markers", new_callable=AsyncMock)
    @patch("argumentation_analysis.utils.extract_repair.marker_repair_logic.generate_report") # Cible l'emplacement original
    @pytest.mark.asyncio
    async def test_run_extract_repair_pipeline_with_save_and_json_export(
        mock_generate_marker_repair_report: MagicMock, # Renommé pour correspondre au nom importé
        mock_repair_extract_markers: AsyncMock,  # Renommé pour correspondre au nom importé
        MockFetchService: MagicMock,
        MockExtractService: MagicMock,
        MockCacheService: MagicMock,
        MockCryptoService: MagicMock,
        MockDefinitionService: MagicMock,
        mock_create_llm_service: MagicMock, # Renommé pour correspondre au nom importé
        mock_project_root: Path,
        mock_llm_service: MagicMock, # Fixture
        mock_definition_service: MagicMock # Fixture
    ):
        """Teste le pipeline avec sauvegarde et export JSON."""
        mock_create_llm_service.return_value = mock_llm_service
    
        # Configurer les instances mockées
        MockCryptoService.return_value = MagicMock()
        MockCacheService.return_value = MagicMock()
        MockExtractService.return_value = MagicMock()
        MockFetchService.return_value = MagicMock()
        MockDefinitionService.return_value = mock_definition_service # Utiliser la fixture configurée
    
        updated_defs_mock = ExtractDefinitions(sources=[SourceDefinition(source_name="Updated", source_type="text", schema="file", host_parts=[], path="", extracts=[])])
        # Assurer que load_definitions retourne un tuple correct
        mock_definition_service.load_definitions.return_value = (updated_defs_mock, None)
        mock_repair_extract_markers.return_value = (updated_defs_mock, [])
    
        output_report_path = str(mock_project_root / "report.html")
        output_json_path = str(mock_project_root / "updated.json")
    
        await run_extract_repair_pipeline(
            project_root_dir=mock_project_root,
            output_report_path_str=output_report_path,
            save_changes=True,
            hitler_only=False,
            custom_input_path_str=None,
            output_json_path_str=output_json_path
        )
    
        mock_definition_service.save_definitions.assert_called_once_with(updated_defs_mock)
        mock_definition_service.export_definitions_to_json.assert_called_once_with(
            updated_defs_mock, Path(output_json_path)
        )
>       mock_generate_marker_repair_report.assert_called_once() # Vérifier qu'il est appelé
E       AssertionError: Expected 'generate_report' to have been called once. Called 0 times.

tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py:208: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:18,748 - tests.mocks.jpype_setup - INFO - Test test_run_extract_repair_pipeline_with_save_and_json_export: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:18,748 - tests.mocks.jpype_setup - INFO - Test test_run_extract_repair_pipeline_with_save_and_json_export demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
---------------------------- Captured stdout call -----------------------------
20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] D\xe9marrage du pipeline de r\xe9paration des bornes d\xe9fectueuses...\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Racine du projet utilis\xe9e pour le pipeline: C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Initialisation manuelle des services pour repair_utils...\n20:10:18 [WARNING] [argumentation_analysis.utils.dev_tools.repair_utils] Fichier de fallback JSON C:\\dev\\2025-Epita-Intelligence-Symbolique\\argumentation_analysis\\data\\extract_sources.json non trouv\xe9.\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Services initialis\xe9s manuellement pour repair_utils.\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] 1 sources charg\xe9es dans le pipeline.\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] D\xe9marrage de la r\xe9paration des bornes (pipeline)...\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] R\xe9paration termin\xe9e (pipeline). 0 r\xe9sultats obtenus.\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] G\xe9n\xe9ration du rapport dans C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\\report.html (pipeline)...\n20:10:18 [INFO] [MarkerRepairLogic] G\xe9n\xe9ration du rapport dans 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\\report.html'...\n20:10:18 [INFO] [MarkerRepairLogic] Rapport g\xe9n\xe9r\xe9 dans 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\\report.html'.\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Rapport g\xe9n\xe9r\xe9 dans C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\\report.html (pipeline)\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Sauvegarde des modifications (pipeline)...\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] \u2705 Modifications sauvegard\xe9es avec succ\xe8s (pipeline).\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Exportation des d\xe9finitions JSON mises \xe0 jour vers C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\\updated.json (pipeline)...\n20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Exported successfully
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:211 D\xe9marrage du pipeline de r\xe9paration des bornes d\xe9fectueuses...\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:212 Racine du projet utilis\xe9e pour le pipeline: C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:223 Initialisation manuelle des services pour repair_utils...\nWARNING  argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:236 Fichier de fallback JSON C:\\dev\\2025-Epita-Intelligence-Symbolique\\argumentation_analysis\\data\\extract_sources.json non trouv\xe9.\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:256 Services initialis\xe9s manuellement pour repair_utils.\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:268 1 sources charg\xe9es dans le pipeline.\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:282 D\xe9marrage de la r\xe9paration des bornes (pipeline)...\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:287 R\xe9paration termin\xe9e (pipeline). 0 r\xe9sultats obtenus.\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:291 G\xe9n\xe9ration du rapport dans C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\\report.html (pipeline)...\nINFO     MarkerRepairLogic:marker_repair_logic.py:185 G\xe9n\xe9ration du rapport dans 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\\report.html'...\nINFO     MarkerRepairLogic:marker_repair_logic.py:309 Rapport g\xe9n\xe9r\xe9 dans 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\\report.html'.\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:294 Rapport g\xe9n\xe9r\xe9 dans C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\\report.html (pipeline)\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:297 Sauvegarde des modifications (pipeline)...\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:300 \u2705 Modifications sauvegard\xe9es avec succ\xe8s (pipeline).\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:307 Exportation des d\xe9finitions JSON mises \xe0 jour vers C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_run_extract_repair_pipeli1\\updated.json (pipeline)...\nINFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:311 Exported successfully
_____________ test_run_extract_repair_pipeline_hitler_only_filter _____________

mock_repair_extract_markers = <AsyncMock name='repair_extract_markers' id='1749998827376'>
MockFetchService = <MagicMock name='FetchService' id='1749998826704'>
MockExtractService = <MagicMock name='ExtractService' id='1749998817296'>
MockCacheService = <MagicMock name='CacheService' id='1749998814944'>
MockCryptoService = <MagicMock name='CryptoService' id='1749998815952'>
MockDefinitionService = <MagicMock name='DefinitionService' id='1749998814272'>
mock_create_llm_service = <MagicMock name='create_llm_service' id='1749998825024'>
mock_project_root = WindowsPath('C:/Users/jsboi/AppData/Local/Temp/pytest-of-jsboi/pytest-204/test_run_extract_repair_pipeli2')
mock_llm_service = <MagicMock name='create_llm_service()' id='1746725187472'>
mock_definition_service = <MagicMock name='DefinitionService()' id='1746725181424'>

    @patch("argumentation_analysis.utils.dev_tools.repair_utils.create_llm_service")
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.DefinitionService")
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.CryptoService") # Ajouter les autres patchs de service
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.CacheService")
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.ExtractService")
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.FetchService")
    @patch("argumentation_analysis.utils.dev_tools.repair_utils.repair_extract_markers", new_callable=AsyncMock)
    @pytest.mark.asyncio
    async def test_run_extract_repair_pipeline_hitler_only_filter(
        mock_repair_extract_markers: AsyncMock, # Renommé
        MockFetchService: MagicMock,
        MockExtractService: MagicMock,
        MockCacheService: MagicMock,
        MockCryptoService: MagicMock,
        MockDefinitionService: MagicMock,
        mock_create_llm_service: MagicMock, # Renommé
        mock_project_root: Path,
        mock_llm_service: MagicMock, # Fixture
        mock_definition_service: MagicMock # Fixture
    ):
        """Teste le filtrage --hitler-only."""
        mock_create_llm_service.return_value = mock_llm_service
    
        # Configurer les instances mockées
        MockCryptoService.return_value = MagicMock()
        MockCacheService.return_value = MagicMock()
        MockExtractService.return_value = MagicMock()
        MockFetchService.return_value = MagicMock()
        MockDefinitionService.return_value = mock_definition_service
    
        # Configurer mock_definition_service_fixture pour retourner plusieurs sources
        sources_data = [
            SourceDefinition(source_name="Discours d'Hitler 1", source_type="text", schema="file", host_parts=[], path="", extracts=[]),
            SourceDefinition(source_name="Autre Discours", source_type="text", schema="file", host_parts=[], path="", extracts=[]),
            SourceDefinition(source_name="Texte Hitler sur la fin", source_type="text", schema="file", host_parts=[], path="", extracts=[])
        ]
        mock_definition_service.load_definitions.return_value = (ExtractDefinitions(sources=sources_data), None)
    
        mock_repair_extract_markers.return_value = (ExtractDefinitions(sources=[]), []) # Peu importe le retour ici
    
        await run_extract_repair_pipeline(
            project_root_dir=mock_project_root,
            output_report_path_str=str(mock_project_root / "report.html"),
            save_changes=False,
            hitler_only=True, # Activer le filtre
            custom_input_path_str=None,
            output_json_path_str=None
        )
    
        mock_repair_extract_markers.assert_called_once()
        # Vérifier que les définitions passées à repair_extract_markers sont filtrées
>       called_with_definitions = mock_repair_markers.call_args[0][0]
E       NameError: name 'mock_repair_markers' is not defined

tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py:262: NameError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:18,832 - tests.mocks.jpype_setup - INFO - Test test_run_extract_repair_pipeline_hitler_only_filter: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:18,832 - tests.mocks.jpype_setup - INFO - Test test_run_extract_repair_pipeline_hitler_only_filter demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
---------------------------- Captured stdout call -----------------------------
20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Démarrage du pipeline de réparation des bornes défectueuses...
20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Racine du projet utilisée pour le pipeline: C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_run_extract_repair_pipeli2
20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Initialisation manuelle des services pour repair_utils...
20:10:18 [WARNING] [argumentation_analysis.utils.dev_tools.repair_utils] Fichier de fallback JSON C:\dev\2025-Epita-Intelligence-Symbolique\argumentation_analysis\data\extract_sources.json non trouvé.
20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Services initialisés manuellement pour repair_utils.
20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] 3 sources chargées dans le pipeline.
20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Filtrage des sources (pipeline): 2/3 sources retenues.
20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Démarrage de la réparation des bornes (pipeline)...
20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Réparation terminée (pipeline). 0 résultats obtenus.
20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Génération du rapport dans C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_run_extract_repair_pipeli2\report.html (pipeline)...
20:10:18 [INFO] [MarkerRepairLogic] Génération du rapport dans 'C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_run_extract_repair_pipeli2\report.html'...
20:10:18 [INFO] [MarkerRepairLogic] Rapport généré dans 'C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_run_extract_repair_pipeli2\report.html'.
20:10:18 [INFO] [argumentation_analysis.utils.dev_tools.repair_utils] Rapport généré dans C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_run_extract_repair_pipeli2\report.html (pipeline)
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:211 Démarrage du pipeline de réparation des bornes défectueuses...
INFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:212 Racine du projet utilisée pour le pipeline: C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_run_extract_repair_pipeli2
INFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:223 Initialisation manuelle des services pour repair_utils...
WARNING  argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:236 Fichier de fallback JSON C:\dev\2025-Epita-Intelligence-Symbolique\argumentation_analysis\data\extract_sources.json non trouvé.
INFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:256 Services initialisés manuellement pour repair_utils.
INFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:268 3 sources chargées dans le pipeline.
INFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:276 Filtrage des sources (pipeline): 2/3 sources retenues.
INFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:282 Démarrage de la réparation des bornes (pipeline)...
INFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:287 Réparation terminée (pipeline). 0 résultats obtenus.
INFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:291 Génération du rapport dans C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_run_extract_repair_pipeli2\report.html (pipeline)...
INFO     MarkerRepairLogic:marker_repair_logic.py:185 Génération du rapport dans 'C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_run_extract_repair_pipeli2\report.html'...
INFO     MarkerRepairLogic:marker_repair_logic.py:309 Rapport généré dans 'C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_run_extract_repair_pipeli2\report.html'.
INFO     argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:294 Rapport généré dans C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_run_extract_repair_pipeli2\report.html (pipeline)
________________________ test_setup_agents_successful _________________________

self = <MagicMock name='mock.add_service' id='1750000150448'>
args = (<MagicMock id='1750000149440'>,), kwargs = {}
msg = "Expected 'add_service' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'add_service' to be called once. Called 0 times.

C:\Python313\Lib\unittest\mock.py:988: AssertionError

During handling of the above exception, another exception occurred:

mock_llm_service = <MagicMock id='1750000149440'>
mock_sk_kernel = <MagicMock spec='Kernel' id='1750000149776'>

    @pytest.mark.asyncio
    async def test_setup_agents_successful(mock_llm_service: MagicMock, mock_sk_kernel: MagicMock):
        """Teste la configuration réussie des agents."""
        mock_llm_service.service_id = "test_service_id" # Nécessaire pour get_prompt_execution_settings
    
        # ChatCompletionAgent n'est plus utilisé directement par setup_agents dans repair_utils.py
        # La fonction setup_agents retourne (None, None)
        # with patch("project_core.dev_utils.repair_utils.ChatCompletionAgent", spec=ChatCompletionAgent) as MockAgent:
            # repair_agent_instance = MagicMock()
            # validation_agent_instance = MagicMock()
            # MockAgent.side_effect = [repair_agent_instance, validation_agent_instance]
    
        repair_agent, validation_agent = await setup_agents(mock_llm_service, mock_sk_kernel)
    
>       mock_sk_kernel.add_service.assert_called_once_with(mock_llm_service)
E       AssertionError: Expected 'add_service' to be called once. Called 0 times.

tests\argumentation_analysis\utils\dev_tools\test_repair_utils.py:350: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:18,879 - tests.mocks.jpype_setup - INFO - Test test_setup_agents_successful: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:18,879 - tests.mocks.jpype_setup - INFO - Test test_setup_agents_successful demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
---------------------------- Captured stdout call -----------------------------
20:10:18 [WARNING] [argumentation_analysis.utils.dev_tools.repair_utils] setup_agents: ChatCompletionAgent est temporairement désactivé. Retour de (None, None).
------------------------------ Captured log call ------------------------------
WARNING  argumentation_analysis.utils.dev_tools.repair_utils:repair_utils.py:62 setup_agents: ChatCompletionAgent est temporairement désactivé. Retour de (None, None).
__________________ test_parametrized_dependency_import[nltk] __________________

dependency = 'nltk'

    @pytest.mark.use_real_numpy
    @pytest.mark.parametrize("dependency", PROJECT_DEPENDENCIES_PARAMETRIZED)
    def test_parametrized_dependency_import(dependency):
        """Vérifie que chaque dépendance majeure paramétrée peut être importée."""
        try:
>           importlib.import_module(dependency)

tests\environment_checks\test_core_dependencies.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'nltk', import_ = <function _gcd_import at 0x00000196C95804A0>

>   ???
E   ModuleNotFoundError: No module named 'nltk'

<frozen importlib._bootstrap>:1324: ModuleNotFoundError

During handling of the above exception, another exception occurred:

dependency = 'nltk'

    @pytest.mark.use_real_numpy
    @pytest.mark.parametrize("dependency", PROJECT_DEPENDENCIES_PARAMETRIZED)
    def test_parametrized_dependency_import(dependency):
        """Vérifie que chaque dépendance majeure paramétrée peut être importée."""
        try:
            importlib.import_module(dependency)
            # Optionnel: afficher la version si disponible
            # module = importlib.import_module(dependency)
            # print(f"{dependency} version: {getattr(module, '__version__', 'N/A')}")
        except ImportError as e:
>           pytest.fail(f"Échec de l'importation de la dépendance '{dependency}': {e}")
E           Failed: Échec de l'importation de la dépendance 'nltk': No module named 'nltk'

tests\environment_checks\test_core_dependencies.py:85: Failed
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:18,963 - tests.mocks.jpype_setup - INFO - Test test_parametrized_dependency_import[nltk]: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:18,963 - tests.mocks.jpype_setup - INFO - Test test_parametrized_dependency_import[nltk] demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
_________________ test_parametrized_dependency_import[spacy] __________________

dependency = 'spacy'

    @pytest.mark.use_real_numpy
    @pytest.mark.parametrize("dependency", PROJECT_DEPENDENCIES_PARAMETRIZED)
    def test_parametrized_dependency_import(dependency):
        """Vérifie que chaque dépendance majeure paramétrée peut être importée."""
        try:
>           importlib.import_module(dependency)

tests\environment_checks\test_core_dependencies.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'spacy', import_ = <function _gcd_import at 0x00000196C95804A0>

>   ???
E   ModuleNotFoundError: No module named 'spacy'

<frozen importlib._bootstrap>:1324: ModuleNotFoundError

During handling of the above exception, another exception occurred:

dependency = 'spacy'

    @pytest.mark.use_real_numpy
    @pytest.mark.parametrize("dependency", PROJECT_DEPENDENCIES_PARAMETRIZED)
    def test_parametrized_dependency_import(dependency):
        """Vérifie que chaque dépendance majeure paramétrée peut être importée."""
        try:
            importlib.import_module(dependency)
            # Optionnel: afficher la version si disponible
            # module = importlib.import_module(dependency)
            # print(f"{dependency} version: {getattr(module, '__version__', 'N/A')}")
        except ImportError as e:
>           pytest.fail(f"Échec de l'importation de la dépendance '{dependency}': {e}")
E           Failed: Échec de l'importation de la dépendance 'spacy': No module named 'spacy'

tests\environment_checks\test_core_dependencies.py:85: Failed
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:19,040 - tests.mocks.jpype_setup - INFO - Test test_parametrized_dependency_import[spacy]: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:19,040 - tests.mocks.jpype_setup - INFO - Test test_parametrized_dependency_import[spacy] demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
__________________ test_load_definitions_encrypted_wrong_key __________________

test_env = {'crypto_service': <argumentation_analysis.services.crypto_service.CryptoService object at 0x00000196855CD370>, 'defin...ypte2/test_definitions_dir/extract_definitions.json.enc'), 'key': b'l5jcza0WROjcwSOmNC5tl_3ofHNE246nfmLp4rx0AVg=', ...}

    def test_load_definitions_encrypted_wrong_key(test_env):
        wrong_key = test_env['crypto_service'].generate_key() # bytes
    
>       with pytest.raises(InvalidToken):
E       Failed: DID NOT RAISE <class 'cryptography.fernet.InvalidToken'>

tests\ui\test_extract_definition_persistence.py:108: Failed
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:19,858 - tests.mocks.jpype_setup - INFO - Test test_load_definitions_encrypted_wrong_key: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:19,859 - tests.mocks.jpype_setup - INFO - Test test_load_definitions_encrypted_wrong_key demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:19 [WARNING] [Services.CryptoService] Service de chiffrement initialisé sans clé. Le chiffrement est désactivé.
20:10:19 [INFO] [Services.CryptoService] Nouvelle clé de chiffrement générée avec succès.
20:10:19 [INFO] [Services.CryptoService] Clé de chiffrement sauvegardée dans C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_load_definitions_encrypte2\test_definitions_dir\definitions_key.key
20:10:19 [INFO] [Services.CryptoService] Clé de chiffrement mise à jour.
----------------------------- Captured log setup ------------------------------
WARNING  Services.CryptoService:crypto_service.py:42 Service de chiffrement initialisé sans clé. Le chiffrement est désactivé.
INFO     Services.CryptoService:crypto_service.py:229 Nouvelle clé de chiffrement générée avec succès.
INFO     Services.CryptoService:crypto_service.py:249 Clé de chiffrement sauvegardée dans C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_load_definitions_encrypte2\test_definitions_dir\definitions_key.key
INFO     Services.CryptoService:crypto_service.py:87 Clé de chiffrement mise à jour.
---------------------------- Captured stdout call -----------------------------
20:10:19 [INFO] [Services.CryptoService] Nouvelle cl\xe9 de chiffrement g\xe9n\xe9r\xe9e avec succ\xe8s.\n20:10:19 [INFO] [App.UI.Utils] Chargement et d\xe9chiffrement de 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_definitions_encrypte2\\test_definitions_dir\\extract_definitions.json.enc' avec cl\xe9...\n20:10:19 [ERROR] [argumentation_analysis.utils.core_utils.crypto_utils] Erreur d\xe9chiffrement Fernet (InvalidToken/Signature): \n20:10:19 [WARNING] [App.UI.Utils] \u26a0\ufe0f \xc9chec du d\xe9chiffrement pour 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_definitions_encrypte2\\test_definitions_dir\\extract_definitions.json.enc' (decrypt_data_with_fernet a retourn\xe9 None). Utilisation des d\xe9finitions par d\xe9faut.
---------------------------- Captured stderr call -----------------------------
20:10:19 [ERROR] argumentation_analysis.utils.core_utils.crypto_utils: Erreur déchiffrement Fernet (InvalidToken/Signature): 
------------------------------ Captured log call ------------------------------
INFO     Services.CryptoService:crypto_service.py:229 Nouvelle cl\xe9 de chiffrement g\xe9n\xe9r\xe9e avec succ\xe8s.\nINFO     App.UI.Utils:file_operations.py:33 Chargement et d\xe9chiffrement de 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_definitions_encrypte2\\test_definitions_dir\\extract_definitions.json.enc' avec cl\xe9...\nERROR    argumentation_analysis.utils.core_utils.crypto_utils:crypto_utils.py:159 Erreur d\xe9chiffrement Fernet (InvalidToken/Signature): \nWARNING  App.UI.Utils:file_operations.py:39 \u26a0\ufe0f \xc9chec du d\xe9chiffrement pour 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_definitions_encrypte2\\test_definitions_dir\\extract_definitions.json.enc' (decrypt_data_with_fernet a retourn\xe9 None). Utilisation des d\xe9finitions par d\xe9faut.
__________________________ test_load_malformed_json ___________________________

test_env = {'crypto_service': <argumentation_analysis.services.crypto_service.CryptoService object at 0x00000197742DFD50>, 'defin...json0/test_definitions_dir/extract_definitions.json.enc'), 'key': b'pFvwBhJrLG9ZMUbSOzG13OM4yqvSkB4tvJylMLkLtYQ=', ...}

    def test_load_malformed_json(test_env):
        malformed_json_file = test_env['test_dir'] / "malformed.json"
    
        with open(malformed_json_file, 'w') as f:
            f.write("{'sources': [}")
    
        with pytest.raises(json.JSONDecodeError):
             load_extract_definitions(config_file=malformed_json_file, b64_derived_key=None)
    
        malformed_encrypted_file = test_env['test_dir'] / "malformed_encrypted.json.enc"
        with open(malformed_encrypted_file, 'wb') as f:
            f.write(b"this is not encrypted data")
    
>       with pytest.raises(InvalidToken):
E       Failed: DID NOT RAISE <class 'cryptography.fernet.InvalidToken'>

tests\ui\test_extract_definition_persistence.py:169: Failed
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:19,923 - tests.mocks.jpype_setup - INFO - Test test_load_malformed_json: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:19,924 - tests.mocks.jpype_setup - INFO - Test test_load_malformed_json demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:19 [WARNING] [Services.CryptoService] Service de chiffrement initialisé sans clé. Le chiffrement est désactivé.
20:10:19 [INFO] [Services.CryptoService] Nouvelle clé de chiffrement générée avec succès.
20:10:19 [INFO] [Services.CryptoService] Clé de chiffrement sauvegardée dans C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_load_malformed_json0\test_definitions_dir\definitions_key.key
20:10:19 [INFO] [Services.CryptoService] Clé de chiffrement mise à jour.
----------------------------- Captured log setup ------------------------------
WARNING  Services.CryptoService:crypto_service.py:42 Service de chiffrement initialisé sans clé. Le chiffrement est désactivé.
INFO     Services.CryptoService:crypto_service.py:229 Nouvelle clé de chiffrement générée avec succès.
INFO     Services.CryptoService:crypto_service.py:249 Clé de chiffrement sauvegardée dans C:\Users\jsboi\AppData\Local\Temp\pytest-of-jsboi\pytest-204\test_load_malformed_json0\test_definitions_dir\definitions_key.key
INFO     Services.CryptoService:crypto_service.py:87 Clé de chiffrement mise à jour.
---------------------------- Captured stdout call -----------------------------
20:10:19 [INFO] [App.UI.Utils] Aucune cl\xe9 fournie. Tentative de chargement de 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_malformed_json0\\test_definitions_dir\\malformed.json' comme JSON simple...\n20:10:19 [ERROR] [App.UI.Utils] \u274c Erreur d\xe9codage JSON pour 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_malformed_json0\\test_definitions_dir\\malformed.json': Expecting property name enclosed in double quotes: line 1 column 2 (char 1). L'exception sera relanc\xe9e.\n20:10:19 [INFO] [App.UI.Utils] Chargement et d\xe9chiffrement de 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_malformed_json0\\test_definitions_dir\\malformed_encrypted.json.enc' avec cl\xe9...\n20:10:19 [ERROR] [argumentation_analysis.utils.core_utils.crypto_utils] Erreur d\xe9chiffrement Fernet (InvalidToken/Signature): \n20:10:19 [WARNING] [App.UI.Utils] \u26a0\ufe0f \xc9chec du d\xe9chiffrement pour 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_malformed_json0\\test_definitions_dir\\malformed_encrypted.json.enc' (decrypt_data_with_fernet a retourn\xe9 None). Utilisation des d\xe9finitions par d\xe9faut.
---------------------------- Captured stderr call -----------------------------
20:10:19 [ERROR] argumentation_analysis.utils.core_utils.crypto_utils: Erreur déchiffrement Fernet (InvalidToken/Signature): 
------------------------------ Captured log call ------------------------------
INFO     App.UI.Utils:file_operations.py:54 Aucune cl\xe9 fournie. Tentative de chargement de 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_malformed_json0\\test_definitions_dir\\malformed.json' comme JSON simple...\nERROR    App.UI.Utils:file_operations.py:61 \u274c Erreur d\xe9codage JSON pour 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_malformed_json0\\test_definitions_dir\\malformed.json': Expecting property name enclosed in double quotes: line 1 column 2 (char 1). L'exception sera relanc\xe9e.\nINFO     App.UI.Utils:file_operations.py:33 Chargement et d\xe9chiffrement de 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_malformed_json0\\test_definitions_dir\\malformed_encrypted.json.enc' avec cl\xe9...\nERROR    argumentation_analysis.utils.core_utils.crypto_utils:crypto_utils.py:159 Erreur d\xe9chiffrement Fernet (InvalidToken/Signature): \nWARNING  App.UI.Utils:file_operations.py:39 \u26a0\ufe0f \xc9chec du d\xe9chiffrement pour 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-204\\test_load_malformed_json0\\test_definitions_dir\\malformed_encrypted.json.enc' (decrypt_data_with_fernet a retourn\xe9 None). Utilisation des d\xe9finitions par d\xe9faut.
__________ test_mine_arguments_explicit_premise_conclusion_too_short __________

miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x00000196813E7350>

    def test_mine_arguments_explicit_premise_conclusion_too_short(miner_default: MockArgumentMiner):
        """Teste prémisse/conclusion explicites mais contenu trop court."""
        text = "Prémisse: A. Conclusion: B." # "A." (len 2) et "B." (len 2) < min_length 10
        result = miner_default.mine_arguments(text)
        # Devrait tomber sur Affirmation Simple si le texte global est assez long
        assert len(result) == 1
        assert result[0]["type"] == "Affirmation Simple (Mock)"
    
        text_custom_miner = "Prémisse: Un. Conclusion: Deux." # "Un." (len 3), "Deux." (len 5)
        # Pour miner_custom_config (min_length 5), "Deux." est OK, "Un." ne l'est pas.
        # Donc, l'argument explicite ne devrait pas être formé.
>       result_custom = miner_custom_config.mine_arguments(text_custom_miner)
E       AttributeError: 'function' object has no attribute 'mine_arguments'

tests\unit\argumentation_analysis\mocks\test_argument_mining.py:93: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,096 - tests.mocks.jpype_setup - INFO - Test test_mine_arguments_explicit_premise_conclusion_too_short: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,096 - tests.mocks.jpype_setup - INFO - Test test_mine_arguments_explicit_premise_conclusion_too_short demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner initialisé avec config: {} (min_length: 10)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:18 MockArgumentMiner initialisé avec config: {} (min_length: 10)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner analyse le texte : Prémisse: A. Conclusion: B....
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner a trouvé 1 arguments.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:37 MockArgumentMiner analyse le texte : Prémisse: A. Conclusion: B....
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:120 MockArgumentMiner a trouvé 1 arguments.
_________________ test_mine_arguments_implicit_par_consequent _________________

miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x00000196813C0320>

    def test_mine_arguments_implicit_par_consequent(miner_default: MockArgumentMiner):
        """Teste argument implicite avec 'par conséquent'."""
        text = "Les études le montrent clairement. Par conséquent, nous devons agir."
        result = miner_default.mine_arguments(text)
        assert len(result) == 1
        arg = result[0]
        assert arg["type"] == "Argument Implicite (Mock - par conséquent)"
        assert arg["premise"] == "Les études le montrent clairement"
>       assert arg["conclusion"] == "nous devons agir"
E       AssertionError: assert ', nous devons agir' == 'nous devons agir'
E         
E         - nous devons agir
E         + , nous devons agir
E         ? ++

tests\unit\argumentation_analysis\mocks\test_argument_mining.py:124: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,105 - tests.mocks.jpype_setup - INFO - Test test_mine_arguments_implicit_par_consequent: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,105 - tests.mocks.jpype_setup - INFO - Test test_mine_arguments_implicit_par_consequent demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner initialisé avec config: {} (min_length: 10)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:18 MockArgumentMiner initialisé avec config: {} (min_length: 10)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner analyse le texte : Les études le montrent clairement. Par conséquent, nous devons agir....
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner a trouvé 1 arguments.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:37 MockArgumentMiner analyse le texte : Les études le montrent clairement. Par conséquent, nous devons agir....
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:120 MockArgumentMiner a trouvé 1 arguments.
_____________________ test_mine_arguments_implicit_ainsi ______________________

miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x00000196813C0F50>

    def test_mine_arguments_implicit_ainsi(miner_default: MockArgumentMiner):
        """Teste argument implicite avec 'ainsi'."""
        text = "Le budget est limité. Ainsi, certains projets seront reportés."
        result = miner_default.mine_arguments(text)
        assert len(result) == 1
        arg = result[0]
        assert arg["type"] == "Argument Implicite (Mock - ainsi)"
        assert arg["premise"] == "Le budget est limité"
>       assert arg["conclusion"] == "certains projets seront reportés"
E       AssertionError: assert ', certains p...ront reportés' == 'certains pro...ront reportés'
E         
E         - certains projets seront reportés
E         + , certains projets seront reportés
E         ? ++

tests\unit\argumentation_analysis\mocks\test_argument_mining.py:134: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,110 - tests.mocks.jpype_setup - INFO - Test test_mine_arguments_implicit_ainsi: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,110 - tests.mocks.jpype_setup - INFO - Test test_mine_arguments_implicit_ainsi demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner initialisé avec config: {} (min_length: 10)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:18 MockArgumentMiner initialisé avec config: {} (min_length: 10)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner analyse le texte : Le budget est limité. Ainsi, certains projets seront reportés....
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner a trouvé 1 arguments.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:37 MockArgumentMiner analyse le texte : Le budget est limité. Ainsi, certains projets seront reportés....
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:120 MockArgumentMiner a trouvé 1 arguments.
_________________ test_mine_arguments_explicit_over_implicit __________________

miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x0000019681381A90>

    def test_mine_arguments_explicit_over_implicit(miner_default: MockArgumentMiner):
        """Teste que l'explicite a priorité et évite doublon avec implicite."""
        text = "Prémisse: C'est un fait. Conclusion: Il faut l'accepter. C'est un fait, donc il faut l'accepter."
        # L'explicite devrait trouver "C'est un fait." et "Il faut l'accepter."
        # L'implicite (avec "donc") trouverait "C'est un fait" et "il faut l'accepter."
        # Le mock devrait éviter ce doublon.
        result = miner_default.mine_arguments(text)
>       assert len(result) == 1 # Un seul argument, l'explicite
E       assert 2 == 1
E        +  where 2 = len([{'conclusion': "Il faut l'accepter.", 'confidence': 0.85, 'details': 'Prémisse et conclusion explicitement marquées.'...epter", 'confidence': 0.7, 'details': "Conclusion inférée par l'indicateur 'donc'.", 'premise': "C'est un fait,", ...}])

tests\unit\argumentation_analysis\mocks\test_argument_mining.py:151: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,116 - tests.mocks.jpype_setup - INFO - Test test_mine_arguments_explicit_over_implicit: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,116 - tests.mocks.jpype_setup - INFO - Test test_mine_arguments_explicit_over_implicit demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner initialisé avec config: {} (min_length: 10)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:18 MockArgumentMiner initialisé avec config: {} (min_length: 10)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner analyse le texte : Prémisse: C'est un fait. Conclusion: Il faut l'accepter. C'est un fait, donc il faut l'accepter....
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner a trouvé 2 arguments.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:37 MockArgumentMiner analyse le texte : Prémisse: C'est un fait. Conclusion: Il faut l'accepter. C'est un fait, donc il faut l'accepter....
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:120 MockArgumentMiner a trouvé 2 arguments.
_________________ test_mine_arguments_complex_scenario_mixed __________________

miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x00000196857887D0>

    def test_mine_arguments_complex_scenario_mixed(miner_default: MockArgumentMiner):
        """Teste un scénario mixte avec explicite et implicite."""
        text = (
            "Prémisse: Le ciel est bleu. Conclusion: C'est une belle journée. "
            "Le soleil brille fortement, par conséquent il fait chaud."
        )
        result = miner_default.mine_arguments(text)
        assert len(result) == 2
    
        found_explicit = False
        found_implicit = False
        for arg in result:
            if arg["type"] == "Argument Explicite (Mock)":
                assert arg["premise"] == "Le ciel est bleu."
                assert arg["conclusion"] == "C'est une belle journée."
                found_explicit = True
            elif arg["type"] == "Argument Implicite (Mock - par conséquent)":
>               assert arg["premise"] == "Le soleil brille fortement" # Dernière phrase avant "par conséquent"
E               AssertionError: assert 'Le soleil brille fortement,' == 'Le soleil brille fortement'
E                 
E                 - Le soleil brille fortement
E                 + Le soleil brille fortement,
E                 ?                           +

tests\unit\argumentation_analysis\mocks\test_argument_mining.py:193: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,122 - tests.mocks.jpype_setup - INFO - Test test_mine_arguments_complex_scenario_mixed: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,122 - tests.mocks.jpype_setup - INFO - Test test_mine_arguments_complex_scenario_mixed demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner initialisé avec config: {} (min_length: 10)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:18 MockArgumentMiner initialisé avec config: {} (min_length: 10)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner analyse le texte : Prémisse: Le ciel est bleu. Conclusion: C'est une belle journée. Le soleil brille fortement, par con...
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner a trouvé 2 arguments.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:37 MockArgumentMiner analyse le texte : Prémisse: Le ciel est bleu. Conclusion: C'est une belle journée. Le soleil brille fortement, par con...
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:120 MockArgumentMiner a trouvé 2 arguments.
____________________ test_conclusion_delimitation_explicit ____________________

miner_default = <argumentation_analysis.mocks.argument_mining.MockArgumentMiner object at 0x0000019685788A10>

    def test_conclusion_delimitation_explicit(miner_default: MockArgumentMiner):
        """Teste la délimitation de la conclusion pour un argument explicite."""
        text_with_period = "Prémisse: P1. Conclusion: C1 est vraie. Ceci est une autre phrase."
        result = miner_default.mine_arguments(text_with_period)
        assert len(result) == 1
>       assert result[0]["conclusion"] == "C1 est vraie." # Doit s'arrêter au point.
E       KeyError: 'conclusion'

tests\unit\argumentation_analysis\mocks\test_argument_mining.py:205: KeyError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,128 - tests.mocks.jpype_setup - INFO - Test test_conclusion_delimitation_explicit: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,128 - tests.mocks.jpype_setup - INFO - Test test_conclusion_delimitation_explicit demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner initialisé avec config: {} (min_length: 10)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:18 MockArgumentMiner initialisé avec config: {} (min_length: 10)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner analyse le texte : Prémisse: P1. Conclusion: C1 est vraie. Ceci est une autre phrase....
20:10:21 [INFO] [argumentation_analysis.mocks.argument_mining] MockArgumentMiner a trouvé 1 arguments.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:37 MockArgumentMiner analyse le texte : Prémisse: P1. Conclusion: C1 est vraie. Ceci est une autre phrase....
INFO     argumentation_analysis.mocks.argument_mining:argument_mining.py:120 MockArgumentMiner a trouvé 1 arguments.
________________________ test_detect_bias_confirmation ________________________

detector_default = <argumentation_analysis.mocks.bias_detection.MockBiasDetector object at 0x00000197740B3410>

    def test_detect_bias_confirmation(detector_default: MockBiasDetector):
        text = "Il est évident que cette solution est la meilleure pour tout le monde."
        result = detector_default.detect_biases(text)
        assert len(result) >= 1
        found_bias = any(b["bias_type"] == "Biais de Confirmation (Mock)" and b["detected_pattern"] == r"il est évident que" for b in result)
        assert found_bias
        # Vérifier le contexte (le mock prend 30 chars avant/après)
        bias_entry = next(b for b in result if b["bias_type"] == "Biais de Confirmation (Mock)")
        assert "Il est évident que cette solut" in bias_entry["context_snippet"] # Début du contexte
>       assert "est la meilleure pour tout le" in bias_entry["context_snippet"] # Fin du contexte
E       AssertionError: assert 'est la meilleure pour tout le' in 'Il est évident que cette solution est la meilleu'

tests\unit\argumentation_analysis\mocks\test_bias_detection.py:71: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,138 - tests.mocks.jpype_setup - INFO - Test test_detect_bias_confirmation: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,138 - tests.mocks.jpype_setup - INFO - Test test_detect_bias_confirmation demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.bias_detection] MockBiasDetector initialisé avec config: {} (patterns: 4)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.bias_detection:bias_detection.py:27 MockBiasDetector initialisé avec config: {} (patterns: 4)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.bias_detection] MockBiasDetector analyse le texte : Il est évident que cette solution est la meilleure pour tout le monde....
20:10:21 [INFO] [argumentation_analysis.mocks.bias_detection] MockBiasDetector a trouvé 1 biais potentiels.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.bias_detection:bias_detection.py:46 MockBiasDetector analyse le texte : Il est évident que cette solution est la meilleure pour tout le monde....
INFO     argumentation_analysis.mocks.bias_detection:bias_detection.py:99 MockBiasDetector a trouvé 1 biais potentiels.
__________________ test_extract_claims_global_claim_success ___________________

miner_default = <argumentation_analysis.mocks.claim_mining.MockClaimMiner object at 0x00000197740B17F0>

    def test_extract_claims_global_claim_success(miner_default: MockClaimMiner):
        """Teste le cas d'une revendication globale (pas de mots-clés, pas de phrases assertives distinctes)."""
        text = "Ceci est une affirmation unique sans rien de spécial." # > 8 chars
        result = miner_default.extract_claims(text)
        assert len(result) == 1
        claim = result[0]
>       assert claim["type"] == "Revendication Globale (Mock)"
E       AssertionError: assert 'Revendicatio...ertive (Mock)' == 'Revendication Globale (Mock)'
E         
E         - Revendication Globale (Mock)
E         ?               ^^^^^^
E         + Revendication Assertive (Mock)
E         ?               ^^^ +++++

tests\unit\argumentation_analysis\mocks\test_claim_mining.py:66: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,160 - tests.mocks.jpype_setup - INFO - Test test_extract_claims_global_claim_success: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,160 - tests.mocks.jpype_setup - INFO - Test test_extract_claims_global_claim_success demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.claim_mining] MockClaimMiner initialisé avec config: {} (min_length: 8, keywords: ['il est clair que', 'nous affirmons', 'le point principal est', 'il faut noter'])
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.claim_mining:claim_mining.py:22 MockClaimMiner initialisé avec config: {} (min_length: 8, keywords: ['il est clair que', 'nous affirmons', 'le point principal est', 'il faut noter'])
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.claim_mining] MockClaimMiner analyse le texte : Ceci est une affirmation unique sans rien de spécial....
20:10:21 [INFO] [argumentation_analysis.mocks.claim_mining] MockClaimMiner a trouvé 1 revendications.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.claim_mining:claim_mining.py:41 MockClaimMiner analyse le texte : Ceci est une affirmation unique sans rien de spécial....
INFO     argumentation_analysis.mocks.claim_mining:claim_mining.py:98 MockClaimMiner a trouvé 1 revendications.
_________________ test_extract_claims_global_claim_too_short __________________

miner_custom_config = <argumentation_analysis.mocks.claim_mining.MockClaimMiner object at 0x00000196827648D0>

    def test_extract_claims_global_claim_too_short(miner_custom_config: MockClaimMiner):
        """Teste qu'une revendication globale trop courte n'est pas retournée."""
        # min_claim_length est 5 pour miner_custom_config
        assert miner_custom_config.extract_claims("Test") == [] # len 4 < 5
    
        text_just_long_enough = "Assez" # len 5
        result = miner_custom_config.extract_claims(text_just_long_enough)
        assert len(result) == 1
>       assert result[0]["type"] == "Revendication Globale (Mock)"
E       AssertionError: assert 'Revendicatio...ertive (Mock)' == 'Revendication Globale (Mock)'
E         
E         - Revendication Globale (Mock)
E         ?               ^^^^^^
E         + Revendication Assertive (Mock)
E         ?               ^^^ +++++

tests\unit\argumentation_analysis\mocks\test_claim_mining.py:79: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,164 - tests.mocks.jpype_setup - INFO - Test test_extract_claims_global_claim_too_short: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,164 - tests.mocks.jpype_setup - INFO - Test test_extract_claims_global_claim_too_short demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.claim_mining] MockClaimMiner initialisé avec config: {'min_claim_length': 5, 'claim_keywords': ['point clé:', 'important:']} (min_length: 5, keywords: ['point clé:', 'important:'])
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.claim_mining:claim_mining.py:22 MockClaimMiner initialisé avec config: {'min_claim_length': 5, 'claim_keywords': ['point clé:', 'important:']} (min_length: 5, keywords: ['point clé:', 'important:'])
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.claim_mining] MockClaimMiner analyse le texte : Test...
20:10:21 [INFO] [argumentation_analysis.mocks.claim_mining] MockClaimMiner a trouvé 0 revendications.
20:10:21 [INFO] [argumentation_analysis.mocks.claim_mining] MockClaimMiner analyse le texte : Assez...
20:10:21 [INFO] [argumentation_analysis.mocks.claim_mining] MockClaimMiner a trouvé 1 revendications.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.claim_mining:claim_mining.py:41 MockClaimMiner analyse le texte : Test...
INFO     argumentation_analysis.mocks.claim_mining:claim_mining.py:98 MockClaimMiner a trouvé 0 revendications.
INFO     argumentation_analysis.mocks.claim_mining:claim_mining.py:41 MockClaimMiner analyse le texte : Assez...
INFO     argumentation_analysis.mocks.claim_mining:claim_mining.py:98 MockClaimMiner a trouvé 1 revendications.
_________ test_extract_claims_by_keyword_text_too_short_after_keyword _________

miner_default = <argumentation_analysis.mocks.claim_mining.MockClaimMiner object at 0x00000196813E6F50>

    def test_extract_claims_by_keyword_text_too_short_after_keyword(miner_default: MockClaimMiner):
        """Teste mot-clé mais texte suivant trop court."""
        text = "Finalement, il faut noter A." # "A." (len 2) < min_claim_length 8
        result = miner_default.extract_claims(text)
        # Devrait tomber sur Revendication Globale si le texte entier est assez long
        assert len(result) == 1
>       assert result[0]["type"] == "Revendication Globale (Mock)"
E       AssertionError: assert 'Revendicatio...ertive (Mock)' == 'Revendication Globale (Mock)'
E         
E         - Revendication Globale (Mock)
E         ?               ^^^^^^
E         + Revendication Assertive (Mock)
E         ?               ^^^ +++++

tests\unit\argumentation_analysis\mocks\test_claim_mining.py:112: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,170 - tests.mocks.jpype_setup - INFO - Test test_extract_claims_by_keyword_text_too_short_after_keyword: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,170 - tests.mocks.jpype_setup - INFO - Test test_extract_claims_by_keyword_text_too_short_after_keyword demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.claim_mining] MockClaimMiner initialisé avec config: {} (min_length: 8, keywords: ['il est clair que', 'nous affirmons', 'le point principal est', 'il faut noter'])
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.claim_mining:claim_mining.py:22 MockClaimMiner initialisé avec config: {} (min_length: 8, keywords: ['il est clair que', 'nous affirmons', 'le point principal est', 'il faut noter'])
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.claim_mining] MockClaimMiner analyse le texte : Finalement, il faut noter A....
20:10:21 [INFO] [argumentation_analysis.mocks.claim_mining] MockClaimMiner a trouvé 1 revendications.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.claim_mining:claim_mining.py:41 MockClaimMiner analyse le texte : Finalement, il faut noter A....
INFO     argumentation_analysis.mocks.claim_mining:claim_mining.py:98 MockClaimMiner a trouvé 1 revendications.
______________________ test_score_clarity_long_sentences ______________________

scorer_default = <argumentation_analysis.mocks.clarity_scoring.MockClarityScorer object at 0x0000019681396D70>

    def test_score_clarity_long_sentences(scorer_default: MockClarityScorer):
        """Teste l'impact des phrases longues."""
        # 30 mots / 1 phrase = 30. > 25. Pénalité -0.1
        text = "Ceci est une phrase exceptionnellement et particulièrement longue conçue dans le but unique de tester la fonctionnalité de détection de phrases longues de notre analyseur de clarté."
        result = scorer_default.score_clarity(text)
>       assert result["clarity_score"] == pytest.approx(1.0 - 0.1)
E       assert 0.75 == 0.9 ± 9.0e-07
E         
E         comparison failed
E         Obtained: 0.75
E         Expected: 0.9 ± 9.0e-07

tests\unit\argumentation_analysis\mocks\test_clarity_scoring.py:78: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,193 - tests.mocks.jpype_setup - INFO - Test test_score_clarity_long_sentences: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,193 - tests.mocks.jpype_setup - INFO - Test test_score_clarity_long_sentences demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.clarity_scoring] MockClarityScorer initialisé avec config: {}
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.clarity_scoring:clarity_scoring.py:34 MockClarityScorer initialisé avec config: {}
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.clarity_scoring] MockClarityScorer évalue le texte : Ceci est une phrase exceptionnellement et particulièrement longue conçue dans le but unique de teste...
20:10:21 [INFO] [argumentation_analysis.mocks.clarity_scoring] MockClarityScorer score calculé: 0.75, facteurs: {'long_sentences_avg': 27.0, 'complex_words_ratio': 0.11, 'passive_voice_ratio': 0, 'jargon_count': 0, 'ambiguity_keywords': 0}
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.clarity_scoring:clarity_scoring.py:52 MockClarityScorer évalue le texte : Ceci est une phrase exceptionnellement et particulièrement longue conçue dans le but unique de teste...
INFO     argumentation_analysis.mocks.clarity_scoring:clarity_scoring.py:105 MockClarityScorer score calculé: 0.75, facteurs: {'long_sentences_avg': 27.0, 'complex_words_ratio': 0.11, 'passive_voice_ratio': 0, 'jargon_count': 0, 'ambiguity_keywords': 0}
________________________ test_score_clarity_ambiguity _________________________

scorer_default = <argumentation_analysis.mocks.clarity_scoring.MockClarityScorer object at 0x00000196813E6250>

    def test_score_clarity_ambiguity(scorer_default: MockClarityScorer):
        """Teste l'impact des mots ambigus."""
        # "peut-être", "possiblement", "certains" (3 ambigus). Pénalité -0.1 * 3 = -0.3
        text = "Peut-être que cela fonctionnera. Possiblement demain. Certains pensent ainsi."
        result = scorer_default.score_clarity(text)
>       assert result["clarity_score"] == pytest.approx(1.0 - 0.1 * 3)
E       assert 0.5499999999999999 == 0.7 ± 7.0e-07
E         
E         comparison failed
E         Obtained: 0.5499999999999999
E         Expected: 0.7 ± 7.0e-07

tests\unit\argumentation_analysis\mocks\test_clarity_scoring.py:116: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,212 - tests.mocks.jpype_setup - INFO - Test test_score_clarity_ambiguity: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,212 - tests.mocks.jpype_setup - INFO - Test test_score_clarity_ambiguity demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.clarity_scoring] MockClarityScorer initialisé avec config: {}
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.clarity_scoring:clarity_scoring.py:34 MockClarityScorer initialisé avec config: {}
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.clarity_scoring] MockClarityScorer évalue le texte : Peut-être que cela fonctionnera. Possiblement demain. Certains pensent ainsi....
20:10:21 [INFO] [argumentation_analysis.mocks.clarity_scoring] MockClarityScorer score calculé: 0.55, facteurs: {'long_sentences_avg': 0, 'complex_words_ratio': 0.2, 'passive_voice_ratio': 0, 'jargon_count': 0, 'ambiguity_keywords': 3}
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.clarity_scoring:clarity_scoring.py:52 MockClarityScorer évalue le texte : Peut-être que cela fonctionnera. Possiblement demain. Certains pensent ainsi....
INFO     argumentation_analysis.mocks.clarity_scoring:clarity_scoring.py:105 MockClarityScorer score calculé: 0.55, facteurs: {'long_sentences_avg': 0, 'complex_words_ratio': 0.2, 'passive_voice_ratio': 0, 'jargon_count': 0, 'ambiguity_keywords': 3}
______________________ test_score_clarity_custom_jargon _______________________

scorer_custom_config = <argumentation_analysis.mocks.clarity_scoring.MockClarityScorer object at 0x0000019774010410>

    def test_score_clarity_custom_jargon(scorer_custom_config: MockClarityScorer):
        """Teste avec une liste de jargon personnalisée et pénalité modifiée."""
        # Jargon: "customjargon" (x1). Pénalité custom = -0.5
        text = "Ce texte utilise notre customjargon spécifique."
>       result = scorer_custom_config.score_clarity(text)

tests\unit\argumentation_analysis\mocks\test_clarity_scoring.py:155: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <argumentation_analysis.mocks.clarity_scoring.MockClarityScorer object at 0x0000019774010410>
text = 'Ce texte utilise notre customjargon spécifique.'

    def score_clarity(self, text: str) -> Dict[str, Any]:
        """
        Simule l'évaluation de la clarté du texte.
    
        Args:
            text: Le texte à évaluer.
    
        Returns:
            Un dictionnaire contenant le score de clarté simulé et les facteurs l'influençant.
        """
        if not isinstance(text, str) or not text.strip():
            logger.warning("MockClarityScorer.score_clarity a reçu une entrée non textuelle ou vide.")
            return {"error": "Entrée non textuelle ou vide", "clarity_score": 0.0, "factors": {}}
    
        logger.info("MockClarityScorer évalue le texte : %s...", text[:100])
    
        clarity_score: float = 1.0 # Score de base parfait
        factors: Dict[str, Any] = {penalty: 0 for penalty in self.clarity_penalties}
        text_lower = text.lower()
    
        words = re.findall(r'\b\w+\b', text_lower)
        num_words = len(words)
        if num_words == 0:
            return {"clarity_score": 0.0, "factors": factors, "interpretation": self._interpret_score(0.0)}
    
        sentences = [s.strip() for s in text.split('.') if s.strip()] # Simpliste
        num_sentences = len(sentences) if len(sentences) > 0 else 1
    
        # Longueur moyenne des phrases
        avg_sentence_length = num_words / num_sentences
        if avg_sentence_length > self.max_avg_sentence_length:
            clarity_score += self.clarity_penalties["long_sentences_avg"]
            factors["long_sentences_avg"] = round(avg_sentence_length,1)
    
        # Mots complexes (simplifié: mots > 9 lettres comme proxy pour >3 syllabes)
        complex_words = [w for w in words if len(w) > 9]
        complex_word_ratio = len(complex_words) / num_words
        if complex_word_ratio > self.max_complex_word_ratio:
>           clarity_score += self.clarity_penalties["complex_words_ratio"]
E           KeyError: 'complex_words_ratio'

argumentation_analysis\mocks\clarity_scoring.py:76: KeyError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,244 - tests.mocks.jpype_setup - INFO - Test test_score_clarity_custom_jargon: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,245 - tests.mocks.jpype_setup - INFO - Test test_score_clarity_custom_jargon demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.clarity_scoring] MockClarityScorer initialisé avec config: {'clarity_penalties': {'jargon_count': -0.5}, 'jargon_list': ['customjargon'], 'max_avg_sentence_length': 20}
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.clarity_scoring:clarity_scoring.py:34 MockClarityScorer initialisé avec config: {'clarity_penalties': {'jargon_count': -0.5}, 'jargon_list': ['customjargon'], 'max_avg_sentence_length': 20}
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.clarity_scoring] MockClarityScorer évalue le texte : Ce texte utilise notre customjargon spécifique....
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.clarity_scoring:clarity_scoring.py:52 MockClarityScorer évalue le texte : Ce texte utilise notre customjargon spécifique....
______________________ test_analyze_coherence_ideal_text ______________________

analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x0000019681396EA0>

    def test_analyze_coherence_ideal_text(analyzer_default: MockCoherenceAnalyzer):
        """Teste un texte idéalement cohérent."""
        # Beaucoup de mots de transition, répétition de mots-clés, pas de contradictions.
        # Score de base 0.5
        # Transition words: "donc", "ainsi", "de plus" (3) / ~20 mots. Ratio > 0.02. Score += 0.2
        # Repeated keywords: "cohérence" (x2), "texte" (x2). Count = 2. Score += 0.15
        # Pronoun referencing (texte > 50 mots): Score += 0.1
        # Total attendu = 0.5 + 0.2 + 0.15 + 0.1 = 0.95
        text = (
            "Ce texte est un exemple de cohérence. Donc, il suit une logique claire. "
            "De plus, les idées sont bien liées. Ainsi, la cohérence du texte est assurée. "
            "La structure du texte aide aussi."
        ) # 30 mots
        result = analyzer_default.analyze_coherence(text)
>       assert result["coherence_score"] == pytest.approx(0.5 + 0.2 + 0.15 + 0.1) # 0.95
E       assert 0.7 == 0.95 ± 9.5e-07
E         
E         comparison failed
E         Obtained: 0.7
E         Expected: 0.95 ± 9.5e-07

tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:72: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,278 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_ideal_text: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,278 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_ideal_text demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer initialisé avec config: {}
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:36 MockCoherenceAnalyzer initialisé avec config: {}
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer évalue le texte : Ce texte est un exemple de cohérence. Donc, il suit une logique claire. De plus, les idées sont bien...
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer score calculé: 0.70, facteurs: {'transition_words_ratio': 0.091, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 0, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:55 MockCoherenceAnalyzer évalue le texte : Ce texte est un exemple de cohérence. Donc, il suit une logique claire. De plus, les idées sont bien...
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:117 MockCoherenceAnalyzer score calculé: 0.70, facteurs: {'transition_words_ratio': 0.091, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 0, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
___________________ test_analyze_coherence_transition_words ___________________

analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x0000019681396D70>

    def test_analyze_coherence_transition_words(analyzer_default: MockCoherenceAnalyzer):
        """Teste l'impact des mots de transition."""
        # Score de base 0.5. Pronoms +0.1 (car > 50 mots). Total 0.6
        # 5 mots de transition / ~30 mots. Ratio > 0.02. Score += 0.2. Total = 0.8
        text_good = "Le premier point est important. Donc, nous devons le considérer. De plus, il y a un autre aspect. Par conséquent, la conclusion est évidente. Finalement, c'est clair."
        result_good = analyzer_default.analyze_coherence(text_good)
        assert result_good["factors"]["transition_words_ratio"] > 0
        assert result_good["coherence_score"] > 0.5 + 0.1 # Plus que base + pronoms
    
        text_bad = "Point un. Point deux. Point trois. Point quatre. Point cinq. Point six. Point sept." # Peu de transitions
        result_bad = analyzer_default.analyze_coherence(text_bad)
        assert result_bad["factors"]["transition_words_ratio"] == 0
        # Score = 0.5 (base) + 0.1 (pronoms, car > 50 mots) = 0.6
>       assert result_bad["coherence_score"] == pytest.approx(0.5 + 0.1)
E       assert 0.5 == 0.6 ± 6.0e-07
E         
E         comparison failed
E         Obtained: 0.5
E         Expected: 0.6 ± 6.0e-07

tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:91: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,293 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_transition_words: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,294 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_transition_words demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer initialisé avec config: {}
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:36 MockCoherenceAnalyzer initialisé avec config: {}
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer évalue le texte : Le premier point est important. Donc, nous devons le considérer. De plus, il y a un autre aspect. Pa...
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer score calculé: 0.70, facteurs: {'transition_words_ratio': 0.143, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 0, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer évalue le texte : Point un. Point deux. Point trois. Point quatre. Point cinq. Point six. Point sept....
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer score calculé: 0.50, facteurs: {'transition_words_ratio': 0, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 0, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:55 MockCoherenceAnalyzer évalue le texte : Le premier point est important. Donc, nous devons le considérer. De plus, il y a un autre aspect. Pa...
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:117 MockCoherenceAnalyzer score calculé: 0.70, facteurs: {'transition_words_ratio': 0.143, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 0, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:55 MockCoherenceAnalyzer évalue le texte : Point un. Point deux. Point trois. Point quatre. Point cinq. Point six. Point sept....
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:117 MockCoherenceAnalyzer score calculé: 0.50, facteurs: {'transition_words_ratio': 0, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 0, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
__________________ test_analyze_coherence_repeated_keywords ___________________

analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x00000197743DF1D0>

    def test_analyze_coherence_repeated_keywords(analyzer_default: MockCoherenceAnalyzer):
        """Teste l'impact de la répétition de mots-clés."""
        # Score de base 0.5. Pronoms +0.1. Total 0.6
        # "analyse" (x3), "pertinente" (x2). Count = 2. Score += 0.15. Total = 0.75
        text = "Cette analyse est une analyse très pertinente. L'analyse des données est pertinente."
        result = analyzer_default.analyze_coherence(text)
        assert result["factors"]["repeated_keywords_bonus"] >= 2
>       assert result["coherence_score"] == pytest.approx(0.5 + 0.15 + 0.1)
E       assert 0.65 == 0.75 ± 7.5e-07
E         
E         comparison failed
E         Obtained: 0.65
E         Expected: 0.75 ± 7.5e-07

tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:101: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,302 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_repeated_keywords: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,302 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_repeated_keywords demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer initialisé avec config: {}
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:36 MockCoherenceAnalyzer initialisé avec config: {}
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer évalue le texte : Cette analyse est une analyse très pertinente. L'analyse des données est pertinente....
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer score calculé: 0.65, facteurs: {'transition_words_ratio': 0, 'repeated_keywords_bonus': 2, 'contradiction_penalty': 0, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:55 MockCoherenceAnalyzer évalue le texte : Cette analyse est une analyse très pertinente. L'analyse des données est pertinente....
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:117 MockCoherenceAnalyzer score calculé: 0.65, facteurs: {'transition_words_ratio': 0, 'repeated_keywords_bonus': 2, 'contradiction_penalty': 0, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
____________________ test_analyze_coherence_contradictions ____________________

analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x00000196827678A0>

    def test_analyze_coherence_contradictions(analyzer_default: MockCoherenceAnalyzer):
        """Teste l'impact des contradictions."""
        # Score de base 0.5. Pronoms +0.1. Total 0.6
        # "j'aime" et "je n'aime pas". Contradiction x1. Score -= 0.4. Total = 0.2
        text = "J'aime le chocolat. Mais parfois, je n'aime pas le chocolat du tout."
        result = analyzer_default.analyze_coherence(text)
        assert result["factors"]["contradiction_penalty"] == 1
>       assert result["coherence_score"] == pytest.approx(0.5 - 0.4 + 0.1) # 0.2
E       assert 0.09999999999999998 == 0.19999999999999998 ± 2.0e-07
E         
E         comparison failed
E         Obtained: 0.09999999999999998
E         Expected: 0.19999999999999998 ± 2.0e-07

tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:111: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,308 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_contradictions: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,308 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_contradictions demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer initialisé avec config: {}
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:36 MockCoherenceAnalyzer initialisé avec config: {}
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer évalue le texte : J'aime le chocolat. Mais parfois, je n'aime pas le chocolat du tout....
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer score calculé: 0.10, facteurs: {'transition_words_ratio': 0, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 1, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:55 MockCoherenceAnalyzer évalue le texte : J'aime le chocolat. Mais parfois, je n'aime pas le chocolat du tout....
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:117 MockCoherenceAnalyzer score calculé: 0.10, facteurs: {'transition_words_ratio': 0, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 1, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
_________________ test_analyze_coherence_abrupt_topic_change __________________

analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x0000019682767680>

    def test_analyze_coherence_abrupt_topic_change(analyzer_default: MockCoherenceAnalyzer):
        """Teste l'impact d'un changement de sujet abrupt simulé."""
        # Score de base 0.5. Pronoms +0.1. Total 0.6
        # Changement abrupt. Score -= 0.2. Total = 0.4
        text = "Les pommes sont rouges et délicieuses. Les voitures vont vite."
        prev_summary = "Discussion sur les fruits et leurs couleurs."
        result = analyzer_default.analyze_coherence(text, prev_summary)
>       assert result["factors"]["abrupt_topic_change_penalty"] == 1
E       assert 0 == 1

tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:121: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,312 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_abrupt_topic_change: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,312 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_abrupt_topic_change demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer initialisé avec config: {}
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:36 MockCoherenceAnalyzer initialisé avec config: {}
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer évalue le texte : Les pommes sont rouges et délicieuses. Les voitures vont vite....
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer score calculé: 0.50, facteurs: {'transition_words_ratio': 0, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 0, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:55 MockCoherenceAnalyzer évalue le texte : Les pommes sont rouges et délicieuses. Les voitures vont vite....
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:117 MockCoherenceAnalyzer score calculé: 0.50, facteurs: {'transition_words_ratio': 0, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 0, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
____________ test_analyze_coherence_multiple_factors_and_clamping _____________

analyzer_default = <argumentation_analysis.mocks.coherence_analysis.MockCoherenceAnalyzer object at 0x00000196813E5750>

    def test_analyze_coherence_multiple_factors_and_clamping(analyzer_default: MockCoherenceAnalyzer):
        """Teste le cumul de facteurs et le clampage."""
        # Base 0.5. Pronoms +0.1.
        # Transitions: "donc", "cependant" (x2) / ~20 mots. Ratio > 0.02. Score += 0.2
        # Répétition: "test" (x2). Score += 0.15
        # Contradiction: "j'aime", "je n'aime pas". Score -= 0.4
        # Total = 0.5 + 0.1 + 0.2 + 0.15 - 0.4 = 0.55
        text = "J'aime ce test. Donc, c'est un bon test. Cependant, je n'aime pas toujours ce test."
        result = analyzer_default.analyze_coherence(text)
>       assert result["coherence_score"] == pytest.approx(0.55)
E       assert 0.29999999999999993 == 0.55 ± 5.5e-07
E         
E         comparison failed
E         Obtained: 0.29999999999999993
E         Expected: 0.55 ± 5.5e-07

tests\unit\argumentation_analysis\mocks\test_coherence_analysis.py:140: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,315 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_multiple_factors_and_clamping: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,315 - tests.mocks.jpype_setup - INFO - Test test_analyze_coherence_multiple_factors_and_clamping demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer initialisé avec config: {}
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:36 MockCoherenceAnalyzer initialisé avec config: {}
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer évalue le texte : J'aime ce test. Donc, c'est un bon test. Cependant, je n'aime pas toujours ce test....
20:10:21 [INFO] [argumentation_analysis.mocks.coherence_analysis] MockCoherenceAnalyzer score calculé: 0.30, facteurs: {'transition_words_ratio': 0.111, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 1, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:55 MockCoherenceAnalyzer évalue le texte : J'aime ce test. Donc, c'est un bon test. Cependant, je n'aime pas toujours ce test....
INFO     argumentation_analysis.mocks.coherence_analysis:coherence_analysis.py:117 MockCoherenceAnalyzer score calculé: 0.30, facteurs: {'transition_words_ratio': 0.111, 'repeated_keywords_bonus': 0, 'contradiction_penalty': 1, 'abrupt_topic_change_penalty': 0, 'consistent_pronoun_referencing': 0}
____________________ test_analyze_tone_single_emotion_joy _____________________

analyzer_default = <argumentation_analysis.mocks.emotional_tone_analysis.MockEmotionalToneAnalyzer object at 0x00000197743DE210>

    def test_analyze_tone_single_emotion_joy(analyzer_default: MockEmotionalToneAnalyzer):
        """Teste la détection de la joie."""
        text = "Je suis tellement heureux et content aujourd'hui, c'est une journée joyeuse !"
        # "heureux": 0.3+0.1=0.4
        # "content": 0.3+0.1=0.4 (cumulatif sur le count, puis min(1.0, 0.3*count+0.1))
        # "joyeux": 0.3+0.1=0.4
        # count = 3. score = min(1.0, 0.3*3 + 0.1) = min(1.0, 1.0) = 1.0
        result = analyzer_default.analyze_tone(text)
>       assert result["emotions_scores"]["Joie (Mock)"] == 1.0
E       assert 0.7 == 1.0

tests\unit\argumentation_analysis\mocks\test_emotional_tone_analysis.py:74: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,332 - tests.mocks.jpype_setup - INFO - Test test_analyze_tone_single_emotion_joy: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,332 - tests.mocks.jpype_setup - INFO - Test test_analyze_tone_single_emotion_joy demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.emotional_tone_analysis] MockEmotionalToneAnalyzer initialisé avec config: {} (keywords: 6, threshold: 0.60)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.emotional_tone_analysis:emotional_tone_analysis.py:30 MockEmotionalToneAnalyzer initialisé avec config: {} (keywords: 6, threshold: 0.60)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.emotional_tone_analysis] MockEmotionalToneAnalyzer analyse le texte : Je suis tellement heureux et content aujourd'hui, c'est une journée joyeuse !...
20:10:21 [INFO] [argumentation_analysis.mocks.emotional_tone_analysis] MockEmotionalToneAnalyzer a détecté les scores: {'Joie (Mock)': 0.7, 'Tristesse (Mock)': 0.0, 'Colère (Mock)': 0.0, 'Peur (Mock)': 0.0, 'Surprise (Mock)': 0.0, 'Dégoût (Mock)': 0.0}, dominant: Joie (Mock)
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.emotional_tone_analysis:emotional_tone_analysis.py:49 MockEmotionalToneAnalyzer analyse le texte : Je suis tellement heureux et content aujourd'hui, c'est une journée joyeuse !...
INFO     argumentation_analysis.mocks.emotional_tone_analysis:emotional_tone_analysis.py:93 MockEmotionalToneAnalyzer a détecté les scores: {'Joie (Mock)': 0.7, 'Tristesse (Mock)': 0.0, 'Colère (Mock)': 0.0, 'Peur (Mock)': 0.0, 'Surprise (Mock)': 0.0, 'Dégoût (Mock)': 0.0}, dominant: Joie (Mock)
___________ test_analyze_tone_single_emotion_sadness_strong_keyword ___________

analyzer_default = <argumentation_analysis.mocks.emotional_tone_analysis.MockEmotionalToneAnalyzer object at 0x00000197743988D0>

    def test_analyze_tone_single_emotion_sadness_strong_keyword(analyzer_default: MockEmotionalToneAnalyzer):
        """Teste la détection de la tristesse avec un mot-clé fort."""
        text = "Il se sentait complètement déprimé et abattu."
        # "déprimé": 0.3*1+0.1 = 0.4. Mot fort -> +0.2. Total = 0.6
        # "abattu": 0.3*1+0.1 = 0.4. (le score est par émotion, pas par mot-clé individuel avant cumul)
        # count = 2. score_base = min(1.0, 0.3*2+0.1) = 0.7. Mot fort "déprimé" présent -> +0.2. Total = 0.9
        result = analyzer_default.analyze_tone(text)
>       assert result["emotions_scores"]["Tristesse (Mock)"] == 0.9
E       assert 0.8999999999999999 == 0.9

tests\unit\argumentation_analysis\mocks\test_emotional_tone_analysis.py:91: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,338 - tests.mocks.jpype_setup - INFO - Test test_analyze_tone_single_emotion_sadness_strong_keyword: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,338 - tests.mocks.jpype_setup - INFO - Test test_analyze_tone_single_emotion_sadness_strong_keyword demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.emotional_tone_analysis] MockEmotionalToneAnalyzer initialisé avec config: {} (keywords: 6, threshold: 0.60)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.emotional_tone_analysis:emotional_tone_analysis.py:30 MockEmotionalToneAnalyzer initialisé avec config: {} (keywords: 6, threshold: 0.60)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.emotional_tone_analysis] MockEmotionalToneAnalyzer analyse le texte : Il se sentait complètement déprimé et abattu....
20:10:21 [INFO] [argumentation_analysis.mocks.emotional_tone_analysis] MockEmotionalToneAnalyzer a détecté les scores: {'Joie (Mock)': 0.0, 'Tristesse (Mock)': 0.8999999999999999, 'Colère (Mock)': 0.0, 'Peur (Mock)': 0.0, 'Surprise (Mock)': 0.0, 'Dégoût (Mock)': 0.0}, dominant: Tristesse (Mock)
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.emotional_tone_analysis:emotional_tone_analysis.py:49 MockEmotionalToneAnalyzer analyse le texte : Il se sentait complètement déprimé et abattu....
INFO     argumentation_analysis.mocks.emotional_tone_analysis:emotional_tone_analysis.py:93 MockEmotionalToneAnalyzer a détecté les scores: {'Joie (Mock)': 0.0, 'Tristesse (Mock)': 0.8999999999999999, 'Colère (Mock)': 0.0, 'Peur (Mock)': 0.0, 'Surprise (Mock)': 0.0, 'Dégoût (Mock)': 0.0}, dominant: Tristesse (Mock)
________ test_analyze_tone_mixed_emotions_one_dominant_above_threshold ________

analyzer_default = <argumentation_analysis.mocks.emotional_tone_analysis.MockEmotionalToneAnalyzer object at 0x00000197742DE950>

    def test_analyze_tone_mixed_emotions_one_dominant_above_threshold(analyzer_default: MockEmotionalToneAnalyzer):
        """Teste émotions mixtes avec une clairement dominante."""
        text = "Je suis très très heureux, ravi même ! Un peu triste aussi, mais surtout joyeux."
        # Joie: "heureux" (x2), "ravi", "joyeux". count = 4.
        # score_base = min(1.0, 0.3*4+0.1) = 1.0. "ravi" est fort -> +0.2. Total = 1.0 (capé)
        # Tristesse: "triste". count = 1. score = 0.3*1+0.1 = 0.4.
        result = analyzer_default.analyze_tone(text)
        assert result["emotions_scores"]["Joie (Mock)"] == 1.0
>       assert result["emotions_scores"]["Tristesse (Mock)"] == 0.4
E       assert 0.6000000000000001 == 0.4

tests\unit\argumentation_analysis\mocks\test_emotional_tone_analysis.py:118: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,351 - tests.mocks.jpype_setup - INFO - Test test_analyze_tone_mixed_emotions_one_dominant_above_threshold: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,351 - tests.mocks.jpype_setup - INFO - Test test_analyze_tone_mixed_emotions_one_dominant_above_threshold demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.emotional_tone_analysis] MockEmotionalToneAnalyzer initialisé avec config: {} (keywords: 6, threshold: 0.60)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.emotional_tone_analysis:emotional_tone_analysis.py:30 MockEmotionalToneAnalyzer initialisé avec config: {} (keywords: 6, threshold: 0.60)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.emotional_tone_analysis] MockEmotionalToneAnalyzer analyse le texte : Je suis très très heureux, ravi même ! Un peu triste aussi, mais surtout joyeux....
20:10:21 [INFO] [argumentation_analysis.mocks.emotional_tone_analysis] MockEmotionalToneAnalyzer a détecté les scores: {'Joie (Mock)': 1.0, 'Tristesse (Mock)': 0.6000000000000001, 'Colère (Mock)': 0.0, 'Peur (Mock)': 0.0, 'Surprise (Mock)': 0.0, 'Dégoût (Mock)': 0.0}, dominant: Joie (Mock)
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.emotional_tone_analysis:emotional_tone_analysis.py:49 MockEmotionalToneAnalyzer analyse le texte : Je suis très très heureux, ravi même ! Un peu triste aussi, mais surtout joyeux....
INFO     argumentation_analysis.mocks.emotional_tone_analysis:emotional_tone_analysis.py:93 MockEmotionalToneAnalyzer a détecté les scores: {'Joie (Mock)': 1.0, 'Tristesse (Mock)': 0.6000000000000001, 'Colère (Mock)': 0.0, 'Peur (Mock)': 0.0, 'Surprise (Mock)': 0.0, 'Dégoût (Mock)': 0.0}, dominant: Joie (Mock)
____________________ test_analyze_engagement_appel_action _____________________

analyzer_default = <argumentation_analysis.mocks.engagement_analysis.MockEngagementAnalyzer object at 0x0000019682766AD0>

    def test_analyze_engagement_appel_action(analyzer_default: MockEngagementAnalyzer):
        text = "Cliquez ici pour en savoir plus et rejoignez notre communauté !"
        # "cliquez", "rejoignez"
        # score += 0.3
        result = analyzer_default.analyze_engagement(text)
        assert result["signals_detected"]["appels_action"] == 2
        assert result["engagement_score"] >= 0.3
>       assert result["interpretation"] == "Engageant (Mock)" # 0.3
E       AssertionError: assert 'Peu engageant (Mock)' == 'Engageant (Mock)'
E         
E         - Engageant (Mock)
E         ? ^
E         + Peu engageant (Mock)
E         ? ^^^^^

tests\unit\argumentation_analysis\mocks\test_engagement_analysis.py:93: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,375 - tests.mocks.jpype_setup - INFO - Test test_analyze_engagement_appel_action: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,375 - tests.mocks.jpype_setup - INFO - Test test_analyze_engagement_appel_action demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.engagement_analysis] MockEngagementAnalyzer initialisé avec config: {} (signals: 6)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.engagement_analysis:engagement_analysis.py:34 MockEngagementAnalyzer initialisé avec config: {} (signals: 6)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.engagement_analysis] MockEngagementAnalyzer analyse le texte : Cliquez ici pour en savoir plus et rejoignez notre communauté !...
20:10:21 [INFO] [argumentation_analysis.mocks.engagement_analysis] MockEngagementAnalyzer score calculé: 0.30, signaux: {'questions_directes': 0, 'appels_action': 2, 'pronoms_inclusifs': 0, 'vocabulaire_positif_fort': 0, 'vocabulaire_negatif_fort': 0, 'longueur_texte_bonus': 0}
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.engagement_analysis:engagement_analysis.py:53 MockEngagementAnalyzer analyse le texte : Cliquez ici pour en savoir plus et rejoignez notre communauté !...
INFO     argumentation_analysis.mocks.engagement_analysis:engagement_analysis.py:102 MockEngagementAnalyzer score calculé: 0.30, signaux: {'questions_directes': 0, 'appels_action': 2, 'pronoms_inclusifs': 0, 'vocabulaire_positif_fort': 0, 'vocabulaire_negatif_fort': 0, 'longueur_texte_bonus': 0}
__________________ test_analyze_engagement_pronoms_inclusifs __________________

analyzer_default = <argumentation_analysis.mocks.engagement_analysis.MockEngagementAnalyzer object at 0x0000019682767F00>

    def test_analyze_engagement_pronoms_inclusifs(analyzer_default: MockEngagementAnalyzer):
        text = "Ensemble, nous pouvons faire la différence pour notre futur. Votre aide est précieuse."
        # "ensemble", "nous", "notre", "votre"
        # count = 4. score += 0.1 * min(4, 5) = 0.4
        result = analyzer_default.analyze_engagement(text)
>       assert result["signals_detected"]["pronoms_inclusifs"] >= 4
E       assert 0 >= 4

tests\unit\argumentation_analysis\mocks\test_engagement_analysis.py:100: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,384 - tests.mocks.jpype_setup - INFO - Test test_analyze_engagement_pronoms_inclusifs: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,384 - tests.mocks.jpype_setup - INFO - Test test_analyze_engagement_pronoms_inclusifs demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.engagement_analysis] MockEngagementAnalyzer initialisé avec config: {} (signals: 6)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.engagement_analysis:engagement_analysis.py:34 MockEngagementAnalyzer initialisé avec config: {} (signals: 6)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.engagement_analysis] MockEngagementAnalyzer analyse le texte : Ensemble, nous pouvons faire la différence pour notre futur. Votre aide est précieuse....
20:10:21 [INFO] [argumentation_analysis.mocks.engagement_analysis] MockEngagementAnalyzer score calculé: 0.00, signaux: {'questions_directes': 0, 'appels_action': 0, 'pronoms_inclusifs': 0, 'vocabulaire_positif_fort': 0, 'vocabulaire_negatif_fort': 0, 'longueur_texte_bonus': 0}
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.engagement_analysis:engagement_analysis.py:53 MockEngagementAnalyzer analyse le texte : Ensemble, nous pouvons faire la différence pour notre futur. Votre aide est précieuse....
INFO     argumentation_analysis.mocks.engagement_analysis:engagement_analysis.py:102 MockEngagementAnalyzer score calculé: 0.00, signaux: {'questions_directes': 0, 'appels_action': 0, 'pronoms_inclusifs': 0, 'vocabulaire_positif_fort': 0, 'vocabulaire_negatif_fort': 0, 'longueur_texte_bonus': 0}
______________ test_analyze_engagement_combination_and_clamping _______________

analyzer_default = <argumentation_analysis.mocks.engagement_analysis.MockEngagementAnalyzer object at 0x0000019774012D50>

    def test_analyze_engagement_combination_and_clamping(analyzer_default: MockEngagementAnalyzer):
        text = (
            "Qu'en pensez-vous ? Cliquez ici ! Ensemble, nous ferons des choses incroyables. "
            "C'est une opportunité révolutionnaire. Rejoignez-nous maintenant. Votre avis compte. "
            "N'est-ce pas merveilleux ? " + ("bla " * 30) # Pour bonus longueur
        )
        # Questions: "Qu'en pensez-vous ?", "N'est-ce pas ?" -> count=2. score_q = 0.2 * 2 = 0.4
        # Action: "Cliquez", "Rejoignez" -> count=2. score_a = 0.3
        # Inclusifs: "Ensemble", "nous", "Votre" -> count=3. score_i = 0.1 * 3 = 0.3
        # Positif: "incroyables", "révolutionnaire", "merveilleux" -> count=3. score_p = 0.15
        # Longueur: > 200 -> score_l = 0.05
        # Total = 0.4 + 0.3 + 0.3 + 0.15 + 0.05 = 1.2. Devrait être clampé à 1.0
        result = analyzer_default.analyze_engagement(text)
        assert result["engagement_score"] == 1.0
        assert result["interpretation"] == "Très engageant (Mock)"
        assert result["signals_detected"]["questions_directes"] >= 2
        assert result["signals_detected"]["appels_action"] >= 2
>       assert result["signals_detected"]["pronoms_inclusifs"] >= 3
E       assert 0 >= 3

tests\unit\argumentation_analysis\mocks\test_engagement_analysis.py:147: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,394 - tests.mocks.jpype_setup - INFO - Test test_analyze_engagement_combination_and_clamping: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,394 - tests.mocks.jpype_setup - INFO - Test test_analyze_engagement_combination_and_clamping demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.engagement_analysis] MockEngagementAnalyzer initialisé avec config: {} (signals: 6)
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.engagement_analysis:engagement_analysis.py:34 MockEngagementAnalyzer initialisé avec config: {} (signals: 6)
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.engagement_analysis] MockEngagementAnalyzer analyse le texte : Qu'en pensez-vous ? Cliquez ici ! Ensemble, nous ferons des choses incroyables. C'est une opportunit...
20:10:21 [INFO] [argumentation_analysis.mocks.engagement_analysis] MockEngagementAnalyzer score calculé: 1.00, signaux: {'questions_directes': 4, 'appels_action': 2, 'pronoms_inclusifs': 0, 'vocabulaire_positif_fort': 3, 'vocabulaire_negatif_fort': 0, 'longueur_texte_bonus': 1}
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.engagement_analysis:engagement_analysis.py:53 MockEngagementAnalyzer analyse le texte : Qu'en pensez-vous ? Cliquez ici ! Ensemble, nous ferons des choses incroyables. C'est une opportunit...
INFO     argumentation_analysis.mocks.engagement_analysis:engagement_analysis.py:102 MockEngagementAnalyzer score calculé: 1.00, signaux: {'questions_directes': 4, 'appels_action': 2, 'pronoms_inclusifs': 0, 'vocabulaire_positif_fort': 3, 'vocabulaire_negatif_fort': 0, 'longueur_texte_bonus': 1}
________ test_detect_evidence_by_keyword_text_too_short_after_keyword _________

detector_default = <argumentation_analysis.mocks.evidence_detection.MockEvidenceDetector object at 0x0000019682766140>

    def test_detect_evidence_by_keyword_text_too_short_after_keyword(detector_default: MockEvidenceDetector):
        """Teste mot-clé mais texte suivant trop court."""
        text = "Il a été prouvé que A." # "A." (len 2) < min_evidence_length 15
        result = detector_default.detect_evidence(text)
        assert result == [] # Ne devrait rien trouver car trop court, et pas de fallback global dans ce mock
    
        # Avec config custom (min_length 10)
        text_custom = "Preuve: B." # "B." (len 2) < min_evidence_length 10
>       result_custom = detector_custom_config.detect_evidence(text_custom)
E       AttributeError: 'function' object has no attribute 'detect_evidence'

tests\unit\argumentation_analysis\mocks\test_evidence_detection.py:92: AttributeError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,421 - tests.mocks.jpype_setup - INFO - Test test_detect_evidence_by_keyword_text_too_short_after_keyword: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,421 - tests.mocks.jpype_setup - INFO - Test test_detect_evidence_by_keyword_text_too_short_after_keyword demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.evidence_detection] MockEvidenceDetector initialisé avec config: {} (min_length: 15, keywords: ["selon l'étude", 'les données montrent que', 'par exemple', 'il a été prouvé que', 'comme en témoigne'])
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.evidence_detection:evidence_detection.py:22 MockEvidenceDetector initialisé avec config: {} (min_length: 15, keywords: ["selon l'étude", 'les données montrent que', 'par exemple', 'il a été prouvé que', 'comme en témoigne'])
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.evidence_detection] MockEvidenceDetector analyse le texte : Il a été prouvé que A....
20:10:21 [INFO] [argumentation_analysis.mocks.evidence_detection] MockEvidenceDetector a trouvé 0 éléments de preuve.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.evidence_detection:evidence_detection.py:43 MockEvidenceDetector analyse le texte : Il a été prouvé que A....
INFO     argumentation_analysis.mocks.evidence_detection:evidence_detection.py:107 MockEvidenceDetector a trouvé 0 éléments de preuve.
_____________________ test_detect_evidence_priority_order _____________________

detector_default = <argumentation_analysis.mocks.evidence_detection.MockEvidenceDetector object at 0x0000019681347770>

    def test_detect_evidence_priority_order(detector_default: MockEvidenceDetector):
        """Teste l'ordre de priorité: Mot-clé > Factuel > Citation."""
        # 1. Mot-clé a priorité
        text_keyword_and_factual = "Selon l'étude, 50% des cas sont résolus. Il y a aussi 20 chiffres."
        result1 = detector_default.detect_evidence(text_keyword_and_factual)
        assert len(result1) == 1
        assert result1[0]["type"] == "Preuve par Mot-Clé (Mock)"
>       assert result1[0]["evidence_text"] == "50% des cas sont résolus." # Texte après mot-clé
E       AssertionError: assert ', 50% des cas sont résolus.' == '50% des cas sont résolus.'
E         
E         - 50% des cas sont résolus.
E         + , 50% des cas sont résolus.
E         ? ++

tests\unit\argumentation_analysis\mocks\test_evidence_detection.py:164: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,435 - tests.mocks.jpype_setup - INFO - Test test_detect_evidence_priority_order: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,435 - tests.mocks.jpype_setup - INFO - Test test_detect_evidence_priority_order demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.evidence_detection] MockEvidenceDetector initialisé avec config: {} (min_length: 15, keywords: ["selon l'étude", 'les données montrent que', 'par exemple', 'il a été prouvé que', 'comme en témoigne'])
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.evidence_detection:evidence_detection.py:22 MockEvidenceDetector initialisé avec config: {} (min_length: 15, keywords: ["selon l'étude", 'les données montrent que', 'par exemple', 'il a été prouvé que', 'comme en témoigne'])
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.evidence_detection] MockEvidenceDetector analyse le texte : Selon l'étude, 50% des cas sont résolus. Il y a aussi 20 chiffres....
20:10:21 [INFO] [argumentation_analysis.mocks.evidence_detection] MockEvidenceDetector a trouvé 1 éléments de preuve.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.evidence_detection:evidence_detection.py:43 MockEvidenceDetector analyse le texte : Selon l'étude, 50% des cas sont résolus. Il y a aussi 20 chiffres....
INFO     argumentation_analysis.mocks.evidence_detection:evidence_detection.py:107 MockEvidenceDetector a trouvé 1 éléments de preuve.
___________________ test_detect_evidence_multiple_keywords ____________________

detector_default = <argumentation_analysis.mocks.evidence_detection.MockEvidenceDetector object at 0x0000019774518EF0>

    def test_detect_evidence_multiple_keywords(detector_default: MockEvidenceDetector):
        """Teste la détection de plusieurs mots-clés."""
        text = (
            "Selon l'étude, les faits sont têtus. "
            "Par exemple, les chiffres ne mentent pas."
        )
        result = detector_default.detect_evidence(text)
        assert len(result) == 2
        keywords_found = {ev["keyword_used"] for ev in result}
        assert "selon l'étude" in keywords_found
        assert "par exemple" in keywords_found
    
        evidence_texts = {ev["evidence_text"] for ev in result}
>       assert "les faits sont têtus." in evidence_texts
E       AssertionError: assert 'les faits sont têtus.' in {', les chiffres ne mentent pas.', ', les faits sont têtus.'}

tests\unit\argumentation_analysis\mocks\test_evidence_detection.py:193: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,439 - tests.mocks.jpype_setup - INFO - Test test_detect_evidence_multiple_keywords: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,439 - tests.mocks.jpype_setup - INFO - Test test_detect_evidence_multiple_keywords demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.evidence_detection] MockEvidenceDetector initialisé avec config: {} (min_length: 15, keywords: ["selon l'étude", 'les données montrent que', 'par exemple', 'il a été prouvé que', 'comme en témoigne'])
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.evidence_detection:evidence_detection.py:22 MockEvidenceDetector initialisé avec config: {} (min_length: 15, keywords: ["selon l'étude", 'les données montrent que', 'par exemple', 'il a été prouvé que', 'comme en témoigne'])
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.evidence_detection] MockEvidenceDetector analyse le texte : Selon l'étude, les faits sont têtus. Par exemple, les chiffres ne mentent pas....
20:10:21 [INFO] [argumentation_analysis.mocks.evidence_detection] MockEvidenceDetector a trouvé 2 éléments de preuve.
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.evidence_detection:evidence_detection.py:43 MockEvidenceDetector analyse le texte : Selon l'étude, les faits sont têtus. Par exemple, les chiffres ne mentent pas....
INFO     argumentation_analysis.mocks.evidence_detection:evidence_detection.py:107 MockEvidenceDetector a trouvé 2 éléments de preuve.
__________________________ test_analyze_ironic_tone ___________________________

analyzer = <argumentation_analysis.mocks.rhetorical_analysis.MockRhetoricalAnalyzer object at 0x0000019682767F00>

    def test_analyze_ironic_tone(analyzer: MockRhetoricalAnalyzer):
        """Teste la détection de 'tonalité ironique'."""
        text = "Oh, quelle merveilleuse journée pluvieuse, c'est une tonalité ironique."
        result = analyzer.analyze(text)
    
        # La tonalité ironique peut ajouter une figure de style par défaut si aucune autre n'est trouvée.
>       assert len(result["figures_de_style"]) >= 1
E       assert 0 >= 1
E        +  where 0 = len([])

tests\unit\argumentation_analysis\mocks\test_rhetorical_analysis.py:90: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,468 - tests.mocks.jpype_setup - INFO - Test test_analyze_ironic_tone: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,468 - tests.mocks.jpype_setup - INFO - Test test_analyze_ironic_tone demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.rhetorical_analysis] MockRhetoricalAnalyzer initialisé avec la config : {}
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.rhetorical_analysis:rhetorical_analysis.py:16 MockRhetoricalAnalyzer initialisé avec la config : {}
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.rhetorical_analysis] MockRhetoricalAnalyzer analyse le texte : Oh, quelle merveilleuse journée pluvieuse, c'est une tonalité ironique....
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.rhetorical_analysis:rhetorical_analysis.py:33 MockRhetoricalAnalyzer analyse le texte : Oh, quelle merveilleuse journée pluvieuse, c'est une tonalité ironique....
____________________ test_analyze_combination_all_keywords ____________________

analyzer = <argumentation_analysis.mocks.rhetorical_analysis.MockRhetoricalAnalyzer object at 0x00000196813E6550>

    def test_analyze_combination_all_keywords(analyzer: MockRhetoricalAnalyzer):
        """Teste la combinaison de tous les mots-clés."""
        text = "Un exemple de métaphore, une question rhétorique, et une tonalité ironique."
        result = analyzer.analyze(text)
    
        assert len(result["figures_de_style"]) == 2 # Métaphore et Question
        types = {f["type"] for f in result["figures_de_style"]}
        assert "Métaphore (Mock)" in types
        assert "Question Rhétorique (Mock)" in types
    
        # La tonalité ironique est la dernière vérifiée et devrait s'appliquer
        assert result["tonalite_globale"] == "Ironique (Mock)"
        # Métaphore: +0.3, Question: +0.4, Ironie: -0.2. Total = 0.5
>       assert result["score_engagement_simule"] == 0.5
E       assert 0.49999999999999994 == 0.5

tests\unit\argumentation_analysis\mocks\test_rhetorical_analysis.py:126: AssertionError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:21,473 - tests.mocks.jpype_setup - INFO - Test test_analyze_combination_all_keywords: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:21,473 - tests.mocks.jpype_setup - INFO - Test test_analyze_combination_all_keywords demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
20:10:21 [INFO] [argumentation_analysis.mocks.rhetorical_analysis] MockRhetoricalAnalyzer initialisé avec la config : {}
----------------------------- Captured log setup ------------------------------
INFO     argumentation_analysis.mocks.rhetorical_analysis:rhetorical_analysis.py:16 MockRhetoricalAnalyzer initialisé avec la config : {}
---------------------------- Captured stdout call -----------------------------
20:10:21 [INFO] [argumentation_analysis.mocks.rhetorical_analysis] MockRhetoricalAnalyzer analyse le texte : Un exemple de métaphore, une question rhétorique, et une tonalité ironique....
------------------------------ Captured log call ------------------------------
INFO     argumentation_analysis.mocks.rhetorical_analysis:rhetorical_analysis.py:33 MockRhetoricalAnalyzer analyse le texte : Un exemple de métaphore, une question rhétorique, et une tonalité ironique....
___________ test_generate_performance_visualizations_empty_metrics ____________

args = ()
keywargs = {'tmp_path': WindowsPath('C:/Users/jsboi/AppData/Local/Temp/pytest-of-jsboi/pytest-204/test_generate_performance_visu1')}

    @wraps(func)
    def patched(*args, **keywargs):
>       with self.decoration_helper(patched,
                                    args,
                                    keywargs) as (newargs, newkeywargs):

C:\Python313\Lib\unittest\mock.py:1421: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Python313\Lib\contextlib.py:141: in __enter__
    return next(self.gen)
C:\Python313\Lib\unittest\mock.py:1403: in decoration_helper
    arg = exit_stack.enter_context(patching)
C:\Python313\Lib\contextlib.py:530: in enter_context
    result = _enter(cm)
C:\Python313\Lib\unittest\mock.py:1479: in __enter__
    self.target = self.getter()
C:\Python313\Lib\pkgutil.py:513: in resolve_name
    mod = importlib.import_module(modname)
C:\Python313\Lib\importlib\__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    from __future__ import annotations
    
    
    # start delvewheel patch
    def _delvewheel_patch_1_8_2():
        import os
        libs_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, 'pandas.libs'))
        if os.path.isdir(libs_dir):
            os.add_dll_directory(libs_dir)
    
    
    _delvewheel_patch_1_8_2()
    del _delvewheel_patch_1_8_2
    # end delvewheel patch
    
    import os
    import warnings
    
    __docformat__ = "restructuredtext"
    
    # Let users know if they're missing any of our hard dependencies
    _hard_dependencies = ("numpy", "pytz", "dateutil")
    _missing_dependencies = []
    
    for _dependency in _hard_dependencies:
        try:
            __import__(_dependency)
        except ImportError as _e:  # pragma: no cover
            _missing_dependencies.append(f"{_dependency}: {_e}")
    
    if _missing_dependencies:  # pragma: no cover
>       raise ImportError(
            "Unable to import required dependencies:\n" + "\n".join(_missing_dependencies)
        )
E       ImportError: Unable to import required dependencies:
E       numpy: No module named '_core'

C:\Python313\Lib\site-packages\pandas\__init__.py:32: ImportError
---------------------------- Captured stdout setup ----------------------------
2025-06-05 20:10:24,591 - tests.mocks.jpype_setup - INFO - Test test_generate_performance_visualizations_empty_metrics: REAL JPype forcé par la variable d'environnement USE_REAL_JPYPE.
2025-06-05 20:10:24,591 - tests.mocks.jpype_setup - INFO - Test test_generate_performance_visualizations_empty_metrics demande REAL JPype. Configuration de sys.modules pour utiliser le vrai JPype.
------------------------------ Captured log call ------------------------------
INFO     NumpyMock:legacy_numpy_array_mock.py:1282 NumpyMock: numpy.core._multiarray_umath.__spec__ accédé (retourne MagicMock).
INFO     NumpyMock:legacy_numpy_array_mock.py:1282 NumpyMock: numpy.core._multiarray_umath.__cpu_features__ accédé (retourne MagicMock).
INFO     NumpyMock:legacy_numpy_array_mock.py:1282 NumpyMock: numpy.core._multiarray_umath.__cpu_baseline__ accédé (retourne MagicMock).
INFO     NumpyMock:legacy_numpy_array_mock.py:1282 NumpyMock: numpy.core._multiarray_umath.__cpu_dispatch__ accédé (retourne MagicMock).
INFO     NumpyMock:legacy_numpy_array_mock.py:1282 NumpyMock: numpy.core._multiarray_umath.__cpu_features__ accédé (retourne MagicMock).
INFO     NumpyMock:legacy_numpy_array_mock.py:1282 NumpyMock: numpy.core._multiarray_umath.__cpu_baseline__ accédé (retourne MagicMock).
INFO     NumpyMock:legacy_numpy_array_mock.py:1282 NumpyMock: numpy.core._multiarray_umath.__cpu_dispatch__ accédé (retourne MagicMock).
============================== warnings summary ===============================
..\..\Users\jsboi\AppData\Roaming\Python\Python313\site-packages\pydantic\_internal\_config.py:291
  C:\Users\jsboi\AppData\Roaming\Python\Python313\site-packages\pydantic\_internal\_config.py:291: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/
    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)

..\..\Users\jsboi\AppData\Roaming\Python\Python313\site-packages\pydantic\_internal\_config.py:341
  C:\Users\jsboi\AppData\Roaming\Python\Python313\site-packages\pydantic\_internal\_config.py:341: UserWarning: Valid config keys have changed in V2:
  * 'allow_population_by_field_name' has been renamed to 'populate_by_name'
    warnings.warn(message, UserWarning)

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_argument
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_argument' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_argument
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAgent.test_analyze_argument of <tests.agents.core.informal.test_informal_agent.TestInformalAgent testMethod=test_analyze_argument>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_context
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_context' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_context
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAgent.test_analyze_context of <tests.agents.core.informal.test_informal_agent.TestInformalAgent testMethod=test_analyze_context>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_context_without_analyzer
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_context_without_analyzer' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_context_without_analyzer
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAgent.test_analyze_context_without_analyzer of <tests.agents.core.informal.test_informal_agent.TestInformalAgent testMethod=test_analyze_context_without_analyzer>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_fallacies
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_fallacies' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_fallacies
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAgent.test_analyze_fallacies of <tests.agents.core.informal.test_informal_agent.TestInformalAgent testMethod=test_analyze_fallacies>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_rhetoric
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_rhetoric' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_rhetoric
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAgent.test_analyze_rhetoric of <tests.agents.core.informal.test_informal_agent.TestInformalAgent testMethod=test_analyze_rhetoric>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_rhetoric_without_analyzer
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_rhetoric_without_analyzer' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_rhetoric_without_analyzer
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAgent.test_analyze_rhetoric_without_analyzer of <tests.agents.core.informal.test_informal_agent.TestInformalAgent testMethod=test_analyze_rhetoric_without_analyzer>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_text_with_semantic_kernel
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_text_with_semantic_kernel' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_text_with_semantic_kernel
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAgent.test_analyze_text_with_semantic_kernel of <tests.agents.core.informal.test_informal_agent.TestInformalAgent testMethod=test_analyze_text_with_semantic_kernel>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_text_without_semantic_kernel
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAgent.test_analyze_text_without_semantic_kernel' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_analyze_text_without_semantic_kernel
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAgent.test_analyze_text_without_semantic_kernel of <tests.agents.core.informal.test_informal_agent.TestInformalAgent testMethod=test_analyze_text_without_semantic_kernel>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_identify_arguments_without_kernel
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAgent.test_identify_arguments_without_kernel' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_agent.py::TestInformalAgent::test_identify_arguments_without_kernel
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAgent.test_identify_arguments_without_kernel of <tests.agents.core.informal.test_informal_agent.TestInformalAgent testMethod=test_identify_arguments_without_kernel>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_analysis_methods.py::TestInformalAnalysisMethods::test_analyze_text
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAnalysisMethods.test_analyze_text' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_analysis_methods.py::TestInformalAnalysisMethods::test_analyze_text
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAnalysisMethods.test_analyze_text of <tests.agents.core.informal.test_informal_analysis_methods.TestInformalAnalysisMethods testMethod=test_analyze_text>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_analysis_methods.py::TestInformalAnalysisMethods::test_analyze_text_with_confidence_threshold
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAnalysisMethods.test_analyze_text_with_confidence_threshold' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_analysis_methods.py::TestInformalAnalysisMethods::test_analyze_text_with_confidence_threshold
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAnalysisMethods.test_analyze_text_with_confidence_threshold of <tests.agents.core.informal.test_informal_analysis_methods.TestInformalAnalysisMethods testMethod=test_analyze_text_with_confidence_threshold>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_analysis_methods.py::TestInformalAnalysisMethods::test_analyze_text_with_context
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalAnalysisMethods.test_analyze_text_with_context' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_analysis_methods.py::TestInformalAnalysisMethods::test_analyze_text_with_context
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalAnalysisMethods.test_analyze_text_with_context of <tests.agents.core.informal.test_informal_analysis_methods.TestInformalAnalysisMethods testMethod=test_analyze_text_with_context>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_error_handling.py::TestInformalErrorHandling::test_handle_empty_text
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalErrorHandling.test_handle_empty_text' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_error_handling.py::TestInformalErrorHandling::test_handle_empty_text
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalErrorHandling.test_handle_empty_text of <tests.agents.core.informal.test_informal_error_handling.TestInformalErrorHandling testMethod=test_handle_empty_text>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_error_handling.py::TestInformalErrorHandling::test_handle_fallacy_detector_exception
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalErrorHandling.test_handle_fallacy_detector_exception' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_error_handling.py::TestInformalErrorHandling::test_handle_fallacy_detector_exception
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalErrorHandling.test_handle_fallacy_detector_exception of <tests.agents.core.informal.test_informal_error_handling.TestInformalErrorHandling testMethod=test_handle_fallacy_detector_exception>>)
    return self.run(*args, **kwds)

tests/agents/core/informal/test_informal_error_handling.py::TestInformalErrorHandling::test_handle_none_text
  C:\Python313\Lib\unittest\case.py:606: RuntimeWarning: coroutine 'TestInformalErrorHandling.test_handle_none_text' was never awaited
    if method() is not None:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/agents/core/informal/test_informal_error_handling.py::TestInformalErrorHandling::test_handle_none_text
  C:\Python313\Lib\unittest\case.py:707: DeprecationWarning: It is deprecated to return a value that is not None from a test case (<bound method TestInformalErrorHandling.test_handle_none_text of <tests.agents.core.informal.test_informal_error_handling.TestInformalErrorHandling testMethod=test_handle_none_text>>)
    return self.run(*args, **kwds)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_execute_query_error_tweety
FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_execute_query_invalid_formula
FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_execute_query_rejected
FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_initialization_and_setup
FAILED tests/agents/core/logic/test_propositional_logic_agent.py::TestPropositionalLogicAgent::test_validate_formula_invalid
FAILED tests/argumentation_analysis/utils/dev_tools/test_repair_utils.py::test_run_extract_repair_pipeline_with_save_and_json_export
FAILED tests/argumentation_analysis/utils/dev_tools/test_repair_utils.py::test_run_extract_repair_pipeline_hitler_only_filter
FAILED tests/argumentation_analysis/utils/dev_tools/test_repair_utils.py::test_setup_agents_successful
FAILED tests/environment_checks/test_core_dependencies.py::test_parametrized_dependency_import[nltk]
FAILED tests/environment_checks/test_core_dependencies.py::test_parametrized_dependency_import[spacy]
FAILED tests/ui/test_extract_definition_persistence.py::test_load_definitions_encrypted_wrong_key
FAILED tests/ui/test_extract_definition_persistence.py::test_load_malformed_json
FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_mine_arguments_explicit_premise_conclusion_too_short2025-06-05 20:10:35,327 - tests.mocks.jpype_setup - INFO - jpype_setup.py: pytest_sessionfinish hook triggered. Exit status: 1
2025-06-05 20:10:35,327 - tests.mocks.jpype_setup - INFO -    pytest_sessionfinish: sys.modules['jpype'] IS _REAL_JPYPE_MODULE. Le vrai JPype a potentiellement Ã©tÃ© utilisÃ©.
2025-06-05 20:10:35,327 - tests.mocks.jpype_setup - INFO -    pytest_sessionfinish: Tentative d'arrÃªt de la JVM via shutdown_jvm_if_needed() car le vrai JPype a potentiellement Ã©tÃ© utilisÃ©.
2025-06-05 20:10:35,360 - tests.mocks.jpype_setup - INFO -    pytest_sessionfinish: VÃ©rification de l'Ã©tat de la JVM aprÃ¨s tentative d'arrÃªt.
2025-06-05 20:10:35,360 - tests.mocks.jpype_setup - INFO -    pytest_sessionfinish: JVM encore dÃ©marrÃ©e aprÃ¨s tentative d'arrÃªt: False
2025-06-05 20:10:35,361 - tests.mocks.jpype_setup - INFO -    pytest_sessionfinish: destroy_jvm est False (selon config): False
2025-06-05 20:10:35,361 - tests.mocks.jpype_setup - INFO -    pytest_sessionfinish: JVM non dÃ©marrÃ©e ou destroy_jvm est True. Pas de gestion spÃ©ciale de sys.modules pour atexit depuis ici.

FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_mine_arguments_implicit_par_consequent
FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_mine_arguments_implicit_ainsi
FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_mine_arguments_explicit_over_implicit
FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_mine_arguments_complex_scenario_mixed
FAILED tests/unit/argumentation_analysis/mocks/test_argument_mining.py::test_conclusion_delimitation_explicit
FAILED tests/unit/argumentation_analysis/mocks/test_bias_detection.py::test_detect_bias_confirmation
FAILED tests/unit/argumentation_analysis/mocks/test_claim_mining.py::test_extract_claims_global_claim_success
FAILED tests/unit/argumentation_analysis/mocks/test_claim_mining.py::test_extract_claims_global_claim_too_short
FAILED tests/unit/argumentation_analysis/mocks/test_claim_mining.py::test_extract_claims_by_keyword_text_too_short_after_keyword
FAILED tests/unit/argumentation_analysis/mocks/test_clarity_scoring.py::test_score_clarity_long_sentences
FAILED tests/unit/argumentation_analysis/mocks/test_clarity_scoring.py::test_score_clarity_ambiguity
FAILED tests/unit/argumentation_analysis/mocks/test_clarity_scoring.py::test_score_clarity_custom_jargon
FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_ideal_text
FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_transition_words
FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_repeated_keywords
FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_contradictions
FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_abrupt_topic_change
FAILED tests/unit/argumentation_analysis/mocks/test_coherence_analysis.py::test_analyze_coherence_multiple_factors_and_clamping
FAILED tests/unit/argumentation_analysis/mocks/test_emotional_tone_analysis.py::test_analyze_tone_single_emotion_joy
FAILED tests/unit/argumentation_analysis/mocks/test_emotional_tone_analysis.py::test_analyze_tone_single_emotion_sadness_strong_keyword
FAILED tests/unit/argumentation_analysis/mocks/test_emotional_tone_analysis.py::test_analyze_tone_mixed_emotions_one_dominant_above_threshold
FAILED tests/unit/argumentation_analysis/mocks/test_engagement_analysis.py::test_analyze_engagement_appel_action
FAILED tests/unit/argumentation_analysis/mocks/test_engagement_analysis.py::test_analyze_engagement_pronoms_inclusifs
FAILED tests/unit/argumentation_analysis/mocks/test_engagement_analysis.py::test_analyze_engagement_combination_and_clamping
FAILED tests/unit/argumentation_analysis/mocks/test_evidence_detection.py::test_detect_evidence_by_keyword_text_too_short_after_keyword
FAILED tests/unit/argumentation_analysis/mocks/test_evidence_detection.py::test_detect_evidence_priority_order
FAILED tests/unit/argumentation_analysis/mocks/test_evidence_detection.py::test_detect_evidence_multiple_keywords
FAILED tests/unit/argumentation_analysis/mocks/test_rhetorical_analysis.py::test_analyze_ironic_tone
FAILED tests/unit/argumentation_analysis/mocks/test_rhetorical_analysis.py::test_analyze_combination_all_keywords
FAILED tests/unit/argumentation_analysis/utils/test_visualization_generator.py::test_generate_performance_visualizations_empty_metrics
===== 43 failed, 944 passed, 18 skipped, 32 warnings in 80.93s (0:01:20) ======
Exception ignored in atexit callback <function cleanup_numbered_dir at 0x00000196C9EA8720>:
Traceback (most recent call last):
  File "C:\Users\jsboi\AppData\Roaming\Python\Python313\site-packages\_pytest\pathlib.py", line 374, in cleanup_numbered_dir
    cleanup_dead_symlinks(root)
  File "C:\Users\jsboi\AppData\Roaming\Python\Python313\site-packages\_pytest\pathlib.py", line 359, in cleanup_dead_symlinks
    if not left_dir.resolve().exists():
  File "C:\Python313\Lib\pathlib\_abc.py", line 450, in exists
    self.stat(follow_symlinks=follow_symlinks)
  File "C:\Python313\Lib\pathlib\_local.py", line 515, in stat
    return os.stat(self, follow_symlinks=follow_symlinks)
PermissionError: [WinError 5] Accès refusé: 'C:\\Users\\jsboi\\AppData\\Local\\Temp\\pytest-of-jsboi\\pytest-current'
