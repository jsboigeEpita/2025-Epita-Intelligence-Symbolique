#!/usr/bin/env python3
"""
GÃ‰NÃ‰RATEUR DE TRACES COMPLÃˆTES AUTOMATIQUE
==========================================

GÃ©nÃ¨re de vraies traces d'interactions agentielles avec outils
et synthÃ©tise automatiquement les rÃ©sultats.
"""

import asyncio
import json
import time
import os
import uuid
import hashlib
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
import logging
from pathlib import Path
from dotenv import load_dotenv
import semantic_kernel as sk
from semantic_kernel.functions.kernel_arguments import KernelArguments
from openai import AsyncOpenAI

# Charger les variables d'environnement
load_dotenv()

# Configuration logging avec plus de dÃ©tails
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - [%(levelname)s] - [%(name)s] - %(message)s'
)
logger = logging.getLogger(__name__)

class TraceurComplet:
    """Traceur complet d'interactions agentielles."""
    
    def __init__(self, trace_id: str = None):
        self.trace_id = trace_id or str(uuid.uuid4())[:8]
        self.session_start = datetime.now()
        self.traces = {
            "session_info": {
                "trace_id": self.trace_id,
                "start_time": self.session_start.isoformat(),
                "python_version": "3.12",
                "semantic_kernel_version": sk.__version__
            },
            "api_calls": [],
            "agent_interactions": [],
            "tool_calls": [],
            "orchestrator_traces": [],
            "performance_metrics": {},
            "authenticity_evidence": []
        }
        
    def trace_api_call(self, call_type: str, model: str, prompt: str, response: str, 
                      latency_ms: float, tokens: int, metadata: Dict = None):
        """Trace un appel API complet."""
        call_trace = {
            "timestamp": datetime.now().isoformat(),
            "call_id": str(uuid.uuid4())[:8],
            "type": call_type,
            "model": model,
            "prompt_hash": hashlib.md5(prompt.encode()).hexdigest()[:16],
            "prompt_preview": prompt[:200] + "..." if len(prompt) > 200 else prompt,
            "response_hash": hashlib.md5(response.encode()).hexdigest()[:16],
            "response_preview": response[:200] + "..." if len(response) > 200 else response,
            "response_length": len(response),
            "latency_ms": latency_ms,
            "tokens_used": tokens,
            "metadata": metadata or {}
        }
        self.traces["api_calls"].append(call_trace)
        return call_trace
        
    def trace_agent_interaction(self, agent_name: str, message: str, response: str, 
                               processing_time: float, metadata: Dict = None):
        """Trace une interaction d'agent."""
        interaction_trace = {
            "timestamp": datetime.now().isoformat(),
            "interaction_id": str(uuid.uuid4())[:8],
            "agent_name": agent_name,
            "message_hash": hashlib.md5(message.encode()).hexdigest()[:16],
            "message_preview": message[:150] + "..." if len(message) > 150 else message,
            "response_hash": hashlib.md5(response.encode()).hexdigest()[:16],
            "response_preview": response[:150] + "..." if len(response) > 150 else response,
            "response_length": len(response),
            "processing_time_ms": processing_time * 1000,
            "metadata": metadata or {}
        }
        self.traces["agent_interactions"].append(interaction_trace)
        return interaction_trace
        
    def trace_tool_call(self, tool_name: str, arguments: Dict, result: Any, 
                       execution_time: float, success: bool):
        """Trace un appel d'outil."""
        tool_trace = {
            "timestamp": datetime.now().isoformat(),
            "tool_call_id": str(uuid.uuid4())[:8],
            "tool_name": tool_name,
            "arguments": arguments,
            "result_type": type(result).__name__,
            "result_preview": str(result)[:200] + "..." if len(str(result)) > 200 else str(result),
            "execution_time_ms": execution_time * 1000,
            "success": success
        }
        self.traces["tool_calls"].append(tool_trace)
        return tool_trace
        
    def trace_orchestrator_step(self, step_name: str, agents_involved: List[str], 
                               input_data: Dict, output_data: Dict, step_duration: float):
        """Trace une Ã©tape d'orchestration."""
        orchestrator_trace = {
            "timestamp": datetime.now().isoformat(),
            "step_id": str(uuid.uuid4())[:8],
            "step_name": step_name,
            "agents_involved": agents_involved,
            "input_hash": hashlib.md5(json.dumps(input_data, sort_keys=True).encode()).hexdigest()[:16],
            "output_hash": hashlib.md5(json.dumps(output_data, sort_keys=True).encode()).hexdigest()[:16],
            "step_duration_ms": step_duration * 1000,
            "data_preview": {
                "input_keys": list(input_data.keys()),
                "output_keys": list(output_data.keys()),
                "input_size": len(str(input_data)),
                "output_size": len(str(output_data))
            }
        }
        self.traces["orchestrator_traces"].append(orchestrator_trace)
        return orchestrator_trace
        
    def add_authenticity_evidence(self, evidence_type: str, description: str, 
                                 proof_data: Dict, confidence_score: float):
        """Ajoute une preuve d'authenticitÃ©."""
        evidence = {
            "timestamp": datetime.now().isoformat(),
            "evidence_id": str(uuid.uuid4())[:8],
            "type": evidence_type,
            "description": description,
            "proof_data": proof_data,
            "confidence_score": confidence_score
        }
        self.traces["authenticity_evidence"].append(evidence)
        return evidence
        
    def finalize_session(self):
        """Finalise la session et calcule les mÃ©triques."""
        session_end = datetime.now()
        session_duration = (session_end - self.session_start).total_seconds()
        
        # Calcul des mÃ©triques de performance
        self.traces["performance_metrics"] = {
            "session_duration_seconds": session_duration,
            "total_api_calls": len(self.traces["api_calls"]),
            "total_agent_interactions": len(self.traces["agent_interactions"]),
            "total_tool_calls": len(self.traces["tool_calls"]),
            "total_orchestrator_steps": len(self.traces["orchestrator_traces"]),
            "avg_api_latency_ms": sum(call["latency_ms"] for call in self.traces["api_calls"]) / max(1, len(self.traces["api_calls"])),
            "total_tokens_used": sum(call["tokens_used"] for call in self.traces["api_calls"]),
            "session_end_time": session_end.isoformat()
        }
        
        return self.traces

class TesteurTracesCompletes:
    """Testeur avec traces complÃ¨tes."""
    
    def __init__(self, dataset_name: str = None):
        self.dataset_name = dataset_name or f"dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.traceur = TraceurComplet()
        self.client = None
        
    async def setup_openai_client(self):
        """Configure le client OpenAI."""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key or "test" in api_key.lower() or "fake" in api_key.lower():
            raise ValueError("ClÃ© API OpenAI invalide ou factice dÃ©tectÃ©e")
        self.client = AsyncOpenAI(api_key=api_key)
        
    async def test_appel_api_direct(self, prompt: str) -> Dict:
        """Test d'appel API direct avec traÃ§age complet."""
        logger.info(f"[TRACE] Test appel API direct - prompt: {prompt[:50]}...")
        
        start_time = time.time()
        
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=800
            )
            
            latency = (time.time() - start_time) * 1000
            response_text = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            
            # Tracer l'appel API
            api_trace = self.traceur.trace_api_call(
                call_type="direct_openai",
                model="gpt-4o-mini",
                prompt=prompt,
                response=response_text,
                latency_ms=latency,
                tokens=tokens_used,
                metadata={"usage": dict(response.usage)}
            )
            
            # Preuves d'authenticitÃ©
            self.traceur.add_authenticity_evidence(
                evidence_type="api_call_direct",
                description="Appel API OpenAI direct avec vraie latence et tokens",
                proof_data={
                    "latency_ms": latency,
                    "tokens_used": tokens_used,
                    "model": "gpt-4o-mini",
                    "response_length": len(response_text)
                },
                confidence_score=0.95
            )
            
            return {
                "success": True,
                "api_trace": api_trace,
                "response": response_text,
                "latency_ms": latency,
                "tokens": tokens_used
            }
            
        except Exception as e:
            logger.error(f"[TRACE] Erreur appel API: {e}")
            return {"success": False, "error": str(e)}
            
    async def test_orchestrateur_avec_traces(self, scenario: Dict) -> Dict:
        """Test orchestrateur avec traÃ§age complet des interactions."""
        logger.info(f"[TRACE] Test orchestrateur - scenario: {scenario.get('name', 'unnamed')}")
        
        try:
            # Import des modules d'orchestration
            from argumentation_analysis.orchestration.cluedo_orchestrator import run_cluedo_game
            from argumentation_analysis.utils.semantic_kernel_compatibility import create_kernel_with_chat_completion
            
            # Tracer l'Ã©tape de setup
            setup_start = time.time()
            kernel = create_kernel_with_chat_completion(
                api_key=os.getenv("OPENAI_API_KEY"),
                model="gpt-4o-mini",
                service_id="trace_test"
            )
            setup_time = time.time() - setup_start
            
            self.traceur.trace_orchestrator_step(
                step_name="kernel_setup",
                agents_involved=["system"],
                input_data={"model": "gpt-4o-mini", "service_id": "trace_test"},
                output_data={"kernel_ready": True},
                step_duration=setup_time
            )
            
            # Tracer l'exÃ©cution de l'orchestrateur
            orchestrator_start = time.time()
            
            history = await run_cluedo_game(
                kernel=kernel,
                initial_question=scenario["question"],
                max_iterations=2
            )
            
            orchestrator_time = time.time() - orchestrator_start
            
            # Analyser les messages pour tracer les interactions
            agent_interactions = []
            for i, message in enumerate(history):
                if hasattr(message, 'role') and hasattr(message, 'content'):
                    interaction = {
                        "turn": i,
                        "agent": message.role,
                        "content_length": len(message.content or ""),
                        "content_preview": (message.content or "")[:100]
                    }
                    agent_interactions.append(interaction)
                    
                    # Tracer chaque interaction d'agent
                    self.traceur.trace_agent_interaction(
                        agent_name=message.role,
                        message=f"Turn {i}",
                        response=message.content or "",
                        processing_time=orchestrator_time / max(1, len(history)),
                        metadata={"turn_number": i, "total_turns": len(history)}
                    )
            
            # Tracer l'Ã©tape complÃ¨te d'orchestration
            self.traceur.trace_orchestrator_step(
                step_name="cluedo_game_execution",
                agents_involved=["Sherlock", "Watson"],
                input_data=scenario,
                output_data={
                    "conversation_turns": len(history),
                    "total_words": sum(len((msg.content or "").split()) for msg in history),
                    "execution_time": orchestrator_time
                },
                step_duration=orchestrator_time
            )
            
            # Preuves d'authenticitÃ© de l'orchestrateur
            self.traceur.add_authenticity_evidence(
                evidence_type="orchestrator_execution",
                description="Orchestrateur avec agents rÃ©els et interactions complÃ¨tes",
                proof_data={
                    "execution_time_seconds": orchestrator_time,
                    "conversation_turns": len(history),
                    "agents_involved": list(set(msg.role for msg in history if hasattr(msg, 'role'))),
                    "total_interactions": len(agent_interactions)
                },
                confidence_score=0.90
            )
            
            return {
                "success": True,
                "execution_time": orchestrator_time,
                "conversation_turns": len(history),
                "agent_interactions": agent_interactions,
                "history": [{"role": msg.role, "content": msg.content} for msg in history if hasattr(msg, 'role')]
            }
            
        except Exception as e:
            logger.error(f"[TRACE] Erreur orchestrateur: {e}")
            return {"success": False, "error": str(e)}

class SynthetiseurAutomatique:
    """SynthÃ©tiseur automatique de traces."""
    
    @staticmethod
    def generer_synthese_complete(traces: Dict) -> Dict:
        """GÃ©nÃ¨re une synthÃ¨se complÃ¨te automatique des traces."""
        synthese = {
            "resume_executif": {},
            "metriques_techniques": {},
            "preuves_authenticite": {},
            "analyse_performance": {},
            "recommandations": []
        }
        
        # RÃ©sumÃ© exÃ©cutif
        synthese["resume_executif"] = {
            "session_id": traces["session_info"]["trace_id"],
            "duree_totale_secondes": traces["performance_metrics"]["session_duration_seconds"],
            "appels_api_reussis": len([call for call in traces["api_calls"] if "error" not in call]),
            "interactions_agents": traces["performance_metrics"]["total_agent_interactions"],
            "etapes_orchestration": traces["performance_metrics"]["total_orchestrator_steps"],
            "score_authenticite": len(traces["authenticity_evidence"])
        }
        
        # MÃ©triques techniques
        synthese["metriques_techniques"] = {
            "latence_api_moyenne_ms": traces["performance_metrics"]["avg_api_latency_ms"],
            "tokens_totaux_consommes": traces["performance_metrics"]["total_tokens_used"],
            "temps_execution_orchestrateur": max([step["step_duration_ms"] for step in traces["orchestrator_traces"]], default=0),
            "nombre_outils_utilises": len(set(tool["tool_name"] for tool in traces["tool_calls"])),
            "taux_succes_global": (
                len([call for call in traces["api_calls"] if "error" not in call]) / 
                max(1, len(traces["api_calls"]))
            ) * 100
        }
        
        # Preuves d'authenticitÃ©
        synthese["preuves_authenticite"] = {
            "score_confiance_moyen": sum(evidence["confidence_score"] for evidence in traces["authenticity_evidence"]) / max(1, len(traces["authenticity_evidence"])),
            "types_preuves": list(set(evidence["type"] for evidence in traces["authenticity_evidence"])),
            "preuves_critiques": [
                evidence for evidence in traces["authenticity_evidence"]
                if evidence["confidence_score"] > 0.8
            ],
            "indicateurs_mocks": SynthetiseurAutomatique._detecter_indicateurs_mocks(traces)
        }
        
        # Analyse performance
        synthese["analyse_performance"] = {
            "seuils_respectes": {
                "latence_api_realiste": traces["performance_metrics"]["avg_api_latency_ms"] > 1000,
                "execution_orchestrateur_realiste": max([step["step_duration_ms"] for step in traces["orchestrator_traces"]], default=0) > 5000,
                "tokens_consommes_realistes": traces["performance_metrics"]["total_tokens_used"] > 100
            },
            "comparaison_attendu": {
                "latence_vs_attendu": traces["performance_metrics"]["avg_api_latency_ms"] / 3000,  # 3s attendu
                "tokens_vs_attendu": traces["performance_metrics"]["total_tokens_used"] / 500      # 500 tokens attendus
            }
        }
        
        # Recommandations automatiques
        synthese["recommandations"] = SynthetiseurAutomatique._generer_recommandations(traces, synthese)
        
        return synthese
    
    @staticmethod
    def _detecter_indicateurs_mocks(traces: Dict) -> List[str]:
        """DÃ©tecte automatiquement les indicateurs de mocks."""
        indicateurs = []
        
        # Latences trop rapides
        avg_latency = traces["performance_metrics"]["avg_api_latency_ms"]
        if avg_latency < 500:
            indicateurs.append(f"LATENCE_SUSPECTE: {avg_latency:.1f}ms (< 500ms)")
            
        # RÃ©ponses trop uniformes
        api_calls = traces["api_calls"]
        if len(api_calls) > 1:
            latencies = [call["latency_ms"] for call in api_calls]
            if max(latencies) - min(latencies) < 100:
                indicateurs.append("LATENCES_UNIFORMES: Variation < 100ms")
                
        # Tokens trop prÃ©visibles
        if len(api_calls) > 1:
            tokens = [call["tokens_used"] for call in api_calls]
            if len(set(tokens)) == 1:
                indicateurs.append("TOKENS_IDENTIQUES: MÃªme nombre de tokens pour tous les appels")
                
        return indicateurs
    
    @staticmethod
    def _generer_recommandations(traces: Dict, synthese: Dict) -> List[str]:
        """GÃ©nÃ¨re des recommandations automatiques."""
        recommandations = []
        
        if synthese["metriques_techniques"]["taux_succes_global"] < 90:
            recommandations.append("AMELIORER_ROBUSTESSE: Taux de succÃ¨s < 90%")
            
        if not synthese["analyse_performance"]["seuils_respectes"]["latence_api_realiste"]:
            recommandations.append("VERIFIER_APPELS_API: Latences trop rapides, possible mock")
            
        if len(synthese["preuves_authenticite"]["indicateurs_mocks"]) > 0:
            recommandations.append("ELIMINER_MOCKS: Indicateurs de mocks dÃ©tectÃ©s")
            
        if synthese["resume_executif"]["interactions_agents"] < 2:
            recommandations.append("AUGMENTER_INTERACTIONS: Pas assez d'interactions entre agents")
            
        return recommandations

async def main():
    """Point d'entrÃ©e principal avec gestion de dataset paramÃ©trable."""
    import sys
    
    # Gestion des arguments de ligne de commande
    dataset_name = sys.argv[1] if len(sys.argv) > 1 else None
    if not dataset_name:
        dataset_name = f"traces_auto_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    print(f"[TRACE] DÃ‰MARRAGE GÃ‰NÃ‰RATEUR DE TRACES COMPLÃˆTES")
    print(f"[DATA] Dataset: {dataset_name}")
    print("=" * 60)
    
    # Initialisation du testeur
    testeur = TesteurTracesCompletes(dataset_name)
    await testeur.setup_openai_client()
    
    # GÃ©nÃ©ration de donnÃ©es synthÃ©tiques uniques
    test_id = str(uuid.uuid4())[:8]
    scenario_unique = {
        "name": f"enquete_quantique_{test_id}",
        "question": f"""ENQUÃŠTE AUTHENTIQUE {test_id}
        
Dans le laboratoire de l'EPITA, le professeur von Neumann a disparu mystÃ©rieusement.

Indices trouvÃ©s:
- Ã‰quation de SchrÃ¶dinger incomplÃ¨te au tableau
- Chat quantique absent de sa boÃ®te
- Particule manquante dans l'accÃ©lÃ©rateur

Suspects:
- Dr. Heisenberg (expert en principe d'incertitude)
- Prof. Einstein (opposÃ© Ã  la mÃ©canique quantique)
- Mme. Curie (spÃ©cialiste des radiations)

Sherlock, analysez cette situation. Watson, aidez avec la logique.
Maximum 2 tours pour cette dÃ©monstration d'authenticitÃ©."""
    }
    
    # Test 1: Appel API direct
    print("\n[TEST] [TEST 1] Appel API direct avec traÃ§age...")
    prompt_test = f"Analysez ce mystÃ¨re quantique (ID: {test_id}): {scenario_unique['question'][:200]}..."
    resultat_api = await testeur.test_appel_api_direct(prompt_test)
    
    if resultat_api["success"]:
        print(f"[OK] API Direct: {resultat_api['latency_ms']:.1f}ms, {resultat_api['tokens']} tokens")
    else:
        print(f"[ERROR] API Direct: {resultat_api['error']}")
    
    # Test 2: Orchestrateur avec traces
    print("\n[TEST] [TEST 2] Orchestrateur avec traÃ§age complet...")
    resultat_orchestrateur = await testeur.test_orchestrateur_avec_traces(scenario_unique)
    
    if resultat_orchestrateur["success"]:
        print(f"[OK] Orchestrateur: {resultat_orchestrateur['execution_time']:.1f}s, {resultat_orchestrateur['conversation_turns']} tours")
    else:
        print(f"[ERROR] Orchestrateur: {resultat_orchestrateur['error']}")
    
    # Finalisation et synthÃ¨se automatique
    print("\n[DATA] GÃ©nÃ©ration de la synthÃ¨se automatique...")
    traces_finales = testeur.traceur.finalize_session()
    synthese_auto = SynthetiseurAutomatique.generer_synthese_complete(traces_finales)
    
    # Sauvegarde des traces et synthÃ¨se
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Traces complÃ¨tes
    traces_file = f"reports/traces_completes_{dataset_name}_{timestamp}.json"
    with open(traces_file, 'w', encoding='utf-8') as f:
        json.dump(traces_finales, f, indent=2, ensure_ascii=False)
    
    # SynthÃ¨se automatique
    synthese_file = f"reports/synthese_auto_{dataset_name}_{timestamp}.json"
    with open(synthese_file, 'w', encoding='utf-8') as f:
        json.dump(synthese_auto, f, indent=2, ensure_ascii=False)
    
    # Rapport final automatique
    rapport_final = {
        "dataset": dataset_name,
        "timestamp": timestamp,
        "files": {
            "traces_completes": traces_file,
            "synthese_automatique": synthese_file
        },
        "resume": synthese_auto["resume_executif"],
        "authenticite": {
            "score_confiance": synthese_auto["preuves_authenticite"]["score_confiance_moyen"],
            "indicateurs_mocks": synthese_auto["preuves_authenticite"]["indicateurs_mocks"],
            "verdict": "AUTHENTIQUE" if synthese_auto["preuves_authenticite"]["score_confiance_moyen"] > 0.8 else "SUSPECT"
        },
        "recommandations": synthese_auto["recommandations"]
    }
    
    rapport_file = f"reports/rapport_final_{dataset_name}_{timestamp}.json"
    with open(rapport_file, 'w', encoding='utf-8') as f:
        json.dump(rapport_final, f, indent=2, ensure_ascii=False)
    
    # Affichage des rÃ©sultats
    print("\n" + "="*60)
    print("ğŸ“‹ SYNTHÃˆSE AUTOMATIQUE GÃ‰NÃ‰RÃ‰E")
    print("="*60)
    print(f"[DATA] Score authenticitÃ©: {synthese_auto['preuves_authenticite']['score_confiance_moyen']:.2f}")
    print(f"â±ï¸  DurÃ©e totale: {synthese_auto['resume_executif']['duree_totale_secondes']:.1f}s")
    print(f"[TEST] Appels API: {synthese_auto['resume_executif']['appels_api_reussis']}")
    print(f"ğŸ¤– Interactions agents: {synthese_auto['resume_executif']['interactions_agents']}")
    print(f"âš ï¸  Indicateurs mocks: {len(synthese_auto['preuves_authenticite']['indicateurs_mocks'])}")
    
    if synthese_auto["preuves_authenticite"]["indicateurs_mocks"]:
        print("\nğŸš¨ INDICATEURS DE MOCKS DÃ‰TECTÃ‰S:")
        for indicateur in synthese_auto["preuves_authenticite"]["indicateurs_mocks"]:
            print(f"   - {indicateur}")
    
    if synthese_auto["recommandations"]:
        print("\nğŸ’¡ RECOMMANDATIONS AUTOMATIQUES:")
        for rec in synthese_auto["recommandations"]:
            print(f"   - {rec}")
    
    print(f"\nğŸ“ Fichiers gÃ©nÃ©rÃ©s:")
    print(f"   - Traces: {traces_file}")
    print(f"   - SynthÃ¨se: {synthese_file}")
    print(f"   - Rapport: {rapport_file}")
    
    verdict = rapport_final["authenticite"]["verdict"]
    print(f"\nğŸ¯ VERDICT: {verdict}")
    
    return rapport_final

if __name__ == "__main__":
    asyncio.run(main())