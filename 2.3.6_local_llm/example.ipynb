{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e229d8f1",
   "metadata": {},
   "source": [
    "## Load multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05469d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llm.local_llm import LocalLlm\n",
    "from local_llm.constants import ModelEnum\n",
    "import time\n",
    "import pandas as pd\n",
    "stats = pd.read_csv(\"benchmarks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a6d797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_answer_time</th>\n",
       "      <th>answer_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TinyLlama1_1B_Q8</td>\n",
       "      <td>0.858762</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama3_1B_Q8</td>\n",
       "      <td>0.269015</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama3_3B_Q6</td>\n",
       "      <td>1.488254</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama3_8B_Q6</td>\n",
       "      <td>2.273151</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qwen2_7B_Q8</td>\n",
       "      <td>1.981633</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Qwen3_14B_Q5</td>\n",
       "      <td>7.718978</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MistralNemo_8B_Q8</td>\n",
       "      <td>8.555013</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Phi2_2B_Q8</td>\n",
       "      <td>2.945099</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Phi4_3B_Q4</td>\n",
       "      <td>3.190574</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  avg_answer_time  answer_quality\n",
       "0   TinyLlama1_1B_Q8         0.858762             1.0\n",
       "1       Llama3_1B_Q8         0.269015             1.0\n",
       "2       Llama3_3B_Q6         1.488254             4.0\n",
       "3       Llama3_8B_Q6         2.273151             5.0\n",
       "4        Qwen2_7B_Q8         1.981633             5.0\n",
       "5       Qwen3_14B_Q5         7.718978             2.5\n",
       "6  MistralNemo_8B_Q8         8.555013             5.0\n",
       "7         Phi2_2B_Q8         2.945099             1.0\n",
       "8         Phi4_3B_Q4         3.190574             3.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4203e51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model MistralNemo_8B_Q8 from models/MistralNemo_8B_Q8/model.gguf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "instance = LocalLlm([ModelEnum.MistralNemo_8B_Q8], max_window_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59810b",
   "metadata": {},
   "source": [
    "## Ask them to analyze statements in order to find argumentative fallacies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b93fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting...\n",
      " {\n",
      "  \"arguments\": [\n",
      "    {\n",
      "      \"text\": \"Vaccines are dangerous because my neighbor experienced side effects after his vaccination.\",\n",
      "      \"is_valid\": false,\n",
      "      \"fallacy_type\": \"Hasty Generalization\",\n",
      "      \"explanation\": \"The argument is based on a single anecdotal experience, which is not sufficient to make a general claim about the safety of vaccines.\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Pharmaceutical companies are only interested in making a profit.\",\n",
      "      \"is_valid\": true,\n",
      "      \"fallacy_type\": \"null\",\n",
      "      \"explanation\": \"This statement is generally true and does not contain a fallacy.\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Therefore, we should not trust vaccines.\",\n",
      "      \"is_valid\": false,\n",
      "      \"fallacy_type\": \"False Dilemma\",\n",
      "      \"explanation\": \"The argument presents a false choice between trusting pharmaceutical companies and trusting vaccines. In reality, one can trust vaccines without trusting pharmaceutical companies' business practices.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#question = \"Les vaccins sont dangereux car mon voisin a eu des effets secondaires après sa vaccination. De plus, les laboratoires pharmaceutiques ne cherchent qu'à faire du profit. Par conséquent, nous ne devrions pas faire confiance aux vaccins.\"\n",
    "question = \"Vaccines are dangerous because my neighbor experienced side effects after his vaccination. Furthermore, pharmaceutical companies are only interested in making a profit. Therefore, we should not trust vaccines.\"\n",
    "#question = \"Si l'IA devient plus intelligente que les humains, alors soit elle nous aidera, soit elle nous remplacera. Or, une IA superintelligente n'aura pas besoin de nous aider. Donc elle nous remplacera.\"\n",
    "#question = \"Votre argument sur le climat est invalide car vous n'êtes pas climatologue.\"\n",
    "#question = \"Si nous n'agissons pas maintenant contre le changement climatique, nos enfants vivront dans un monde invivable. Nous devons donc interdire immédiatement toutes les voitures.\"\n",
    "#question = \"Tous les cygnes que j'ai vus sont blancs, donc tous les cygnes sont blancs.\"\n",
    "#question = \"Terrorism is the best political weapon for nothing drives people harder than a fear of sudden death.\"\n",
    "\n",
    "print(instance(ModelEnum.MistralNemo_8B_Q8, question, max_tokens=500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc79404b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50919/570000798.py:1: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  stats[\"answer_quality\"][stats[\"model\"] == ModelEnum.MistralNemo_8B_Q8.name] = 5\n",
      "/tmp/ipykernel_50919/570000798.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stats[\"answer_quality\"][stats[\"model\"] == ModelEnum.MistralNemo_8B_Q8.name] = 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>load_time</th>\n",
       "      <th>avg_answer_time</th>\n",
       "      <th>answer_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TinyLlama1_1B_Q8</td>\n",
       "      <td>1.382449</td>\n",
       "      <td>0.858762</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama3_1B_Q8</td>\n",
       "      <td>1.459934</td>\n",
       "      <td>0.269015</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama3_3B_Q6</td>\n",
       "      <td>0.907212</td>\n",
       "      <td>1.488254</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama3_8B_Q6</td>\n",
       "      <td>7.657012</td>\n",
       "      <td>2.273151</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qwen2_7B_Q8</td>\n",
       "      <td>9.364495</td>\n",
       "      <td>1.981633</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Qwen3_14B_Q5</td>\n",
       "      <td>63.091192</td>\n",
       "      <td>7.718978</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MistralNemo_8B_Q8</td>\n",
       "      <td>54.724201</td>\n",
       "      <td>8.555013</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Phi2_2B_Q8</td>\n",
       "      <td>14.900199</td>\n",
       "      <td>2.945099</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Phi4_3B_Q4</td>\n",
       "      <td>14.494035</td>\n",
       "      <td>3.190574</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  load_time  avg_answer_time  answer_quality\n",
       "0   TinyLlama1_1B_Q8   1.382449         0.858762             1.0\n",
       "1       Llama3_1B_Q8   1.459934         0.269015             1.0\n",
       "2       Llama3_3B_Q6   0.907212         1.488254             4.0\n",
       "3       Llama3_8B_Q6   7.657012         2.273151             5.0\n",
       "4        Qwen2_7B_Q8   9.364495         1.981633             5.0\n",
       "5       Qwen3_14B_Q5  63.091192         7.718978             2.5\n",
       "6  MistralNemo_8B_Q8  54.724201         8.555013             5.0\n",
       "7         Phi2_2B_Q8  14.900199         2.945099             1.0\n",
       "8         Phi4_3B_Q4  14.494035         3.190574             3.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats[\"answer_quality\"][stats[\"model\"] == ModelEnum.MistralNemo_8B_Q8.name] = 5\n",
    "stats.to_csv(\"benchmarks.csv\", index=False)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d5361f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model TinyLlama1_1B_Q8 from models/TinyLlama1_1B_Q8/model.gguf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50919/2383101347.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stats[stats[\"model\"] == entry.name][[\"model\", \"load_time\", \"avg_answer_time\"]] = [entry.name, load_time, avg_answer_time]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Llama3_1B_Q8 from models/Llama3_1B_Q8/model.gguf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Llama3_3B_Q6 from models/Llama3_3B_Q6/model.gguf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Llama3_8B_Q6 from models/Llama3_8B_Q6/model.gguf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (10240) -- the full capacity of the model will not be utilized\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Qwen2_7B_Q8 from models/Qwen2_7B_Q8/model.gguf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "100%|██████████| 1/1 [00:38<00:00, 38.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Qwen3_14B_Q5 from models/Qwen3_14B_Q5/model.gguf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
      "100%|██████████| 1/1 [01:09<00:00, 69.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model MistralNemo_8B_Q8 from models/MistralNemo_8B_Q8/model.gguf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "100%|██████████| 1/1 [00:57<00:00, 57.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Phi2_2B_Q8 from models/Phi2_2B_Q8/model.gguf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Phi4_3B_Q4 from models/Phi4_3B_Q4/model.gguf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "Prompting...\n",
      "               model  load_time  avg_answer_time  answer_quality\n",
      "0   TinyLlama1_1B_Q8   1.382449         0.858762             1.0\n",
      "1       Llama3_1B_Q8   1.459934         0.269015             1.0\n",
      "2       Llama3_3B_Q6   0.907212         1.488254             4.0\n",
      "3       Llama3_8B_Q6   7.657012         2.273151             5.0\n",
      "4        Qwen2_7B_Q8   9.364495         1.981633             5.0\n",
      "5       Qwen3_14B_Q5  63.091192         7.718978             2.5\n",
      "6  MistralNemo_8B_Q8  54.724201         8.555013             5.0\n",
      "7         Phi2_2B_Q8  14.900199         2.945099             1.0\n",
      "8         Phi4_3B_Q4  14.494035         3.190574             3.5\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Les vaccins sont dangereux car mon voisin a eu des effets secondaires après sa vaccination. De plus, les laboratoires pharmaceutiques ne cherchent qu'à faire du profit. Par conséquent, nous ne devrions pas faire confiance aux vaccins.\",\n",
    "    \"Vaccines are dangerous because my neighbor experienced side effects after his vaccination. Furthermore, pharmaceutical companies are only interested in making a profit. Therefore, we should not trust vaccines.\",\n",
    "    \"Si l'IA devient plus intelligente que les humains, alors soit elle nous aidera, soit elle nous remplacera. Or, une IA superintelligente n'aura pas besoin de nous aider. Donc elle nous remplacera.\",\n",
    "    \"Votre argument sur le climat est invalide car vous n'êtes pas climatologue.\",\n",
    "    \"Si nous n'agissons pas maintenant contre le changement climatique, nos enfants vivront dans un monde invivable. Nous devons donc interdire immédiatement toutes les voitures.\",\n",
    "    \"Tous les cygnes que j'ai vus sont blancs, donc tous les cygnes sont blancs.\",\n",
    "    \"Terrorism is the best political weapon for nothing drives people harder than a fear of sudden death.\",\n",
    "]\n",
    "\n",
    "\n",
    "for entry in ModelEnum:\n",
    "    t0 = time.time()\n",
    "    instance = LocalLlm([entry], max_window_size=2048)\n",
    "    load_time = time.time() - t0\n",
    "    times = []\n",
    "    t0 = time.time()\n",
    "    for question in questions:\n",
    "        answer = instance(entry, question, max_tokens=500)\n",
    "    avg_answer_time = (time.time() - t0) / len(questions)\n",
    "    stats[stats[\"model\"] == entry.name][[\"model\", \"load_time\", \"avg_answer_time\"]] = [entry.name, load_time, avg_answer_time]\n",
    "    del instance\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e347634",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.to_csv(\"benchmarks.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iasy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
