#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test de Migration - Script 1: unified_production_analyzer.py
============================================================

Test de validation de la transformation du premier script selon la spÃ©cification
technique de migration vers le pipeline unifiÃ© central.

Tests couverts :
1. âœ… Interface CLI identique
2. âœ… Mapping des paramÃ¨tres vers ExtendedOrchestrationConfig  
3. âœ… DÃ©lÃ©gation au pipeline unifiÃ©
4. âœ… CompatibilitÃ© des rÃ©sultats
5. âœ… Toutes les fonctionnalitÃ©s prÃ©servÃ©es

Version: 1.0.0
Auteur: Roo - Validation Migration
"""

import os
import sys
import asyncio
import logging
from pathlib import Path

# Ajout du rÃ©pertoire racine au chemin
project_root = Path(__file__).resolve().parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Import du script transformÃ©
from scripts.consolidated.unified_production_analyzer import (
    UnifiedProductionAnalyzer,
    UnifiedProductionConfig,
    LogicType,
    MockLevel,
    OrchestrationType,
    AnalysisMode,
    create_cli_parser,
    create_config_from_args
)

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger("TestMigrationScript1")


class TestMigrationScript1:
    """Tests de validation de la migration du script 1"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.TestMigrationScript1")
        self.test_results = []
    
    async def test_configuration_mapping(self):
        """Test 1: Validation du mapping de configuration"""
        self.logger.info("ğŸ§ª Test 1: Mapping de configuration CLI â†’ ExtendedOrchestrationConfig")
        
        try:
            # Configuration de test
            config = UnifiedProductionConfig(
                logic_type=LogicType.FOL,
                mock_level=MockLevel.NONE,
                orchestration_type=OrchestrationType.UNIFIED,
                analysis_modes=[AnalysisMode.UNIFIED, AnalysisMode.FALLACIES],
                enable_conversation_trace=True,
                max_workers=4
            )
            
            analyzer = UnifiedProductionAnalyzer(config)
            
            # Test mapping orchestration
            orchestration_mode = analyzer._map_orchestration_mode()
            assert orchestration_mode.value in ["hierarchical_full", "conversation", "operational_direct", "real", "pipeline"]
            
            # Test mapping analysis type
            analysis_type = analyzer._map_analysis_type("rhetorical")
            assert analysis_type.value in ["rhetorical", "comprehensive", "fallacy_focused", "argument_structure"]
            
            # Test construction config unifiÃ©e
            unified_config = analyzer._build_config("rhetorical")
            assert unified_config is not None
            assert hasattr(unified_config, 'analysis_modes')
            assert hasattr(unified_config, 'orchestration_mode')
            assert hasattr(unified_config, 'enable_hierarchical')
            
            self.test_results.append(("Configuration Mapping", "âœ… RÃ‰USSI"))
            self.logger.info("âœ… Test 1 rÃ©ussi - Configuration correctement mappÃ©e")
            
        except Exception as e:
            self.test_results.append(("Configuration Mapping", f"âŒ Ã‰CHEC: {e}"))
            self.logger.error(f"âŒ Test 1 Ã©chouÃ©: {e}")
    
    def test_cli_interface_preservation(self):
        """Test 2: Validation de la prÃ©servation de l'interface CLI"""
        self.logger.info("ğŸ§ª Test 2: PrÃ©servation interface CLI")
        
        try:
            # Test crÃ©ation parser CLI
            parser = create_cli_parser()
            assert parser is not None
            
            # Test simulation arguments CLI
            test_args = [
                "--analysis-modes", "unified", "fallacies",
                "--orchestration-type", "unified", 
                "--logic-type", "fol",
                "--mock-level", "none",
                "--enable-conversation-trace",
                "--max-workers", "4"
            ]
            
            # Parse avec arguments de test
            args = parser.parse_args(test_args + ["Test text"])
            
            # Validation des attributs CLI essentiels
            assert hasattr(args, 'analysis_modes')
            assert hasattr(args, 'orchestration_type')
            assert hasattr(args, 'logic_type')
            assert hasattr(args, 'mock_level')
            assert hasattr(args, 'enable_conversation_trace')
            assert hasattr(args, 'max_workers')
            
            # Test crÃ©ation configuration depuis args
            config = create_config_from_args(args)
            assert config is not None
            assert config.logic_type == LogicType.FOL
            assert config.mock_level == MockLevel.NONE
            assert config.orchestration_type == OrchestrationType.UNIFIED
            
            self.test_results.append(("Interface CLI", "âœ… RÃ‰USSI"))
            self.logger.info("âœ… Test 2 rÃ©ussi - Interface CLI prÃ©servÃ©e")
            
        except Exception as e:
            self.test_results.append(("Interface CLI", f"âŒ Ã‰CHEC: {e}"))
            self.logger.error(f"âŒ Test 2 Ã©chouÃ©: {e}")
    
    async def test_pipeline_delegation(self):
        """Test 3: Validation de la dÃ©lÃ©gation au pipeline unifiÃ©"""
        self.logger.info("ğŸ§ª Test 3: DÃ©lÃ©gation au pipeline unifiÃ©")
        
        try:
            # Configuration minimale pour test
            config = UnifiedProductionConfig(
                mock_level=MockLevel.FULL,  # Mode test pour Ã©viter les dÃ©pendances
                logic_type=LogicType.FOL,
                orchestration_type=OrchestrationType.UNIFIED,
                analysis_modes=[AnalysisMode.UNIFIED],
                check_dependencies=False  # Ignorer la validation des dÃ©pendances pour le test
            )
            
            analyzer = UnifiedProductionAnalyzer(config)
            
            # Test initialisation
            init_success = await analyzer.initialize()
            assert init_success, "L'initialisation doit rÃ©ussir"
            
            # Test construction de config unifiÃ©e
            unified_config = analyzer._build_config("rhetorical")
            
            # Validation des attributs clÃ©s de la config unifiÃ©e
            assert unified_config.enable_hierarchical == True
            assert unified_config.enable_specialized_orchestrators == True
            assert unified_config.enable_communication_middleware == True
            assert unified_config.save_orchestration_trace == config.save_trace
            
            self.test_results.append(("DÃ©lÃ©gation Pipeline", "âœ… RÃ‰USSI"))
            self.logger.info("âœ… Test 3 rÃ©ussi - DÃ©lÃ©gation correctement configurÃ©e")
            
        except Exception as e:
            self.test_results.append(("DÃ©lÃ©gation Pipeline", f"âŒ Ã‰CHEC: {e}"))
            self.logger.error(f"âŒ Test 3 Ã©chouÃ©: {e}")
    
    async def test_error_handling(self):
        """Test 4: Validation de la gestion d'erreur"""
        self.logger.info("ğŸ§ª Test 4: Gestion d'erreur")
        
        try:
            # Configuration invalide pour tester la gestion d'erreur
            config = UnifiedProductionConfig(
                tweety_retry_count=0,  # Invalide
                llm_retry_count=0,     # Invalide
                max_workers=0          # Invalide si parallel activÃ©
            )
            
            # Test validation de configuration
            is_valid, errors = config.validate()
            assert not is_valid, "La configuration invalide doit Ãªtre dÃ©tectÃ©e"
            assert len(errors) > 0, "Des erreurs doivent Ãªtre rapportÃ©es"
            
            self.test_results.append(("Gestion Erreur", "âœ… RÃ‰USSI"))
            self.logger.info("âœ… Test 4 rÃ©ussi - Gestion d'erreur fonctionnelle")
            
        except Exception as e:
            self.test_results.append(("Gestion Erreur", f"âŒ Ã‰CHEC: {e}"))
            self.logger.error(f"âŒ Test 4 Ã©chouÃ©: {e}")
    
    def test_enum_compatibility(self):
        """Test 5: Validation de la compatibilitÃ© des enums"""
        self.logger.info("ğŸ§ª Test 5: CompatibilitÃ© des enums")
        
        try:
            # Test que tous les enums conservent leurs valeurs
            assert LogicType.FOL.value == "fol"
            assert LogicType.PL.value == "propositional"
            assert LogicType.MODAL.value == "modal"
            
            assert MockLevel.NONE.value == "none"
            assert MockLevel.PARTIAL.value == "partial"
            assert MockLevel.FULL.value == "full"
            
            assert OrchestrationType.UNIFIED.value == "unified"
            assert OrchestrationType.CONVERSATION.value == "conversation"
            assert OrchestrationType.MICRO.value == "micro"
            assert OrchestrationType.REAL_LLM.value == "real_llm"
            
            self.test_results.append(("CompatibilitÃ© Enums", "âœ… RÃ‰USSI"))
            self.logger.info("âœ… Test 5 rÃ©ussi - Enums compatibles")
            
        except Exception as e:
            self.test_results.append(("CompatibilitÃ© Enums", f"âŒ Ã‰CHEC: {e}"))
            self.logger.error(f"âŒ Test 5 Ã©chouÃ©: {e}")
    
    async def run_all_tests(self):
        """ExÃ©cute tous les tests de validation"""
        self.logger.info("ğŸš€ DÃ©marrage des tests de migration - Script 1")
        self.logger.info("=" * 60)
        
        # Tests synchrones
        self.test_cli_interface_preservation()
        self.test_enum_compatibility()
        
        # Tests asynchrones
        await self.test_configuration_mapping()
        await self.test_pipeline_delegation()
        await self.test_error_handling()
        
        # RÃ©sumÃ© des rÃ©sultats
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š RÃ‰SULTATS DES TESTS DE MIGRATION")
        
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if "âœ…" in r[1]])
        failed_tests = total_tests - passed_tests
        
        for test_name, result in self.test_results:
            self.logger.info(f"{result} {test_name}")
        
        self.logger.info("-" * 60)
        self.logger.info(f"Total: {total_tests} | RÃ©ussis: {passed_tests} | Ã‰chouÃ©s: {failed_tests}")
        
        if failed_tests == 0:
            self.logger.info("ğŸ‰ MIGRATION SCRIPT 1 VALIDÃ‰E - Tous les tests passent!")
            return True
        else:
            self.logger.error(f"âŒ MIGRATION INCOMPLÃˆTE - {failed_tests} test(s) Ã©chouÃ©(s)")
            return False


async def main():
    """Fonction principale de test"""
    try:
        tester = TestMigrationScript1()
        success = await tester.run_all_tests()
        
        if success:
            print("\nğŸ¯ CONCLUSION: La migration du script 1 est conforme aux spÃ©cifications!")
            print("âœ… Interface CLI prÃ©servÃ©e")
            print("âœ… Configuration correctement mappÃ©e vers ExtendedOrchestrationConfig") 
            print("âœ… DÃ©lÃ©gation au pipeline unifiÃ© fonctionnelle")
            print("âœ… Gestion d'erreur maintenue")
            print("âœ… CompatibilitÃ© assurÃ©e")
            return 0
        else:
            print("\nâŒ MIGRATION INCOMPLÃˆTE - Des corrections sont nÃ©cessaires")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ Erreur dans les tests: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)