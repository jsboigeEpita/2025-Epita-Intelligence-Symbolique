# Guide de Contribution

Merci de contribuer au projet **2025-Epita-Intelligence-Symbolique** ! ğŸ‰

Ce guide vous aidera Ã  configurer votre environnement de dÃ©veloppement et Ã  comprendre notre workflow de tests, particuliÃ¨rement pour les tests nÃ©cessitant des clÃ©s API externes.

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Configuration de l'Environnement](#configuration-de-lenvironnement)
2. [Tests NÃ©cessitant des API Keys](#tests-nÃ©cessitant-des-api-keys)
3. [Markers Pytest Disponibles](#markers-pytest-disponibles)
4. [ExÃ©cution Locale avec API Keys](#exÃ©cution-locale-avec-api-keys)
5. [Comportement en CI](#comportement-en-ci)
6. [Structure des Tests](#structure-des-tests)
7. [Best Practices](#best-practices)
8. [Ressources Utiles](#ressources-utiles)

---

## Configuration de l'Environnement

### PrÃ©requis

- **Python 3.10+** via Miniconda/Anaconda
- **Java 11+** (pour Tweety/JPype)
- **Git**

### Installation

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/jsboigeEpita/2025-Epita-Intelligence-Symbolique.git
cd 2025-Epita-Intelligence-Symbolique

# CrÃ©er l'environnement conda
conda env create -f environment.yml

# Activer l'environnement
conda activate epita-symbolic-ai

# VÃ©rifier l'installation
pytest --version
```

---

## Tests NÃ©cessitant des API Keys

Certains tests nÃ©cessitent des clÃ©s API pour tester des intÃ©grations rÃ©elles avec des services externes. Ces tests sont **automatiquement skippÃ©s** si les clÃ©s ne sont pas configurÃ©es.

### Markers Pytest Disponibles

Le projet utilise des **markers pytest personnalisÃ©s** pour gÃ©rer les dÃ©pendances aux API keys :

| Marker | Description | ClÃ© Requise |
|--------|-------------|-------------|
| `@pytest.mark.requires_api` | Test nÃ©cessite **au moins une** clÃ© API | `OPENAI_API_KEY` OU `GITHUB_TOKEN` OU `OPENROUTER_API_KEY` |
| `@pytest.mark.requires_openai` | Test nÃ©cessite spÃ©cifiquement OpenAI | `OPENAI_API_KEY` |
| `@pytest.mark.requires_github` | Test nÃ©cessite spÃ©cifiquement GitHub | `GITHUB_TOKEN` |
| `@pytest.mark.requires_openrouter` | Test nÃ©cessite spÃ©cifiquement OpenRouter | `OPENROUTER_API_KEY` |

### Comment Ajouter un Test NÃ©cessitant une API

**Exemple avec OpenAI :**

```python
import pytest

@pytest.mark.requires_openai
def test_openai_integration():
    """Test requiring OpenAI API key."""
    from openai import OpenAI
    
    client = OpenAI()  # Utilise automatiquement OPENAI_API_KEY
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "Hello!"}],
        max_tokens=5
    )
    
    assert response.choices[0].message.content
```

**Exemple avec marker gÃ©nÃ©rique :**

```python
import pytest
import os

@pytest.mark.requires_api
def test_any_api_integration():
    """Test requiring at least one API key."""
    # Ce test sera skippÃ© si AUCUNE clÃ© API n'est disponible
    # Vous pouvez adapter le comportement selon les clÃ©s prÃ©sentes
    
    if os.getenv("OPENAI_API_KEY"):
        # Test avec OpenAI
        pass
    elif os.getenv("GITHUB_TOKEN"):
        # Test avec GitHub
        pass
```

**âš ï¸ Important :**
- **Ne PAS** utiliser `pytest.skip()` manuellement dans le corps du test
- Utiliser les markers pytest pour que le comportement soit cohÃ©rent
- Les markers gÃ¨rent automatiquement le skip via le `conftest.py`

---

## ExÃ©cution Locale avec API Keys

### 1. Configurer les Secrets Localement

CrÃ©ez un fichier **`.env`** Ã  la racine du projet (ce fichier est dans `.gitignore` et ne sera **jamais commitÃ©**) :

```bash
# .env
OPENAI_API_KEY=sk-proj-...
GITHUB_TOKEN=ghp_...
OPENROUTER_API_KEY=sk-or-v1-...
```

ğŸ’¡ **Astuce :** Copiez `.env.example` si disponible :

```bash
cp .env.example .env
# Puis Ã©ditez .env avec vos clÃ©s
```

### 2. Obtenir des ClÃ©s API

| Service | URL | CoÃ»t |
|---------|-----|------|
| **OpenAI** | [platform.openai.com/api-keys](https://platform.openai.com/api-keys) | Free tier disponible |
| **OpenRouter** | [openrouter.ai/keys](https://openrouter.ai/keys) | Pay-as-you-go |
| **GitHub** | [github.com/settings/tokens](https://github.com/settings/tokens) | Gratuit |

### 3. ExÃ©cuter les Tests

```bash
# Activer l'environnement
conda activate epita-symbolic-ai

# ExÃ©cuter TOUS les tests (y compris ceux avec API)
pytest tests/

# ExÃ©cuter UNIQUEMENT les tests nÃ©cessitant OpenAI
pytest tests/ -m requires_openai

# ExÃ©cuter en EXCLUANT les tests nÃ©cessitant des API
pytest tests/ -m "not requires_api"

# Mode verbose avec dÃ©tails
pytest tests/ -v

# Avec rapport de couverture
pytest tests/ --cov=. --cov-report=html
```

### 4. VÃ©rifier Quels Tests Seront SkippÃ©s

```bash
# Lister tous les tests avec leur statut
pytest --collect-only -q

# Lister uniquement les tests nÃ©cessitant des API
pytest --collect-only -m requires_api -q
```

---

## Comportement en CI

### Sur GitHub Actions

Les tests s'exÃ©cutent automatiquement sur chaque **push** et **pull request** vers `main`.

**Avec Secrets ConfigurÃ©s (Repository Principal) :**
- âœ… Tests de linting et formatage
- âœ… Tests unitaires
- âœ… Tests d'intÃ©gration **avec API rÃ©elles**
- ğŸ“Š RÃ©sumÃ© automatique des tests (passed/failed/skipped)
- ğŸ“¦ Artefacts uploadÃ©s (rapports XML, logs)

**Sans Secrets (Forks) :**
- âœ… Tests de linting et formatage
- âœ… Tests unitaires
- â­ï¸ Tests d'intÃ©gration **skippÃ©s gracieusement**
- â„¹ï¸ Message clair expliquant pourquoi les tests sont skippÃ©s

**Exemple de sortie CI :**

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š TEST EXECUTION SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Passed:  142 / 165
âŒ Failed:  0 / 165
â­ï¸  Skipped: 23 / 165

â„¹ï¸  Note: 23 tests were skipped (likely due to missing API keys)
   These tests require OPENAI_API_KEY or other API credentials.
   Configure secrets in GitHub repository settings to run them.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Configuration des Secrets GitHub

Pour les **mainteneurs** du repository :

1. Aller dans **Settings** â†’ **Secrets and variables** â†’ **Actions**
2. Ajouter les secrets :
   - `OPENAI_API_KEY` : ClÃ© OpenAI
   - `TEXT_CONFIG_PASSPHRASE` : Passphrase pour donnÃ©es Ã©ducatives
   - (Optionnel) `OPENROUTER_API_KEY`

---

## Structure des Tests

```
tests/
â”œâ”€â”€ unit/              # Tests unitaires (PAS de dÃ©pendances externes)
â”‚   â”œâ”€â”€ test_core.py
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ integration/       # Tests d'intÃ©gration (peuvent nÃ©cessiter des API)
â”‚   â”œâ”€â”€ test_openai_integration.py
â”‚   â””â”€â”€ test_github_integration.py
â”œâ”€â”€ performance/       # Tests de performance (souvent avec API)
â”‚   â””â”€â”€ test_oracle_performance.py
â””â”€â”€ e2e/              # Tests end-to-end (nÃ©cessitent souvent des API)
    â””â”€â”€ test_full_workflow.py
```

### Conventions de Nommage

- Tests unitaires : `test_<module>_<fonction>.py`
- Tests d'intÃ©gration : `test_<service>_integration.py`
- Tests nÃ©cessitant API : **TOUJOURS** marquer avec `@pytest.mark.requires_*`

---

## Best Practices

### âœ… DO

1. **Toujours marquer** les tests nÃ©cessitant des API avec les markers appropriÃ©s
2. **Minimiser** les appels API dans les tests (utiliser des mocks quand possible)
3. **Documenter** les dÃ©pendances API dans les docstrings
4. **Tester localement** AVEC et SANS API keys avant de push
5. **Utiliser `.env`** pour les secrets locaux (jamais commit)

### âŒ DON'T

1. **Ne JAMAIS** committer le fichier `.env` ou des clÃ©s API
2. **Ne PAS** hardcoder des clÃ©s API dans le code
3. **Ne PAS** utiliser `pytest.skip()` manuellement (utiliser les markers)
4. **Ne PAS** faire de longs appels API sans timeout
5. **Ne PAS** assumer que les API keys sont toujours disponibles

### Exemple de Test Bien StructurÃ©

```python
import pytest
import os
from openai import OpenAI

@pytest.mark.requires_openai
@pytest.mark.integration
def test_openai_chat_completion():
    """
    Test d'intÃ©gration avec l'API OpenAI GPT-4o-mini.
    
    PrÃ©requis :
    - OPENAI_API_KEY configurÃ©e dans .env
    - CrÃ©dit disponible sur le compte OpenAI
    
    Ce test vÃ©rifie que :
    - La connexion Ã  OpenAI fonctionne
    - Le modÃ¨le gpt-4o-mini rÃ©pond correctement
    - La rÃ©ponse est non-vide
    """
    client = OpenAI()  # Utilise OPENAI_API_KEY de l'environnement
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "Say 'test'"}],
        max_tokens=5,
        timeout=10  # Toujours dÃ©finir un timeout !
    )
    
    assert response.choices[0].message.content.strip().lower() == "test"
```

---

## Ressources Utiles

### Documentation

- **Architecture CI/CD** : [`docs/architecture/ci_secrets_strategy.md`](docs/architecture/ci_secrets_strategy.md)
- **Configuration Pytest** : [`pytest.ini`](pytest.ini)
- **Workflow CI** : [`.github/workflows/ci.yml`](.github/workflows/ci.yml)
- **README Principal** : [`README.md`](README.md)

### Markers Pytest

Voir tous les markers disponibles :

```bash
pytest --markers
```

### Aide Pytest

```bash
pytest --help                    # Aide gÃ©nÃ©rale
pytest --collect-only -q         # Lister tous les tests
pytest -k "test_name" -v         # ExÃ©cuter un test spÃ©cifique
pytest --lf                      # Relancer seulement les tests Ã©chouÃ©s
pytest --sw                      # ArrÃªter au premier Ã©chec
```

---

## Questions ?

- **Issues** : [github.com/jsboigeEpita/2025-Epita-Intelligence-Symbolique/issues](https://github.com/jsboigeEpita/2025-Epita-Intelligence-Symbolique/issues)
- **Discussions** : [github.com/jsboigeEpita/2025-Epita-Intelligence-Symbolique/discussions](https://github.com/jsboigeEpita/2025-Epita-Intelligence-Symbolique/discussions)
- **Email** : Jean-Sebastien.Boige@epita.fr

---

## Remerciements

Merci de contribuer Ã  ce projet ! Votre aide est prÃ©cieuse. ğŸ™

Si vous avez des suggestions pour amÃ©liorer ce guide, n'hÃ©sitez pas Ã  ouvrir une issue ou une pull request.