#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Production Analyzer - Point d'entr√©e CLI principal pour l'analyse en production
========================================================================================

Script consolid√© int√©grant les meilleurs √©l√©ments de :
- scripts/main/analyze_text.py (CLI complet avec 20+ param√®tres)
- scripts/execution/advanced_rhetorical_analysis.py (production mature)
- scripts/main/analyze_text_authentic.py (authenticit√© 100%)

Innovations int√©gr√©es :
- Configuration LLM centralis√©e (√©limine 15+ duplications)
- M√©canisme de retry automatique pour TweetyProject
- Syst√®me de validation d'authenticit√© √† 100%
- Architecture d'orchestration multi-agents sophistiqu√©e
- TraceAnalyzer v2.0 avec conversation agentielle

Version: 1.0.0
Cr√©√©: 10/06/2025
Auteur: Roo
"""

import os
import sys
import asyncio
import logging
import argparse
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import traceback

# Ajout du r√©pertoire racine du projet au chemin
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Configuration avanc√©e du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)8s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("UnifiedProductionAnalyzer")


class LogicType(Enum):
    """Types de logique support√©s avec fallback automatique"""
    FOL = "fol"
    PL = "propositional" 
    MODAL = "modal"


class MockLevel(Enum):
    """Niveaux de simulation pour le contr√¥le d'authenticit√©"""
    NONE = "none"        # 100% authentique - d√©faut production
    PARTIAL = "partial"   # Hybride d√©veloppement
    FULL = "full"        # Test uniquement


class OrchestrationType(Enum):
    """Types d'orchestration multi-agents"""
    UNIFIED = "unified"
    CONVERSATION = "conversation"
    MICRO = "micro"
    REAL_LLM = "real_llm"


class AnalysisMode(Enum):
    """Modes d'analyse rh√©torique"""
    FALLACIES = "fallacies"
    COHERENCE = "coherence"
    SEMANTIC = "semantic"
    UNIFIED = "unified"
    ADVANCED = "advanced"


@dataclass
class UnifiedProductionConfig:
    """Configuration centralis√©e pour l'analyse en production"""
    
    # === Configuration LLM Centralis√©e ===
    llm_service: str = "openai"
    llm_model: str = "gpt-4"
    llm_temperature: float = 0.3
    llm_max_tokens: int = 2000
    llm_timeout: int = 60
    llm_retry_count: int = 3
    llm_retry_delay: float = 1.0
    
    # === Configuration Logique ===
    logic_type: LogicType = LogicType.FOL
    enable_fallback: bool = True
    fallback_order: List[LogicType] = field(default_factory=lambda: [LogicType.FOL, LogicType.PL])
    
    # === Configuration Authenticit√© ===
    mock_level: MockLevel = MockLevel.NONE
    require_real_gpt: bool = True
    require_real_tweety: bool = True
    require_full_taxonomy: bool = True
    validate_tools: bool = True
    
    # === Configuration Orchestration ===
    orchestration_type: OrchestrationType = OrchestrationType.UNIFIED
    enable_conversation_trace: bool = True
    enable_micro_orchestration: bool = False
    max_agents: int = 5
    
    # === Configuration Analyse ===
    analysis_modes: List[AnalysisMode] = field(default_factory=lambda: [AnalysisMode.UNIFIED])
    batch_size: int = 10
    enable_parallel: bool = True
    max_workers: int = 4
    
    # === Configuration Retry TweetyProject ===
    tweety_retry_count: int = 5
    tweety_retry_delay: float = 2.0
    tweety_timeout: int = 30
    tweety_memory_limit: str = "2g"
    
    # === Configuration Rapports ===
    output_format: str = "json"
    enable_detailed_logs: bool = True
    save_intermediate: bool = False
    report_level: str = "production"
    
    # === Configuration Validation ===
    validate_inputs: bool = True
    validate_outputs: bool = True
    check_dependencies: bool = True
    
    def validate(self) -> Tuple[bool, List[str]]:
        """Valide la configuration et retourne les erreurs d√©tect√©es"""
        errors = []
        
        # Validation authenticit√©
        if self.mock_level == MockLevel.NONE:
            if not self.require_real_gpt:
                errors.append("Mode production requiert require_real_gpt=True")
            if not self.require_real_tweety:
                errors.append("Mode production requiert require_real_tweety=True")
        
        # Validation retry
        if self.tweety_retry_count < 1:
            errors.append("tweety_retry_count doit √™tre >= 1")
        if self.llm_retry_count < 1:
            errors.append("llm_retry_count doit √™tre >= 1")
            
        # Validation parall√©lisme
        if self.enable_parallel and self.max_workers < 1:
            errors.append("max_workers doit √™tre >= 1 si enable_parallel=True")
            
        return len(errors) == 0, errors


class RetryMechanism:
    """M√©canisme de retry intelligent pour TweetyProject et LLM"""
    
    def __init__(self, config: UnifiedProductionConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.RetryMechanism")
        
    async def retry_with_fallback(self, operation: callable, *args, **kwargs) -> Any:
        """
        Ex√©cute une op√©ration avec retry et fallback automatique
        
        Args:
            operation: Fonction √† ex√©cuter
            *args, **kwargs: Arguments de la fonction
            
        Returns:
            R√©sultat de l'op√©ration ou exception
        """
        last_exception = None
        
        # Retry principal
        for attempt in range(self.config.tweety_retry_count):
            try:
                self.logger.debug(f"Tentative {attempt + 1}/{self.config.tweety_retry_count}")
                result = await operation(*args, **kwargs)
                if attempt > 0:
                    self.logger.info(f"‚úÖ Succ√®s apr√®s {attempt + 1} tentatives")
                return result
                
            except Exception as e:
                last_exception = e
                self.logger.warning(f"‚ö†Ô∏è Tentative {attempt + 1} √©chou√©e: {e}")
                
                if attempt < self.config.tweety_retry_count - 1:
                    delay = self.config.tweety_retry_delay * (2 ** attempt)  # Backoff exponentiel
                    self.logger.debug(f"Attente {delay}s avant retry...")
                    await asyncio.sleep(delay)
        
        # Fallback si activ√©
        if self.config.enable_fallback and hasattr(operation, '__fallback__'):
            try:
                self.logger.warning("üîÑ Activation du fallback...")
                return await operation.__fallback__(*args, **kwargs)
            except Exception as fallback_error:
                self.logger.error(f"‚ùå √âchec du fallback: {fallback_error}")
                
        # √âchec final
        self.logger.error(f"‚ùå √âchec d√©finitif apr√®s {self.config.tweety_retry_count} tentatives")
        raise last_exception


class TraceAnalyzerV2:
    """TraceAnalyzer v2.0 avec conversation agentielle avanc√©e"""
    
    def __init__(self, config: UnifiedProductionConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.TraceAnalyzerV2")
        self.conversation_history = []
        self.agent_states = {}
        
    async def initialize(self) -> bool:
        """Initialise l'analyseur de traces"""
        try:
            self.logger.info("üîß Initialisation TraceAnalyzer v2.0...")
            
            # Initialisation des √©tats d'agents
            self.agent_states = {
                "sherlock": {"status": "ready", "context": {}},
                "watson": {"status": "ready", "context": {}}, 
                "oracle": {"status": "ready", "context": {}},
                "synthesis": {"status": "ready", "context": {}}
            }
            
            self.logger.info("‚úÖ TraceAnalyzer v2.0 initialis√©")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation TraceAnalyzer: {e}")
            return False
    
    async def capture_conversation(self, agent_name: str, message: str, response: str) -> str:
        """Capture une interaction conversationnelle"""
        timestamp = datetime.now().isoformat()
        
        conversation_entry = {
            "timestamp": timestamp,
            "agent": agent_name,
            "message": message,
            "response": response,
            "context": self.agent_states.get(agent_name, {})
        }
        
        self.conversation_history.append(conversation_entry)
        
        # Mise √† jour de l'√©tat de l'agent
        if agent_name in self.agent_states:
            self.agent_states[agent_name]["last_interaction"] = timestamp
            self.agent_states[agent_name]["interaction_count"] = \
                self.agent_states[agent_name].get("interaction_count", 0) + 1
        
        self.logger.debug(f"üìù Conversation captur√©e: {agent_name} -> {len(response)} chars")
        return conversation_entry["timestamp"]
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """G√©n√®re un r√©sum√© de la conversation"""
        return {
            "total_interactions": len(self.conversation_history),
            "agents_used": list(self.agent_states.keys()),
            "conversation_duration": self._calculate_duration(),
            "agent_stats": self.agent_states
        }
    
    def _calculate_duration(self) -> float:
        """Calcule la dur√©e totale de la conversation"""
        if len(self.conversation_history) < 2:
            return 0.0
        
        start_time = datetime.fromisoformat(self.conversation_history[0]["timestamp"])
        end_time = datetime.fromisoformat(self.conversation_history[-1]["timestamp"])
        
        return (end_time - start_time).total_seconds()


class UnifiedLLMService:
    """Service LLM centralis√© avec gestion d'authenticit√©"""
    
    def __init__(self, config: UnifiedProductionConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.UnifiedLLMService")
        self.retry_mechanism = RetryMechanism(config)
        self._client = None
        
    async def initialize(self) -> bool:
        """Initialise le service LLM selon la configuration"""
        try:
            self.logger.info(f"üîß Initialisation service LLM ({self.config.llm_service})...")
            
            # Validation authenticit√©
            if self.config.mock_level == MockLevel.NONE and not self.config.require_real_gpt:
                raise ValueError("Mode production requiert un LLM authentique")
            
            # Initialisation selon le type de service
            if self.config.llm_service == "openai":
                await self._initialize_openai()
            elif self.config.llm_service == "mock":
                if self.config.mock_level == MockLevel.NONE:
                    raise ValueError("Service mock interdit en mode production")
                await self._initialize_mock()
            else:
                raise ValueError(f"Service LLM non support√©: {self.config.llm_service}")
            
            self.logger.info("‚úÖ Service LLM initialis√©")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation LLM: {e}")
            return False
    
    async def _initialize_openai(self):
        """Initialise le client OpenAI authentique"""
        try:
            import openai
            self._client = openai.AsyncOpenAI()
            
            # Test de connexion
            await self._client.models.list()
            self.logger.info("‚úÖ Client OpenAI authentique connect√©")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur connexion OpenAI: {e}")
            raise
    
    async def _initialize_mock(self):
        """Initialise un service mock pour les tests"""
        self.logger.warning("‚ö†Ô∏è Utilisation d'un service LLM mock")
        self._client = "mock_client"
    
    async def analyze_text(self, text: str, analysis_type: str = "rhetorical") -> Dict[str, Any]:
        """Analyse un texte avec retry automatique"""
        
        async def _perform_analysis():
            if self.config.mock_level == MockLevel.NONE:
                return await self._real_analysis(text, analysis_type)
            else:
                return await self._mock_analysis(text, analysis_type)
        
        return await self.retry_mechanism.retry_with_fallback(_perform_analysis)
    
    async def _real_analysis(self, text: str, analysis_type: str) -> Dict[str, Any]:
        """Analyse authentique via OpenAI"""
        prompt = self._build_prompt(text, analysis_type)
        
        response = await self._client.chat.completions.create(
            model=self.config.llm_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.config.llm_temperature,
            max_tokens=self.config.llm_max_tokens,
            timeout=self.config.llm_timeout
        )
        
        return {
            "analysis": response.choices[0].message.content,
            "model_used": self.config.llm_model,
            "tokens_used": response.usage.total_tokens,
            "authentic": True
        }
    
    async def _mock_analysis(self, text: str, analysis_type: str) -> Dict[str, Any]:
        """Analyse simul√©e pour tests"""
        await asyncio.sleep(0.1)  # Simulation latence
        return {
            "analysis": f"[MOCK] Analyse {analysis_type} de {len(text)} caract√®res",
            "model_used": "mock",
            "tokens_used": len(text) // 4,
            "authentic": False
        }
    
    def _build_prompt(self, text: str, analysis_type: str) -> str:
        """Construit le prompt d'analyse selon le type"""
        prompts = {
            "rhetorical": f"Analysez les techniques rh√©toriques dans ce texte:\n\n{text}",
            "fallacies": f"Identifiez les sophismes dans ce texte:\n\n{text}",
            "coherence": f"√âvaluez la coh√©rence logique de ce texte:\n\n{text}",
            "semantic": f"Analysez la s√©mantique de ce texte:\n\n{text}"
        }
        
        return prompts.get(analysis_type, prompts["rhetorical"])


class DependencyValidator:
    """Validateur de d√©pendances pr√©-ex√©cution"""
    
    def __init__(self, config: UnifiedProductionConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.DependencyValidator")
        
    async def validate_all(self) -> Tuple[bool, List[str]]:
        """Valide toutes les d√©pendances critiques"""
        self.logger.info("üîç Validation des d√©pendances...")
        
        errors = []
        
        # Validation packages Python
        python_deps = await self._validate_python_dependencies()
        errors.extend(python_deps)
        
        # Validation TweetyProject si requis
        if self.config.require_real_tweety:
            tweety_deps = await self._validate_tweety_dependencies()
            errors.extend(tweety_deps)
        
        # Validation LLM
        llm_deps = await self._validate_llm_dependencies()
        errors.extend(llm_deps)
        
        # Validation syst√®me
        system_deps = await self._validate_system_dependencies()
        errors.extend(system_deps)
        
        success = len(errors) == 0
        
        if success:
            self.logger.info("‚úÖ Toutes les d√©pendances valid√©es")
        else:
            self.logger.error(f"‚ùå {len(errors)} erreurs de d√©pendances d√©tect√©es")
            
        return success, errors
    
    async def _validate_python_dependencies(self) -> List[str]:
        """Valide les d√©pendances Python critiques"""
        errors = []
        critical_packages = [
            "asyncio", "pathlib", "json", "logging",
            "argparse", "datetime", "typing"
        ]
        
        for package in critical_packages:
            try:
                __import__(package)
            except ImportError:
                errors.append(f"Package Python manquant: {package}")
        
        return errors
    
    async def _validate_tweety_dependencies(self) -> List[str]:
        """Valide TweetyProject et JPype"""
        errors = []
        
        try:
            import jpype1 as jpype
            if not jpype.isJVMStarted():
                errors.append("JVM TweetyProject non d√©marr√©e")
        except ImportError:
            errors.append("JPype1 non install√© - requis pour TweetyProject")
            
        # V√©rification des JARs TweetyProject
        libs_dir = project_root / "libs" / "tweety"
        if not libs_dir.exists():
            errors.append("R√©pertoire libs/tweety manquant")
        else:
            jar_files = list(libs_dir.glob("*.jar"))
            if len(jar_files) == 0:
                errors.append("Aucun JAR TweetyProject trouv√©")
        
        return errors
    
    async def _validate_llm_dependencies(self) -> List[str]:
        """Valide les d√©pendances LLM"""
        errors = []
        
        if self.config.llm_service == "openai" and self.config.mock_level == MockLevel.NONE:
            try:
                import openai
                # V√©rification cl√© API
                if not os.getenv("OPENAI_API_KEY"):
                    errors.append("Variable OPENAI_API_KEY manquante")
            except ImportError:
                errors.append("Package openai manquant")
        
        return errors
    
    async def _validate_system_dependencies(self) -> List[str]:
        """Valide les d√©pendances syst√®me"""
        errors = []
        
        # V√©rification espace disque
        import shutil
        free_space = shutil.disk_usage(".").free
        min_space = 1024 * 1024 * 100  # 100MB minimum
        
        if free_space < min_space:
            errors.append(f"Espace disque insuffisant: {free_space // (1024*1024)}MB disponible")
        
        # V√©rification m√©moire (estimation)
        try:
            import psutil
            available_memory = psutil.virtual_memory().available
            min_memory = 1024 * 1024 * 512  # 512MB minimum
            
            if available_memory < min_memory:
                errors.append(f"M√©moire insuffisante: {available_memory // (1024*1024)}MB disponible")
        except ImportError:
            # psutil optionnel
            pass
        
        return errors


class UnifiedProductionAnalyzer:
    """Analyseur de production unifi√© - Point d'entr√©e principal"""
    
    def __init__(self, config: UnifiedProductionConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.UnifiedProductionAnalyzer")
        
        # Composants principaux
        self.llm_service = UnifiedLLMService(config)
        self.trace_analyzer = TraceAnalyzerV2(config)
        self.dependency_validator = DependencyValidator(config)
        
        # √âtat
        self.initialized = False
        self.analysis_results = []
        self.session_id = None
        
    async def initialize(self) -> bool:
        """Initialise tous les composants du syst√®me"""
        try:
            self.logger.info("üöÄ Initialisation Unified Production Analyzer...")
            
            # G√©n√©ration ID de session
            self.session_id = f"upa_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.logger.info(f"üìã Session ID: {self.session_id}")
            
            # Validation configuration
            config_valid, config_errors = self.config.validate()
            if not config_valid:
                self.logger.error("‚ùå Configuration invalide:")
                for error in config_errors:
                    self.logger.error(f"  - {error}")
                return False
            
            # Validation d√©pendances si activ√©e
            if self.config.check_dependencies:
                deps_valid, deps_errors = await self.dependency_validator.validate_all()
                if not deps_valid:
                    self.logger.error("‚ùå D√©pendances invalides:")
                    for error in deps_errors:
                        self.logger.error(f"  - {error}")
                    return False
            
            # Initialisation des composants
            if not await self.llm_service.initialize():
                return False
                
            if not await self.trace_analyzer.initialize():
                return False
            
            self.initialized = True
            self.logger.info("‚úÖ Unified Production Analyzer initialis√©")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation: {e}")
            self.logger.debug(traceback.format_exc())
            return False
    
    async def analyze_text(self, text: str, analysis_modes: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Analyse un texte selon les modes sp√©cifi√©s
        
        Args:
            text: Texte √† analyser
            analysis_modes: Modes d'analyse (par d√©faut: configuration)
            
        Returns:
            R√©sultats d'analyse structur√©s
        """
        if not self.initialized:
            raise RuntimeError("Analyseur non initialis√©")
        
        start_time = time.time()
        analysis_id = f"analysis_{len(self.analysis_results) + 1}"
        
        self.logger.info(f"üîç D√©marrage analyse: {analysis_id}")
        self.logger.debug(f"Texte: {len(text)} caract√®res")
        
        # D√©termination des modes d'analyse
        modes = analysis_modes or [mode.value for mode in self.config.analysis_modes]
        self.logger.debug(f"Modes: {modes}")
        
        try:
            results = {}
            
            # Analyse selon chaque mode
            for mode in modes:
                self.logger.debug(f"Mode: {mode}")
                
                # Capture conversation si activ√©e
                if self.config.enable_conversation_trace:
                    await self.trace_analyzer.capture_conversation(
                        "analyzer", f"Starting {mode} analysis", f"Processing {len(text)} chars"
                    )
                
                # Analyse LLM
                mode_result = await self.llm_service.analyze_text(text, mode)
                results[mode] = mode_result
                
                self.logger.debug(f"‚úÖ Mode {mode} termin√©")
            
            # Synth√®se des r√©sultats
            analysis_result = {
                "id": analysis_id,
                "session_id": self.session_id,
                "timestamp": datetime.now().isoformat(),
                "text_length": len(text),
                "modes_analyzed": modes,
                "results": results,
                "execution_time": time.time() - start_time,
                "config_snapshot": {
                    "logic_type": self.config.logic_type.value,
                    "mock_level": self.config.mock_level.value,
                    "orchestration_type": self.config.orchestration_type.value
                }
            }
            
            # Conversation summary si activ√©
            if self.config.enable_conversation_trace:
                analysis_result["conversation_summary"] = self.trace_analyzer.get_conversation_summary()
            
            # Stockage r√©sultat
            self.analysis_results.append(analysis_result)
            
            self.logger.info(f"‚úÖ Analyse {analysis_id} termin√©e en {analysis_result['execution_time']:.2f}s")
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur analyse {analysis_id}: {e}")
            self.logger.debug(traceback.format_exc())
            
            # R√©sultat d'erreur
            error_result = {
                "id": analysis_id,
                "session_id": self.session_id,
                "timestamp": datetime.now().isoformat(),
                "error": str(e),
                "execution_time": time.time() - start_time
            }
            
            self.analysis_results.append(error_result)
            raise
    
    async def analyze_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """Analyse un lot de textes en parall√®le ou s√©quentiel"""
        self.logger.info(f"üì¶ Analyse batch: {len(texts)} textes")
        
        if self.config.enable_parallel:
            return await self._analyze_batch_parallel(texts)
        else:
            return await self._analyze_batch_sequential(texts)
    
    async def _analyze_batch_parallel(self, texts: List[str]) -> List[Dict[str, Any]]:
        """Analyse en parall√®le avec contr√¥le de concurrence"""
        semaphore = asyncio.Semaphore(self.config.max_workers)
        
        async def analyze_with_semaphore(text):
            async with semaphore:
                return await self.analyze_text(text)
        
        tasks = [analyze_with_semaphore(text) for text in texts]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Conversion des exceptions en r√©sultats d'erreur
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "id": f"batch_error_{i}",
                    "error": str(result),
                    "text_index": i
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _analyze_batch_sequential(self, texts: List[str]) -> List[Dict[str, Any]]:
        """Analyse s√©quentielle avec barre de progression"""
        results = []
        
        for i, text in enumerate(texts):
            self.logger.info(f"üìÑ Texte {i+1}/{len(texts)}")
            try:
                result = await self.analyze_text(text)
                results.append(result)
            except Exception as e:
                self.logger.error(f"‚ùå Erreur texte {i+1}: {e}")
                results.append({
                    "id": f"sequential_error_{i}",
                    "error": str(e),
                    "text_index": i
                })
        
        return results
    
    def generate_report(self, output_path: Optional[Path] = None) -> Dict[str, Any]:
        """G√©n√®re un rapport complet de session"""
        
        report = {
            "session_info": {
                "id": self.session_id,
                "timestamp": datetime.now().isoformat(),
                "total_analyses": len(self.analysis_results),
                "config": {
                    "logic_type": self.config.logic_type.value,
                    "mock_level": self.config.mock_level.value,
                    "orchestration_type": self.config.orchestration_type.value,
                    "analysis_modes": [mode.value for mode in self.config.analysis_modes]
                }
            },
            "results_summary": {
                "successful_analyses": len([r for r in self.analysis_results if "error" not in r]),
                "failed_analyses": len([r for r in self.analysis_results if "error" in r]),
                "total_execution_time": sum(r.get("execution_time", 0) for r in self.analysis_results),
                "average_execution_time": None
            },
            "detailed_results": self.analysis_results
        }
        
        # Calcul moyenne si analyses r√©ussies
        successful_count = report["results_summary"]["successful_analyses"]
        if successful_count > 0:
            report["results_summary"]["average_execution_time"] = \
                report["results_summary"]["total_execution_time"] / successful_count
        
        # Sauvegarde si chemin sp√©cifi√©
        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.logger.info(f"üìÑ Rapport sauvegard√©: {output_path}")
        
        return report


def create_cli_parser() -> argparse.ArgumentParser:
    """Cr√©e le parser CLI avec tous les param√®tres essentiels"""
    
    parser = argparse.ArgumentParser(
        description="Unified Production Analyzer - Analyse rh√©torique en production",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # === Arguments principaux ===
    parser.add_argument(
        "input",
        nargs="?",
        help="Texte √† analyser ou chemin vers fichier/dossier"
    )
    
    parser.add_argument(
        "--batch",
        action="store_true",
        help="Mode batch pour analyse multiple"
    )
    
    # === Configuration LLM ===
    llm_group = parser.add_argument_group("Configuration LLM")
    llm_group.add_argument(
        "--llm-service",
        choices=["openai", "mock"],
        default="openai",
        help="Service LLM √† utiliser"
    )
    llm_group.add_argument(
        "--llm-model",
        default="gpt-4",
        help="Mod√®le LLM sp√©cifique"
    )
    llm_group.add_argument(
        "--llm-temperature",
        type=float,
        default=0.3,
        help="Temp√©rature du mod√®le LLM"
    )
    llm_group.add_argument(
        "--llm-max-tokens",
        type=int,
        default=2000,
        help="Nombre maximum de tokens"
    )
    
    # === Configuration Logique ===
    logic_group = parser.add_argument_group("Configuration Logique")
    logic_group.add_argument(
        "--logic-type",
        choices=["fol", "propositional", "modal"],
        default="fol",
        help="Type de logique √† utiliser"
    )
    logic_group.add_argument(
        "--enable-fallback",
        action="store_true",
        default=True,
        help="Activer fallback automatique FOL->PL"
    )
    logic_group.add_argument(
        "--no-fallback",
        action="store_false",
        dest="enable_fallback",
        help="D√©sactiver fallback automatique"
    )
    
    # === Configuration Authenticit√© ===
    auth_group = parser.add_argument_group("Configuration Authenticit√©")
    auth_group.add_argument(
        "--mock-level",
        choices=["none", "partial", "full"],
        default="none",
        help="Niveau de simulation (none=100%% authentique)"
    )
    auth_group.add_argument(
        "--require-real-gpt",
        action="store_true",
        default=True,
        help="Exiger un LLM authentique"
    )
    auth_group.add_argument(
        "--allow-mock-gpt",
        action="store_false",
        dest="require_real_gpt",
        help="Autoriser LLM simul√©"
    )
    auth_group.add_argument(
        "--require-real-tweety",
        action="store_true",
        default=True,
        help="Exiger TweetyProject authentique"
    )
    auth_group.add_argument(
        "--allow-mock-tweety",
        action="store_false", 
        dest="require_real_tweety",
        help="Autoriser TweetyProject simul√©"
    )
    
    # === Configuration Orchestration ===
    orch_group = parser.add_argument_group("Configuration Orchestration")
    orch_group.add_argument(
        "--orchestration-type",
        choices=["unified", "conversation", "micro", "real_llm"],
        default="unified",
        help="Type d'orchestration multi-agents"
    )
    orch_group.add_argument(
        "--enable-conversation-trace",
        action="store_true",
        default=True,
        help="Activer capture conversation agentielle"
    )
    orch_group.add_argument(
        "--no-conversation-trace",
        action="store_false",
        dest="enable_conversation_trace",
        help="D√©sactiver capture conversation"
    )
    orch_group.add_argument(
        "--max-agents",
        type=int,
        default=5,
        help="Nombre maximum d'agents simultan√©s"
    )
    
    # === Configuration Analyse ===
    analysis_group = parser.add_argument_group("Configuration Analyse")
    analysis_group.add_argument(
        "--analysis-modes",
        nargs="+",
        choices=["fallacies", "coherence", "semantic", "unified", "advanced"],
        default=["unified"],
        help="Modes d'analyse √† appliquer"
    )
    analysis_group.add_argument(
        "--enable-parallel",
        action="store_true",
        default=True,
        help="Activer traitement parall√®le"
    )
    analysis_group.add_argument(
        "--no-parallel",
        action="store_false",
        dest="enable_parallel",
        help="Forcer traitement s√©quentiel"
    )
    analysis_group.add_argument(
        "--max-workers",
        type=int,
        default=4,
        help="Nombre maximum de workers parall√®les"
    )
    
    # === Configuration Retry ===
    retry_group = parser.add_argument_group("Configuration Retry")
    retry_group.add_argument(
        "--tweety-retry-count",
        type=int,
        default=5,
        help="Nombre de tentatives TweetyProject"
    )
    retry_group.add_argument(
        "--tweety-retry-delay",
        type=float,
        default=2.0,
        help="D√©lai entre tentatives TweetyProject"
    )
    retry_group.add_argument(
        "--llm-retry-count",
        type=int,
        default=3,
        help="Nombre de tentatives LLM"
    )
    
    # === Configuration Sortie ===
    output_group = parser.add_argument_group("Configuration Sortie")
    output_group.add_argument(
        "--output-format",
        choices=["json", "yaml", "txt"],
        default="json",
        help="Format de sortie des r√©sultats"
    )
    output_group.add_argument(
        "--output-file",
        type=Path,
        help="Fichier de sortie (d√©faut: auto-g√©n√©r√©)"
    )
    output_group.add_argument(
        "--save-intermediate",
        action="store_true",
        help="Sauvegarder r√©sultats interm√©diaires"
    )
    output_group.add_argument(
        "--report-level",
        choices=["minimal", "standard", "production", "debug"],
        default="production",
        help="Niveau de d√©tail des rapports"
    )
    
    # === Configuration Validation ===
    validation_group = parser.add_argument_group("Configuration Validation")
    validation_group.add_argument(
        "--validate-inputs",
        action="store_true",
        default=True,
        help="Valider les entr√©es"
    )
    validation_group.add_argument(
        "--no-validate-inputs",
        action="store_false",
        dest="validate_inputs",
        help="D√©sactiver validation entr√©es"
    )
    validation_group.add_argument(
        "--check-dependencies",
        action="store_true",
        default=True,
        help="V√©rifier les d√©pendances au d√©marrage"
    )
    validation_group.add_argument(
        "--no-check-dependencies",
        action="store_false",
        dest="check_dependencies",
        help="Ignorer v√©rification d√©pendances"
    )
    
    # === Options g√©n√©rales ===
    general_group = parser.add_argument_group("Options G√©n√©rales")
    general_group.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Mode verbeux"
    )
    general_group.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="Mode silencieux"
    )
    general_group.add_argument(
        "--config-file",
        type=Path,
        help="Fichier de configuration JSON"
    )
    general_group.add_argument(
        "--version",
        action="version",
        version="Unified Production Analyzer 1.0.0"
    )
    
    return parser


def load_config_from_file(config_path: Path) -> Dict[str, Any]:
    """Charge la configuration depuis un fichier JSON"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"‚ùå Erreur chargement configuration {config_path}: {e}")
        return {}


def create_config_from_args(args) -> UnifiedProductionConfig:
    """Cr√©e la configuration depuis les arguments CLI"""
    
    # Chargement config fichier si sp√©cifi√©
    file_config = {}
    if args.config_file and args.config_file.exists():
        file_config = load_config_from_file(args.config_file)
        logger.info(f"üìÑ Configuration charg√©e depuis {args.config_file}")
    
    # Mapping des enums
    logic_type_map = {
        "fol": LogicType.FOL,
        "propositional": LogicType.PL,
        "modal": LogicType.MODAL
    }
    
    mock_level_map = {
        "none": MockLevel.NONE,
        "partial": MockLevel.PARTIAL,
        "full": MockLevel.FULL
    }
    
    orchestration_type_map = {
        "unified": OrchestrationType.UNIFIED,
        "conversation": OrchestrationType.CONVERSATION,
        "micro": OrchestrationType.MICRO,
        "real_llm": OrchestrationType.REAL_LLM
    }
    
    analysis_modes_map = {
        "fallacies": AnalysisMode.FALLACIES,
        "coherence": AnalysisMode.COHERENCE,
        "semantic": AnalysisMode.SEMANTIC,
        "unified": AnalysisMode.UNIFIED,
        "advanced": AnalysisMode.ADVANCED
    }
    
    # Construction configuration (CLI prioritaire sur fichier)
    config_dict = {
        # LLM
        "llm_service": getattr(args, "llm_service", file_config.get("llm_service", "openai")),
        "llm_model": getattr(args, "llm_model", file_config.get("llm_model", "gpt-4")),
        "llm_temperature": getattr(args, "llm_temperature", file_config.get("llm_temperature", 0.3)),
        "llm_max_tokens": getattr(args, "llm_max_tokens", file_config.get("llm_max_tokens", 2000)),
        "llm_retry_count": getattr(args, "llm_retry_count", file_config.get("llm_retry_count", 3)),
        
        # Logique
        "logic_type": logic_type_map.get(
            getattr(args, "logic_type", file_config.get("logic_type", "fol")),
            LogicType.FOL
        ),
        "enable_fallback": getattr(args, "enable_fallback", file_config.get("enable_fallback", True)),
        
        # Authenticit√©
        "mock_level": mock_level_map.get(
            getattr(args, "mock_level", file_config.get("mock_level", "none")),
            MockLevel.NONE
        ),
        "require_real_gpt": getattr(args, "require_real_gpt", file_config.get("require_real_gpt", True)),
        "require_real_tweety": getattr(args, "require_real_tweety", file_config.get("require_real_tweety", True)),
        
        # Orchestration
        "orchestration_type": orchestration_type_map.get(
            getattr(args, "orchestration_type", file_config.get("orchestration_type", "unified")),
            OrchestrationType.UNIFIED
        ),
        "enable_conversation_trace": getattr(args, "enable_conversation_trace", 
                                           file_config.get("enable_conversation_trace", True)),
        "max_agents": getattr(args, "max_agents", file_config.get("max_agents", 5)),
        
        # Analyse
        "analysis_modes": [
            analysis_modes_map.get(mode, AnalysisMode.UNIFIED) 
            for mode in getattr(args, "analysis_modes", file_config.get("analysis_modes", ["unified"]))
        ],
        "enable_parallel": getattr(args, "enable_parallel", file_config.get("enable_parallel", True)),
        "max_workers": getattr(args, "max_workers", file_config.get("max_workers", 4)),
        
        # Retry
        "tweety_retry_count": getattr(args, "tweety_retry_count", file_config.get("tweety_retry_count", 5)),
        "tweety_retry_delay": getattr(args, "tweety_retry_delay", file_config.get("tweety_retry_delay", 2.0)),
        
        # Sortie
        "output_format": getattr(args, "output_format", file_config.get("output_format", "json")),
        "save_intermediate": getattr(args, "save_intermediate", file_config.get("save_intermediate", False)),
        "report_level": getattr(args, "report_level", file_config.get("report_level", "production")),
        
        # Validation
        "validate_inputs": getattr(args, "validate_inputs", file_config.get("validate_inputs", True)),
        "check_dependencies": getattr(args, "check_dependencies", file_config.get("check_dependencies", True))
    }
    
    return UnifiedProductionConfig(**config_dict)


async def main():
    """Fonction principale du script"""
    
    # Configuration logging selon verbosit√©
    parser = create_cli_parser()
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    elif args.quiet:
        logging.getLogger().setLevel(logging.WARNING)
    
    logger.info("üöÄ Unified Production Analyzer v1.0.0")
    logger.info("=" * 60)
    
    try:
        # Cr√©ation configuration
        config = create_config_from_args(args)
        logger.info(f"‚öôÔ∏è Configuration: {config.logic_type.value}/{config.mock_level.value}")
        
        # Validation authenticit√© mode production
        if config.mock_level == MockLevel.NONE:
            logger.info("üõ°Ô∏è Mode production authentique activ√© (0% mocks)")
        else:
            logger.warning(f"‚ö†Ô∏è Mode {config.mock_level.value} - Non recommand√© en production")
        
        # Initialisation analyseur
        analyzer = UnifiedProductionAnalyzer(config)
        
        if not await analyzer.initialize():
            logger.error("‚ùå √âchec initialisation - Arr√™t")
            sys.exit(1)
        
        # D√©termination de l'entr√©e
        if not args.input:
            logger.error("‚ùå Aucune entr√©e sp√©cifi√©e")
            parser.print_help()
            sys.exit(1)
        
        input_path = Path(args.input)
        
        # Traitement selon le type d'entr√©e
        if input_path.exists():
            if input_path.is_file():
                # Fichier unique
                logger.info(f"üìÑ Analyse fichier: {input_path}")
                with open(input_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                result = await analyzer.analyze_text(text, args.analysis_modes)
                logger.info(f"‚úÖ Analyse termin√©e: {result['id']}")
                
            elif input_path.is_dir() and args.batch:
                # Dossier en mode batch
                logger.info(f"üìÅ Analyse dossier: {input_path}")
                
                text_files = list(input_path.glob("*.txt")) + list(input_path.glob("*.md"))
                texts = []
                
                for file_path in text_files:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        texts.append(f.read())
                
                results = await analyzer.analyze_batch(texts)
                logger.info(f"‚úÖ Batch termin√©: {len(results)} analyses")
                
            else:
                logger.error("‚ùå Dossier sp√©cifi√© mais mode batch non activ√© (--batch)")
                sys.exit(1)
        else:
            # Texte direct
            logger.info("üìù Analyse texte direct")
            result = await analyzer.analyze_text(args.input, args.analysis_modes)
            logger.info(f"‚úÖ Analyse termin√©e: {result['id']}")
        
        # G√©n√©ration rapport final
        output_file = args.output_file
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = Path(f"reports/unified_analysis_{timestamp}.json")
        
        report = analyzer.generate_report(output_file)
        
        # R√©sum√© final
        logger.info("=" * 60)
        logger.info("üìä R√âSUM√â FINAL")
        logger.info(f"Session: {analyzer.session_id}")
        logger.info(f"Analyses r√©ussies: {report['results_summary']['successful_analyses']}")
        logger.info(f"Analyses √©chou√©es: {report['results_summary']['failed_analyses']}")
        logger.info(f"Temps total: {report['results_summary']['total_execution_time']:.2f}s")
        
        if report['results_summary']['average_execution_time']:
            logger.info(f"Temps moyen: {report['results_summary']['average_execution_time']:.2f}s")
        
        logger.info(f"Rapport: {output_file}")
        logger.info("üéâ Analyse termin√©e avec succ√®s!")
        
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è Interruption utilisateur")
        sys.exit(130)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        logger.debug(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    # Gestion de l'event loop selon la plateforme
    try:
        asyncio.run(main())
    except Exception as e:
        logger.error(f"‚ùå Erreur event loop: {e}")
        sys.exit(1)