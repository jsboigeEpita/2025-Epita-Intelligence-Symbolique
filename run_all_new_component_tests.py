#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Orchestrateur Master de Validation des Nouveaux Composants
========================================================

Script principal pour ex√©cuter tous les nouveaux tests cr√©√©s et g√©n√©rer
un rapport consolid√© de validation du syst√®me complet.

Usage:
    python run_all_new_component_tests.py --authentic --verbose
    python run_all_new_component_tests.py --fast
    python run_all_new_component_tests.py --component TweetyErrorAnalyzer
"""

import argparse
import subprocess
import sys
import time
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import traceback

# Configuration de l'encodage pour Windows
if sys.platform == "win32":
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    # Tentative de forcer UTF-8 sur stdout
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except:
        pass

# Configuration des couleurs pour output console
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    PURPLE = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    END = '\033[0m'

@dataclass
class TestSuiteResult:
    """R√©sultat d'ex√©cution d'une suite de tests."""
    name: str
    files: List[str]
    passed: int = 0
    failed: int = 0
    skipped: int = 0
    errors: int = 0
    duration: float = 0.0
    success: bool = False
    error_messages: List[str] = None
    coverage: Optional[float] = None
    authenticity_level: str = "UNKNOWN"
    
    def __post_init__(self):
        if self.error_messages is None:
            self.error_messages = []
    
    @property
    def total_tests(self) -> int:
        return self.passed + self.failed + self.skipped + self.errors
    
    @property
    def pass_rate(self) -> float:
        if self.total_tests == 0:
            return 0.0
        return (self.passed / self.total_tests) * 100

@dataclass
class ValidationReport:
    """Rapport consolid√© de validation."""
    timestamp: str
    total_duration: float
    suite_results: List[TestSuiteResult]
    global_status: str
    authenticity_level: str
    recommendations: List[str]
    
    @property
    def total_tests(self) -> int:
        return sum(r.total_tests for r in self.suite_results)
    
    @property
    def total_passed(self) -> int:
        return sum(r.passed for r in self.suite_results)
    
    @property
    def total_failed(self) -> int:
        return sum(r.failed for r in self.suite_results)
    
    @property
    def global_pass_rate(self) -> float:
        if self.total_tests == 0:
            return 0.0
        return (self.total_passed / self.total_tests) * 100

class TestSuiteRunner:
    """Ex√©cuteur de suites de tests avec validation."""
    
    # Configuration des suites de tests
    TEST_SUITES = {
        "TweetyErrorAnalyzer": {
            "files": ["tests/unit/argumentation_analysis/test_tweety_error_analyzer.py"],
            "description": "Analyseur d'erreurs Tweety avec feedback BNF",
            "expected_tests": 21,
            "fast": True,
            "authenticity_required": False
        },
        "UnifiedConfig": {
            "files": [
                "tests/unit/config/test_unified_config.py",
                "tests/unit/scripts/test_configuration_cli.py", 
                "tests/unit/integration/test_unified_config_integration.py"
            ],
            "description": "Syst√®me de configuration unifi√©",
            "expected_tests": 12,
            "fast": True,
            "authenticity_required": False
        },
        "FirstOrderLogicAgent": {
            "files": ["tests/unit/agents/test_fol_logic_agent.py"],
            "description": "Agent logique premier ordre avec int√©gration Tweety",
            "expected_tests": 25,
            "fast": True,
            "authenticity_required": True
        },
        "AuthenticitySystem": {
            "files": [
                "tests/unit/authentication/test_mock_elimination_advanced.py",
                "tests/integration/test_authentic_components.py",
                "tests/unit/authentication/test_cli_authentic_commands.py"
            ],
            "description": "Syst√®me d'authenticit√© et √©limination des mocks",
            "expected_tests": 17,
            "fast": False,
            "authenticity_required": True
        },
        "UnifiedOrchestrations": {
            "files": [
                "tests/unit/orchestration/test_unified_orchestrations.py",
                "tests/integration/test_unified_system_integration.py"
            ],
            "description": "Orchestrations syst√®me unifi√©es",
            "expected_tests": 8,
            "fast": False,
            "authenticity_required": True
        }
    }
    
    def __init__(self, authentic: bool = False, verbose: bool = False):
        self.authentic = authentic
        self.verbose = verbose
        self.project_root = Path(__file__).parent
        
    def _check_prerequisites(self) -> Tuple[bool, List[str]]:
        """V√©rifie les pr√©requis syst√®me."""
        issues = []
        
        # V√©rification Python et pytest
        try:
            result = subprocess.run([sys.executable, "-m", "pytest", "--version"], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode != 0:
                issues.append("pytest non disponible")
        except Exception:
            issues.append("pytest non disponible")
        
        # V√©rification configuration unifi√©e
        config_file = self.project_root / "config" / "unified_config.py"
        if not config_file.exists():
            issues.append("Configuration unifi√©e manquante")
        
        # V√©rifications authentiques si requises
        if self.authentic:
            # V√©rification cl√© API OpenAI
            import os
            if not os.getenv("OPENAI_API_KEY"):
                issues.append("Cl√© API OpenAI manquante")
            
            # V√©rification JAR Tweety
            tweety_jar = self.project_root / "libs" / "tweety.jar"
            if not tweety_jar.exists():
                issues.append("JAR Tweety manquant")
            
            # V√©rification taxonomie sophismes
            taxonomy_dir = self.project_root / "config" / "taxonomies"
            if not taxonomy_dir.exists():
                issues.append("Taxonomie sophismes manquante")
        
        return len(issues) == 0, issues
    
    def _run_pytest_command(self, files: List[str], suite_name: str) -> Tuple[TestSuiteResult, str]:
        """Ex√©cute une commande pytest et parse les r√©sultats."""
        start_time = time.time()
        
        # Construction de la commande pytest
        cmd = [
            sys.executable, "-m", "pytest",
            "-v",
            "--tb=short",
            "--no-header"
        ]
        
        # Ajout de la couverture si authentique
        if self.authentic:
            cmd.extend(["--cov=argumentation_analysis", "--cov-report=term-missing"])
        
        # Ajout des fichiers
        existing_files = []
        for file_path in files:
            full_path = self.project_root / file_path
            if full_path.exists():
                existing_files.append(str(full_path))
            else:
                print(f"‚ö†Ô∏è  Fichier manquant: {file_path}")
        
        if not existing_files:
            duration = time.time() - start_time
            return TestSuiteResult(
                name=suite_name,
                files=files,
                success=False,
                duration=duration,
                error_messages=["Aucun fichier de test trouv√©"]
            ), ""
        
        cmd.extend(existing_files)
        
        # Ex√©cution
        try:
            if self.verbose:
                print(f"üîÑ Ex√©cution: {' '.join(cmd)}")
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,  # 5 minutes max
                cwd=str(self.project_root),
                encoding='utf-8',
                errors='replace'  # Remplace les caract√®res non d√©codables
            )
            
            duration = time.time() - start_time
            # Gestion s√©curis√©e des outputs None
            stdout = result.stdout or ""
            stderr = result.stderr or ""
            output = stdout + stderr
            
            # Parse des r√©sultats pytest
            return self._parse_pytest_output(
                output, result.returncode, suite_name, files, duration
            ), output
            
        except subprocess.TimeoutExpired:
            duration = time.time() - start_time
            return TestSuiteResult(
                name=suite_name,
                files=files,
                success=False,
                duration=duration,
                error_messages=["Timeout lors de l'ex√©cution des tests"]
            ), "TIMEOUT"
        except Exception as e:
            duration = time.time() - start_time
            return TestSuiteResult(
                name=suite_name,
                files=files,
                success=False,
                duration=duration,
                error_messages=[f"Erreur d'ex√©cution: {str(e)}"]
            ), str(e)
    
    def _parse_pytest_output(self, output: str, return_code: int, 
                           suite_name: str, files: List[str], duration: float) -> TestSuiteResult:
        """Parse la sortie de pytest pour extraire les m√©triques."""
        result = TestSuiteResult(name=suite_name, files=files, duration=duration)
        
        # Recherche des statistiques finales pytest
        lines = output.split('\n')
        for line in lines:
            line = line.strip()
            
            # Format: "= 21 passed in 0.09s ="
            if " passed " in line and " in " in line:
                try:
                    parts = line.split()
                    for i, part in enumerate(parts):
                        if part == "passed" and i > 0:
                            result.passed = int(parts[i-1])
                        elif part == "failed" and i > 0:
                            result.failed = int(parts[i-1])
                        elif part == "skipped" and i > 0:
                            result.skipped = int(parts[i-1])
                        elif part == "error" and i > 0:
                            result.errors = int(parts[i-1])
                except (ValueError, IndexError):
                    pass
            
            # Recherche des erreurs
            if "FAILED" in line or "ERROR" in line:
                result.error_messages.append(line)
            
            # Couverture si disponible
            if "Total coverage:" in line:
                try:
                    coverage_str = line.split(":")[-1].strip().rstrip('%')
                    result.coverage = float(coverage_str)
                except ValueError:
                    pass
        
        # D√©termination du succ√®s
        result.success = (return_code == 0 and result.failed == 0 and result.errors == 0)
        
        # Niveau d'authenticit√©
        if self.authentic and result.success:
            result.authenticity_level = "AUTHENTIC"
        elif result.success:
            result.authenticity_level = "MOCK"
        else:
            result.authenticity_level = "FAILED"
        
        return result
    
    def run_suite(self, suite_name: str) -> Tuple[TestSuiteResult, str]:
        """Ex√©cute une suite de tests sp√©cifique."""
        if suite_name not in self.TEST_SUITES:
            raise ValueError(f"Suite inconnue: {suite_name}")
        
        suite_config = self.TEST_SUITES[suite_name]
        
        # V√©rification des pr√©requis d'authenticit√©
        if suite_config["authenticity_required"] and not self.authentic:
            print(f"‚ö†Ô∏è  {suite_name}: Mode authentique recommand√©")
        
        print(f"\n{'='*60}")
        print(f"üß™ EX√âCUTION: {suite_name}")
        print(f"üìù {suite_config['description']}")
        print(f"üìÅ Fichiers: {len(suite_config['files'])}")
        print(f"üéØ Tests attendus: {suite_config['expected_tests']}")
        print(f"{'='*60}")
        
        return self._run_pytest_command(suite_config["files"], suite_name)
    
    def run_all_suites(self, component_filter: Optional[str] = None, 
                      fast_only: bool = False) -> ValidationReport:
        """Ex√©cute toutes les suites de tests."""
        start_time = time.time()
        
        print(f"\n{Colors.BOLD}{Colors.BLUE}[TEST] VALIDATION COMPLETE DES NOUVEAUX COMPOSANTS{Colors.END}")
        print(f"{Colors.BLUE}{'='*60}{Colors.END}")
        print(f"[TIME] Debut: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"[MODE] Mode: {'Authentique' if self.authentic else 'Mock'}")
        print(f"[SPEED] Rapide: {'Oui' if fast_only else 'Non'}")
        
        # V√©rification des pr√©requis
        prereq_ok, issues = self._check_prerequisites()
        if not prereq_ok:
            print(f"\n{Colors.RED}[ERROR] PREREQUIS MANQUANTS:{Colors.END}")
            for issue in issues:
                print(f"  - {issue}")
            if not self.authentic:
                print(f"{Colors.YELLOW}[INFO] Passage en mode degrade{Colors.END}")
            else:
                print(f"{Colors.RED}[ERROR] Impossible de continuer en mode authentique{Colors.END}")
                sys.exit(1)
        
        # S√©lection des suites
        suites_to_run = []
        for suite_name, config in self.TEST_SUITES.items():
            # Filtrage par composant
            if component_filter and suite_name != component_filter:
                continue
            
            # Filtrage mode rapide
            if fast_only and not config["fast"]:
                continue
            
            suites_to_run.append(suite_name)
        
        print(f"\n[LIST] Suites selectionnees: {len(suites_to_run)}")
        for suite in suites_to_run:
            print(f"  - {suite}")
        
        # Ex√©cution des suites
        results = []
        outputs = {}
        
        for i, suite_name in enumerate(suites_to_run, 1):
            print(f"\n{Colors.CYAN}[{i}/{len(suites_to_run)}]{Colors.END}")
            
            try:
                result, output = self.run_suite(suite_name)
                results.append(result)
                outputs[suite_name] = output
                
                # Affichage du r√©sultat
                status_icon = "[OK]" if result.success else "[FAIL]"
                color = Colors.GREEN if result.success else Colors.RED
                
                print(f"{color}{status_icon} {suite_name}: {result.passed}/{result.total_tests} tests ({result.pass_rate:.1f}%) - {result.duration:.2f}s{Colors.END}")
                
                if not result.success and self.verbose:
                    print(f"{Colors.RED}Erreurs:{Colors.END}")
                    for error in result.error_messages[:3]:  # Max 3 erreurs
                        print(f"  {error}")
                
            except Exception as e:
                error_result = TestSuiteResult(
                    name=suite_name,
                    files=self.TEST_SUITES[suite_name]["files"],
                    success=False,
                    duration=0.0,
                    error_messages=[f"Exception: {str(e)}"]
                )
                results.append(error_result)
                outputs[suite_name] = traceback.format_exc()
                
                print(f"{Colors.RED}[EXCEPTION] {suite_name}: {str(e)}{Colors.END}")
        
        # G√©n√©ration du rapport
        total_duration = time.time() - start_time
        
        # D√©termination du statut global
        all_passed = all(r.success for r in results)
        global_status = "[OK] PRODUCTION READY" if all_passed else "[FAIL] ISSUES DETECTEES"
        
        # Niveau d'authenticit√© global
        if self.authentic and all_passed:
            authenticity_level = "100% AUTHENTIQUE"
        elif all_passed:
            authenticity_level = "MOCK VALID√â"
        else:
            authenticity_level = "√âCHEC PARTIEL"
        
        # Recommandations
        recommendations = []
        if not all_passed:
            failed_suites = [r.name for r in results if not r.success]
            recommendations.append(f"Corriger les erreurs dans: {', '.join(failed_suites)}")
        
        if not self.authentic:
            recommendations.append("Ex√©cuter en mode authentique pour validation compl√®te")
        
        for result in results:
            if result.total_tests < self.TEST_SUITES[result.name]["expected_tests"]:
                recommendations.append(f"{result.name}: Tests manquants d√©tect√©s")
        
        report = ValidationReport(
            timestamp=datetime.now().isoformat(),
            total_duration=total_duration,
            suite_results=results,
            global_status=global_status,
            authenticity_level=authenticity_level,
            recommendations=recommendations
        )
        
        # Affichage du r√©sum√©
        self._print_summary(report)
        
        # Sauvegarde des outputs d√©taill√©s si verbose
        if self.verbose:
            self._save_detailed_outputs(outputs, report)
        
        return report
    
    def _print_summary(self, report: ValidationReport):
        """Affiche le r√©sum√© consolid√©."""
        print(f"\n{Colors.BOLD}{Colors.BLUE}[SUMMARY] RESUME GLOBAL{Colors.END}")
        print(f"{Colors.BLUE}{'='*50}{Colors.END}")
        
        # R√©sultats par suite
        for result in report.suite_results:
            status_icon = "[OK]" if result.success else "[FAIL]"
            color = Colors.GREEN if result.success else Colors.RED
            print(f"{color}{status_icon} {result.name:<20} : {result.passed:>2}/{result.total_tests:<2} tests ({result.pass_rate:>5.1f}%) - {result.duration:>5.2f}s{Colors.END}")
        
        print(f"\n{Colors.BOLD}[METRICS] METRIQUES GLOBALES{Colors.END}")
        print(f"- Total tests : {report.total_passed}/{report.total_tests} ({report.global_pass_rate:.1f}%)")
        print(f"- Temps total : {report.total_duration:.2f}s")
        print(f"- Authenticit√© : {report.authenticity_level}")
        
        # Statut final
        if "[OK]" in report.global_status:
            print(f"\n{Colors.GREEN}{Colors.BOLD}[SUCCESS] {report.global_status}{Colors.END}")
        else:
            print(f"\n{Colors.RED}{Colors.BOLD}[WARNING] {report.global_status}{Colors.END}")
        
        # Recommandations
        if report.recommendations:
            print(f"\n{Colors.YELLOW}[RECOMMENDATIONS] RECOMMANDATIONS{Colors.END}")
            for i, rec in enumerate(report.recommendations, 1):
                print(f"{i}. {rec}")
    
    def _save_detailed_outputs(self, outputs: Dict[str, str], report: ValidationReport):
        """Sauvegarde les outputs d√©taill√©s."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Outputs raw
        output_file = self.project_root / f"validation_outputs_{timestamp}.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"VALIDATION OUTPUTS - {report.timestamp}\n")
            f.write("="*80 + "\n\n")
            
            for suite_name, output in outputs.items():
                f.write(f"SUITE: {suite_name}\n")
                f.write("-"*40 + "\n")
                # Gestion s√©curis√©e des outputs None
                if output is not None:
                    f.write(output)
                else:
                    f.write("ERREUR: Output manquant ou None")
                f.write("\n\n")
        
        print(f"\n[SAVE] Outputs detailles sauves: {output_file}")
    
    def save_report(self, report: ValidationReport, output_file: Optional[str] = None):
        """Sauvegarde le rapport en JSON."""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"validation_report_{timestamp}.json"
        
        output_path = self.project_root / output_file
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(report), f, indent=2, ensure_ascii=False)
        
        print(f"\n[SAVE] Rapport JSON sauve: {output_path}")
        return output_path

def main():
    """Point d'entr√©e principal."""
    parser = argparse.ArgumentParser(
        description="Orchestrateur Master de Validation des Nouveaux Composants",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'usage:
  python run_all_new_component_tests.py --authentic --verbose
  python run_all_new_component_tests.py --fast
  python run_all_new_component_tests.py --component TweetyErrorAnalyzer
  python run_all_new_component_tests.py --report validation_report.json
        """
    )
    
    parser.add_argument(
        "--authentic", 
        action="store_true",
        help="Mode authentique (composants r√©els, API, etc.)"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true", 
        help="Affichage d√©taill√©"
    )
    
    parser.add_argument(
        "--fast",
        action="store_true",
        help="Tests unitaires rapides seulement"
    )
    
    parser.add_argument(
        "--component",
        choices=list(TestSuiteRunner.TEST_SUITES.keys()),
        help="Ex√©cuter un composant sp√©cifique"
    )
    
    parser.add_argument(
        "--level",
        choices=["unit", "integration", "all"],
        default="all",
        help="Niveau de tests √† ex√©cuter"
    )
    
    parser.add_argument(
        "--report",
        help="Fichier de sortie pour le rapport JSON"
    )
    
    parser.add_argument(
        "--output",
        help="Alias pour --report"
    )
    
    args = parser.parse_args()
    
    # Configuration du runner
    runner = TestSuiteRunner(authentic=args.authentic, verbose=args.verbose)
    
    try:
        # Ex√©cution
        report = runner.run_all_suites(
            component_filter=args.component,
            fast_only=args.fast
        )
        
        # Sauvegarde du rapport
        if args.report or args.output:
            runner.save_report(report, args.report or args.output)
        
        # Code de retour
        all_passed = all(r.success for r in report.suite_results)
        sys.exit(0 if all_passed else 1)
        
    except KeyboardInterrupt:
        print(f"\n{Colors.YELLOW}[INTERRUPT] Interruption utilisateur{Colors.END}")
        sys.exit(130)
    except Exception as e:
        print(f"\n{Colors.RED}[CRITICAL] ERREUR CRITIQUE: {str(e)}{Colors.END}")
        if args.verbose:
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()